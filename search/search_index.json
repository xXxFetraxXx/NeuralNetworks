{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Accueil","text":"<p>Bienvenue sur la documentation officielle de NeuralNetworks, une biblioth\u00e8que Python pour cr\u00e9er et entra\u00eener facilement vos r\u00e9seaux de neurones.  </p>"},{"location":"#fonctionnalites-principales","title":"Fonctionnalit\u00e9s principales","text":"<ul> <li>Cr\u00e9ation simple de MLP (Perceptron Multi-Couche)</li> <li>Entra\u00eenement facile avec PyTorch</li> <li>Visualisation des r\u00e9seaux et des pertes</li> <li>Support pour Fourier features et encodage avanc\u00e9</li> <li>Modules et fonctions modulaires pour prototyper rapidement</li> </ul>"},{"location":"Container/","title":"NeuralNetworks.Container","text":"<p><code>class NeuralNetworks.Container (dictionnaire)</code> [source]</p> <p>Permet de cr\u00e9\u00e9 un selectionneur \u00e0 partir d'un dictionnaire et d'afficher simplement son contenu.</p> sous-classes Description <code>norms</code> Selectionneur pr\u00e9fabriqu\u00e9 de Fonction d'activation <code>crits</code> Selectionneur pr\u00e9fabriqu\u00e9 de Fonction objectif <code>optims</code> Selectionneur pr\u00e9fabriqu\u00e9 d'Optimisation"},{"location":"Container/crits/","title":"NeuralNetworks.crits","text":"<p><code>NeuralNetworks.crits</code> [source]</p> <p>inh\u00e9rite des propri\u00e9t\u00e9s de la classe <code>Container</code> </p> Valeurs Module PyTorch <code>'MSE'</code> <code>nn.MSELoss</code> <code>'L1'</code> <code>nn.L1Loss()</code> <code>'SmoothL1'</code> <code>nn.SmoothL1Loss()</code> <code>'Huber'</code> <code>nn.HuberLoss()</code> <code>'CrossEntropy'</code> <code>nn.CrossEntropyLoss()</code> <code>'KLDiv'</code> <code>nn.KLDivLoss()</code> <code>'PoissonNLL'</code> <code>nn.PoissonNLLLoss()</code> <code>'MultiLabelSoftMargin'</code> <code>nn.MultiLabelSoftMarginLoss()</code>"},{"location":"Container/get/","title":"NeuralNetworks.Container.get","text":"<p><code>NeuralNetworks.Container.get (name)</code> [source]</p> <p>S\u00e9l\u00e9ctionne l'indice <code>name</code> dans le dictionnaire. Si <code>name</code> n'est pas trouv\u00e9, renvoie le premier \u00e9l\u00e9ment du dictionnaire.</p>"},{"location":"Container/norms/","title":"NeuralNetworks.norms","text":"<p><code>NeuralNetworks.norms</code> [source]</p> <p>inh\u00e9rite des propri\u00e9t\u00e9s de la classe <code>Container</code> </p> Valeurs Module PyTorch <code>'ReLU'</code> <code>nn.ReLU()</code> <code>'LeakyReLU'</code> <code>nn.LeakyReLU()</code> <code>'ELU'</code> <code>nn.ELU()</code> <code>'SELU'</code> <code>nn.SELU()</code> <code>'GELU'</code> <code>nn.GELU()</code> <code>'Mish'</code> <code>nn.Mish()</code> <code>'Softplus'</code> <code>nn.Softplus()</code> <code>'Sigmoid'</code> <code>nn.Sigmoid()</code> <code>'Tanh'</code> <code>nn.Tanh()</code> <code>'Hardtanh'</code> <code>nn.Hardtanh()</code> <code>'Softsign'</code> <code>nn.Softsign()</code>"},{"location":"Container/optims/","title":"NeuralNetworks.optims","text":"<p><code>NeuralNetworks.optims</code> [source]</p> <p>inh\u00e9rite des propri\u00e9t\u00e9s de la classe <code>Container</code> </p> Valeurs Module PyTorch <code>'Adadelta'</code> <code>optim.Adadelta()</code> <code>'Adafactor'</code> <code>optim.Adafactor()</code> <code>'Adam'</code> <code>optim.Adam()</code> <code>'AdamW'</code> <code>optim.AdamW()</code> <code>'Adamax'</code> <code>optim.Adamax()</code> <code>'ASGD'</code> <code>optim.ASGD()</code> <code>'NAdam'</code> <code>optim.NAdam()</code> <code>'RAdam'</code> <code>optim.RAdam()</code> <code>'RMSprop'</code> <code>optim.RMSprop()</code> <code>'Rprop'</code> <code>optim.Rprop()</code> <code>'SGD'</code> <code>optim.SGD()</code>"},{"location":"Trainer/","title":"NeuralNetworks.Trainer","text":"<p><code>class NeuralNetworks.Trainer (*nets, inputs, outputs, init_train_size, final_train_size, optim, init_lr, final_lr, crit, batch_size)</code> [source]</p> <p>Classe pour entra\u00eener des r\u00e9seaux avec mini-batchs et Automatic Mixed Precision.</p> Param\u00e8tres Type Optionnel <code>*nets</code> <code>Module</code> Non <code>inputs</code> <code>torch.Tensor([float])</code> Non <code>outputs</code> <code>torch.Tensor([float])</code> Non <code>init_train_size</code> <code>float</code> Oui <code>final_train_size</code> <code>float</code> Oui <code>optim</code> <code>optim</code> Oui <code>init_lr</code> <code>float</code> Oui <code>final_lr</code> <code>float</code> Oui <code>crit</code> <code>crit</code> Oui <code>batch_size</code> <code>int</code> Oui Initialisation d'un trainer Trainer_exemple.py<pre><code>from NeuralNetworks import Trainer\n\nT = Trainer (\n    net                       , # (1)!\n    inputs           = inputs , # (2)!\n    outputs          = outputs, # (3)!\n    init_train_size  = 0.001  , \n    final_train_size = 1      , \n    init_lr          = 1e-3   , \n    final_lr         = 1e-5   , \n    optim            = 'Adam' , \n    crit             = 'MSE'  , \n    batch_size       = 163840   \n)\n</code></pre> <ol> <li>Voir <code>Module</code></li> <li>La d\u00e9finition de inputs n'est pas explicit\u00e9e ici</li> <li>La d\u00e9finition de outputs n'est pas explicit\u00e9e ici</li> </ol>"},{"location":"Trainer/lr/","title":"Dynamique du learning rate","text":"<p>Le learning rate a une double d\u00e9pendance, il varie en fonction de l'\u00e9poque mais aussi en fonction du r\u00e9sidu de l'\u00e9poque. Cela permet d'avoir un learning rate adapt\u00e9 en fonction de la taille de l'entrainement et qu'il continu \u00e0 apprendre dans le cas ou les erreurs sont grandes.</p> <p>La fusion des deux dynamiques de fait de la mani\u00e8re suivante:</p> \\[ lr = (\\text{init_lr} - \\text{final_lr}) \\cdot \\max (\\text{lr_epoch} , \\text{lr_loss}) + \\text{final_lr} \\] Dynamique en fonction de l'\u00e9poque \\[ \\text{lr_epoch} = \\begin{cases}         1 - \\frac{1}{2} (6 \\cdot (\\frac{epoch}{0.1 \\cdot \\text{Nb_epochs}})^5 - 15 (\\cdot \\frac{epoch}{0.1 \\cdot \\text{Nb_epochs}})^4 + 10 \\cdot (\\frac{epoch}{0.1 \\cdot \\text{Nb_epochs}})^3) \\quad &amp;\\text{si} \\, epoch &lt;= 0.1 \\cdot \\text{Nb_epochs}\\\\         \\frac{1}{2} (1 - 6 \\cdot (\\frac{epoch - 0.1 \\cdot \\text{Nb_epochs}}{0.9 \\cdot \\text{Nb_epochs}})^5 + 15 \\cdot (\\frac{epoch - 0.1 \\cdot \\text{Nb_epochs}}{0.9 \\cdot \\text{Nb_epochs}})^4 - 10 \\cdot (\\frac{epoch - 0.1 \\cdot \\text{Nb_epochs}}{0.9 \\cdot \\text{Nb_epochs}})^3) \\quad &amp;\\text{si} \\, epoch &gt;= 0.1 \\cdot \\text{Nb_epochs}\\\\     \\end{cases} \\] \u00c9chelle lin\u00e9aire\u00c9chelle logarithmique <p> </p> <p> </p> Dynamique en fonction des r\u00e9sidus \\[ \\text{lr_loss} =\\frac{1}{2} (1 + loss^4 + (loss^2 -2 \\cdot loss +1)^4  ) \\] \u00c9chelle lin\u00e9aire\u00c9chelle logarithmique <p> </p> <p> </p>"},{"location":"Trainer/train/","title":"NeuralNetworks.Trainer.train","text":"<p><code>NeuralNetworks.Trainer.train (num_epochs, disable_tqdm, benchmark)</code> [source]</p> <p>Lancement d'un entrainement avec le trainer d\u00e9finit.</p> Param\u00e8tres Type Optionnel <code>num_epochs</code> <code>int</code> Oui <code>disable_tqdm</code> <code>boolean</code> Oui <code>benchmark</code> <code>boolean</code> Oui Lancement d'un entrainement train_exemple.py<pre><code>from NeuralNetworks import Trainer\n\nT = Trainer (\n    net              , # (1)!\n    inputs  = inputs , # (2)!\n    outputs = outputs, # (3)!\n)\n\nT.train (\n    num_epochs   = 1000 ,\n    disable_tqdm = False,\n    benchmark    = False\n)\n</code></pre> <ol> <li>Voir <code>Module</code></li> <li>La d\u00e9finition de inputs n'est pas explicit\u00e9e ici</li> <li>La d\u00e9finition de outputs n'est pas explicit\u00e9e ici</li> </ol>"},{"location":"UI/learnings/","title":"NeuralNetworks.learnings","text":"<p><code>NeuralNetworks.learnings (*nets, fig_size, color)</code> [source]</p> <p>Affiche les taux d'apprentissage en fonction des \u00e9poques d'entrainement des r\u00e9seaux.</p> Param\u00e8tres Type Optionnel <code>*nets</code> <code>Module</code> Non <code>fig_size</code> <code>int</code> oui <code>color</code> <code>str</code> oui Example learnings.py<pre><code>from NeuralNetworks import MLP, Trainer, learnings\n\nhidden_layers = [256,256,256,256,256,256,256,256,256,256]\nnet = MLP(2, hidden_layers, 3)\n\nT = Trainer (net, inputs = inputs, outputs = outputs) # (1)!\nT.train(1500)\n\nlearnings (net, fig_size = 10)\n</code></pre> <ol> <li>La d\u00e9finition de inputs et outputs n'est pas explicit\u00e9e ici</li> </ol> <p> </p>"},{"location":"UI/losses/","title":"NeuralNetworks.losses","text":"<p><code>NeuralNetworks.losses (*nets, fuse_losses, names, fig_size, color)</code> [source]</p> <p>Affiche les r\u00e9sidus en fonction des \u00e9poques d'entrainement des r\u00e9seaux.</p> Param\u00e8tres Type Optionnel <code>*nets</code> <code>Module</code> Non <code>fuse_losses</code> <code>boolean</code> oui <code>names</code> <code>boolean</code> oui <code>fig_size</code> <code>boolean</code> oui <code>color</code> <code>boolean</code> oui Example Sans fusion des r\u00e9sidusAvec fusion des r\u00e9sidus losses_exemple.py<pre><code>from NeuralNetworks import MLP, Trainer, learnings\n\nhidden_layers = [256,256,256,256,256,256,256,256,256,256]\nnet = MLP(2, hidden_layers, 3)\n\nT = Trainer (net, inputs = inputs, outputs = outputs) # (1)!\nT.train(1500)\n\nlosses (net, fuse_losses = False, names = [\"Sortie 1\", \"Sortie 2\", \"Sortie 3\"], fig_size = 10)\n</code></pre> <ol> <li>La d\u00e9finition de inputs et outputs n'est pas explicit\u00e9e ici</li> </ol> <p> </p> fused_losses_exemple.py<pre><code>from NeuralNetworks import MLP, Trainer, learnings\n\nhidden_layers = [256,256,256,256,256,256,256,256,256,256]\nnet = MLP(2, hidden_layers, 3)\n\nT = Trainer (net, inputs = inputs, outputs = outputs) # (1)!\nT.train(1500)\n\nlosses (net, fuse_losses = True, fig_size = 10)\n</code></pre> <ol> <li>La d\u00e9finition de inputs et outputs n'est pas explicit\u00e9e ici</li> </ol> <p> </p>"},{"location":"dependences/dependences/","title":"Dependances","text":"<p>Modules n\u00e9cessaires au fonctionnement de ce module. S'installent automatiquement lors de l'installation du module.</p> Pytoch <p><pre><code>pip install torch\n</code></pre> </p> numpy <pre><code>pip install numpy\n</code></pre> <p> </p> matplotlib <pre><code>pip install matplotlib\n</code></pre> <p> </p> onnx <pre><code>pip install onnx\n</code></pre> <p> </p> onnx-simplifier <pre><code>pip install onnx-simplifier\n</code></pre> <p> </p> tqdm <pre><code>pip install tqdm\n</code></pre> <p> </p> visualtorch <pre><code>pip install visualtorch\n</code></pre> <p> </p>"},{"location":"dependences/install/","title":"Installation","text":"<pre><code>pip install NeuralNetworks\n</code></pre> <pre><code>import NeuralNetworks\n</code></pre> <p>or</p> <pre><code>from NeuralNetworks import *\n</code></pre> <p>or even</p> <pre><code>from NeuralNetworks import MLP, Trainer\n</code></pre>"},{"location":"dependences/support/","title":"Support","text":"<p>Le programme s\u00e9lectionne automatiquement le meilleur appareil qu'il d\u00e9tecte.</p> Linux <ul> <li>Support Nvidia GPU</li> <li>Support AMD GPU</li> <li>Support Intel GPU</li> <li>Support CPU</li> </ul> Windows <ul> <li>Support Nvidia GPU</li> <li>Support CPU</li> </ul> macOS <ul> <li>Support MPS GPU</li> <li>Support CPU</li> </ul> Syst\u00e8me non reconnu <ul> <li>Support CPU</li> </ul>"},{"location":"module/","title":"NeuralNetworks.Module","text":"<p><code>class NeuralNetworks.Module (_name, **Reconstruction_data)</code> [source]</p> <p>Module complet pour faciliter la cr\u00e9ation et l'entra\u00eenement de mod\u00e8les pytorch en fournissant des classes pr\u00e9configur\u00e9es.</p> sous-classes Description <code>MLP</code> Classe pr\u00e9fabriqu\u00e9e de Multilayer Perceptron <code>VAE</code> Classe pr\u00e9fabriqu\u00e9e de Variational Autoencoder Attributs Type <code>MLP.losses</code> <code>list[float]</code> <code>MLP.learnings</code> <code>list[float]</code> <code>MLP.model</code> <code>nn.Sequential</code> <code>MLP.name</code> <code>str</code>"},{"location":"module/load/","title":"NeuralNetworks.Module.load","text":"<p><code>NeuralNetworks.Module.load (path, device)</code> [source]</p> <p>Charge le r\u00e9seau depuis un format propri\u00e9taire s\u00e9curis\u00e9.</p>"},{"location":"module/mlp/","title":"NeuralNetworks.MLP","text":"<p><code>class MLP (input_size, output_size, hidden_layers, sigmas, fourier_input_size, nb_fourier, norm, name)</code> [source]</p> <p>inh\u00e9rite des propri\u00e9t\u00e9s de la classe <code>Module</code> </p> <p>Cette classe fournit :</p> <ul> <li>Un MLP enti\u00e8rement configurable (dimensions, activation).</li> <li>Option d'encodage Fourier Features sur les entr\u00e9es.</li> </ul> <p>Permet de construire rapidement un r\u00e9seau de neurones multicouches rapidement sans connaisances profondes de pytorch.</p> Param\u00e8tres Type Optionnel Default value <code>input_size</code> <code>int</code> Oui <code>1</code> <code>output_size</code> <code>int</code> Oui <code>1</code> <code>hidden_layers</code> <code>list[int]</code> Oui <code>[1]</code> <code>sigmas</code> <code>list[float]</code> Oui <code>None</code> <code>fourier_input_size</code> <code>int</code> Oui <code>2</code> <code>nb_fourier</code> <code>int</code> Oui <code>8</code> <code>norm</code> <code>norm</code> Oui <code>'Relu'</code> <code>name</code> <code>str</code> Oui <code>'Net'</code> Example MLP_exemple.py<pre><code>from NeuralNetworks import MLP\n\nnet = MLP(\n    input_size    = 2,\n    hidden_layers = [512,512,512,512,512,512,512,512,512,512],\n    output_size   = 3,\n    nb_fourier    = 256,\n    sigmas        = [0.1,1],\n    norm          = \"Relu\",\n    name          = \"Net\"\n)\n</code></pre>"},{"location":"module/save/","title":"NeuralNetworks.Module.save","text":"<p><code>NeuralNetworks.Module.save</code> [source]</p> <p>Enregistre le r\u00e9seau dans un format propri\u00e9taire s\u00e9curis\u00e9.</p>"},{"location":"module/save_onnx/","title":"NeuralNetworks.Module.onnx_save","text":"<p><code>NeuralNetworks.Module.onnx_save</code> [source]</p> <p>Enregistre le r\u00e9seau dans le format <code>onnx</code>.</p>"},{"location":"module/vae/","title":"NeuralNetworks.VAE","text":"<p><code>class VAE (imsize, latentsize, labelsize, channels, linear_channels, name, norm, norm_cc)</code> [Source]</p> <p>inh\u00e9rite des propri\u00e9t\u00e9s de la classe <code>Module</code> </p> <p>Cette classe fournit :</p> <ul> <li>Un VAE enti\u00e8rement configurable.</li> </ul> Param\u00e8tres Type Optionnel <code>imsize</code> <code>int</code> Non <code>latentsize</code> <code>int</code> Non <code>labelsize</code> <code>int</code> Non <code>channels</code> <code>list[int]</code> Oui <code>linear_channels</code> <code>list[int]</code> Oui <code>name</code> <code>str</code> Oui <code>norm</code> <code>str</code> Oui <code>norm_cc</code> <code>str</code> Oui Example VAE_exemple.py<pre><code>from NeuralNetworks import VAE\n\n# WIP\n</code></pre>"}]}