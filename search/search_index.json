{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"NeuralNetworks Module","text":"<p>Module complet pour la cr\u00e9ation, l'entra\u00eenement et la visualisation de Multi-Layer Perceptrons (MLP) avec encodage optionnel Fourier, gestion automatique des pertes, compilation Torch et outils de traitement d'images pour l'apprentissage sur des images RGB.</p>"},{"location":"#contenu-principal","title":"Contenu principal","text":""},{"location":"#classes","title":"Classes","text":""},{"location":"#mlp","title":"MLP","text":"<p>Multi-Layer Perceptron (MLP) avec options avanc\u00e9es :</p> <ul> <li>Encodage Fourier gaussien (RFF) optionnel  </li> <li>Stockage automatique des pertes  </li> <li>Compilation Torch optionnelle pour acc\u00e9l\u00e9rer l\u2019inf\u00e9rence  </li> <li>Gestion flexible de l\u2019optimiseur, de la fonction de perte et de la normalisation  </li> </ul> <p>M\u00e9thodes principales :</p> <ul> <li> <p><code>__init__(layers, learning_rate, Fourier, optimizer, criterion, normalizer, name, Iscompiled)</code>   Initialise le r\u00e9seau avec toutes les options.</p> </li> <li> <p><code>train(inputs, outputs, num_epochs, batch_size)</code>   Entra\u00eene le MLP sur des donn\u00e9es (<code>inputs \u2192 outputs</code>) en utilisant AMP et mini-batchs.</p> </li> <li> <p><code>plot(inputs, img_array)</code>   Affiche l'image originale, la pr\u00e9diction du MLP et la courbe des pertes.</p> </li> <li> <p><code>__call__(x)</code>   Applique l\u2019encodage puis le MLP pour produire une pr\u00e9diction.</p> </li> <li> <p><code>Create_MLP(layers)</code>   Construit le MLP avec normalisation/activation et Sigmoid finale.</p> </li> <li> <p><code>params()</code>   Retourne tous les poids du MLP (ligne par ligne) sous forme de liste de <code>numpy.ndarray</code>.</p> </li> <li> <p><code>nb_params()</code>   Calcule le nombre total de poids dans le MLP.</p> </li> <li> <p><code>neurons()</code>   Retourne la liste des biais (neurones) de toutes les couches lin\u00e9aires.</p> </li> <li> <p><code>__repr__()</code>   Affiche un sch\u00e9ma visuel du MLP via visualtorch et print des dimensions.</p> </li> </ul>"},{"location":"#fonctions-utilitaires","title":"Fonctions utilitaires","text":"<ul> <li> <p><code>tensorise(obj)</code>   Convertit un objet array-like ou tensor en <code>torch.Tensor</code> float32 sur le device actif.</p> </li> <li> <p><code>list_to_cpu(cuda_tensors)</code>   Copie une liste de tenseurs CUDA et les transf\u00e8re sur le CPU.</p> </li> <li> <p><code>rglen(list)</code>   Renvoie un range correspondant aux indices d'une liste.</p> </li> <li> <p><code>fPrintDoc(obj)</code>   Cr\u00e9e une fonction lambda qui affiche le docstring d'un objet.</p> </li> <li> <p><code>image_from_url(url, img_size)</code>   T\u00e9l\u00e9charge une image depuis une URL, la redimensionne et g\u00e9n\u00e8re :</p> </li> <li><code>img_array</code> : <code>np.ndarray (H, W, 3)</code> pour affichage.  </li> <li><code>inputs</code> : tenseur <code>(H*W, 2)</code> coordonn\u00e9es normalis\u00e9es.  </li> <li><code>outputs</code> : tenseur <code>(H*W, 3)</code> valeurs RGB cibles.</li> </ul>"},{"location":"#visualisation-et-comparaison","title":"Visualisation et comparaison","text":"<ul> <li> <p><code>plot(img_array, inputs, *nets)</code>   Affiche pour chaque r\u00e9seau l'image reconstruite \u00e0 partir des entr\u00e9es.</p> </li> <li> <p><code>compare(img_array, inputs, *nets)</code>   Affiche pour chaque r\u00e9seau l'erreur absolue entre l'image originale et la pr\u00e9diction,   et trace \u00e9galement les pertes cumul\u00e9es. Chaque r\u00e9seau doit poss\u00e9der :  </p> </li> <li><code>encoding(x)</code> si RFF activ\u00e9  </li> <li><code>model()</code> retournant un tenseur <code>(N, 3)</code> </li> <li>attribut <code>losses</code></li> </ul>"},{"location":"#objets-et-dictionnaires","title":"Objets et dictionnaires","text":"<ul> <li> <p><code>Norm_list : dict</code>   Contient les modules PyTorch correspondant aux fonctions de normalisation/activation disponibles (ReLU, GELU, Sigmoid, Tanh, etc.)</p> </li> <li> <p><code>Criterion_list : dict</code>   Contient les fonctions de perte PyTorch disponibles (MSE, L1, SmoothL1, BCE, CrossEntropy, etc.)</p> </li> <li> <p><code>Optim_list(self, learning_rate)</code>   Retourne un dictionnaire d\u2019optimiseurs PyTorch initialis\u00e9s avec <code>self.model.parameters()</code>.</p> </li> </ul>"},{"location":"#device-et-configuration","title":"Device et configuration","text":"<ul> <li><code>device</code>   Device par d\u00e9faut (GPU si disponible, sinon CPU).</li> </ul>"},{"location":"#parametres-matplotlib-et-pytorch","title":"Param\u00e8tres matplotlib et PyTorch","text":"<ul> <li>Style global pour fond transparent et texte gris  </li> <li>Optimisations CUDA activ\u00e9es pour TF32, matmul et convolutions  </li> <li>Autograd configur\u00e9 pour privil\u00e9gier les performances</li> </ul>"},{"location":"#notes-generales","title":"Notes g\u00e9n\u00e9rales","text":"<ul> <li>Toutes les m\u00e9thodes de MLP utilisent les tenseurs sur le device global (CPU ou GPU)  </li> <li>Les images doivent \u00eatre normalis\u00e9es entre 0 et 1  </li> <li>Les fonctions interactives (<code>plot</code>, <code>compare</code>) utilisent matplotlib en mode interactif  </li> <li>Le module est con\u00e7u pour fonctionner dans Jupyter et scripts Python classiques</li> </ul>"},{"location":"mlp/","title":"Multi-Layer Perceptron (MLP)","text":"<p>Multi-Layer Perceptron (MLP) avec encodage optionnel Fourier (RFF), suivi automatique des pertes, visualisation et compilation PyTorch.</p> <p>Cette classe fournit :</p> <ul> <li>Un MLP enti\u00e8rement configurable (dimensions, normalisation, activation)  </li> <li>Option d'encodage Fourier (Random Fourier Features) sur les entr\u00e9es  </li> <li>M\u00e9thodes pour entra\u00eener le r\u00e9seau avec mini-batchs et AMP (Automatic Mixed Precision)  </li> <li>Visualisation de l'architecture via visualtorch  </li> <li>Suivi et affichage de la perte d'entra\u00eenement  </li> <li>Acc\u00e8s aux poids, biais et nombre de param\u00e8tres  </li> <li>Compilation du mod\u00e8le via <code>torch.compile</code> pour acc\u00e9l\u00e9rer l'inf\u00e9rence  </li> <li>M\u00e9thode <code>__call__</code> permettant l'utilisation directe comme une fonction (<code>y = net(x)</code>)</li> </ul>"},{"location":"mlp/#parameters","title":"Parameters","text":"Parameter Type Optional Description <code>layers</code> list[int] Yes Dimensions successives du r\u00e9seau (entr\u00e9e \u2192 couches cach\u00e9es \u2192 sortie). Exemple : <code>[in_features, hidden1, hidden2, ..., out_features]</code>. Default: <code>[1, 1, 1]</code> <code>learning_rate</code> float Yes Taux d\u2019apprentissage pour l\u2019optimiseur. Default: <code>1e-3</code> <code>Fourier</code> bool Yes Si True, applique un encodage Fourier gaussien (RFF) sur les entr\u00e9es. Default: <code>True</code> <code>optimizer</code> str Yes Nom de l\u2019optimiseur \u00e0 utiliser (doit exister dans <code>Optim_list</code>). Default: <code>\"ADAM\"</code> <code>criterion</code> str Yes Fonction de perte \u00e0 utiliser (doit exister dans <code>Criterion_list</code>). Default: <code>\"MSE\"</code> <code>normalizer</code> str Yes Type de normalisation / activation pour les couches cach\u00e9es (ex: <code>\"Relu\"</code>). Default: <code>\"Relu\"</code> <code>name</code> str Yes Nom du r\u00e9seau pour identification ou affichage. Default: <code>\"Net\"</code> <code>Iscompiled</code> bool Yes Si True, compile le mod\u00e8le via <code>torch.compile</code> pour acc\u00e9l\u00e9rer l\u2019inf\u00e9rence. Default: <code>True</code>"},{"location":"mlp/#attributes","title":"Attributes","text":"<ul> <li><code>losses : list[torch.Tensor]</code> \u2014 Historique des pertes cumul\u00e9es lors de l'entra\u00eenement  </li> <li><code>layers : list[int]</code> \u2014 Dimensions du r\u00e9seau, ajust\u00e9es si encodage Fourier actif  </li> <li><code>encoding : nn.Module</code> \u2014 Module appliquant l'encodage des entr\u00e9es (RFF ou identity)  </li> <li><code>norm : nn.Module</code> \u2014 Normalisation ou activation utilis\u00e9e dans les couches cach\u00e9es  </li> <li><code>criterion : nn.Module</code> \u2014 Fonction de perte PyTorch sur le device sp\u00e9cifi\u00e9  </li> <li><code>model : nn.Sequential</code> \u2014 MLP complet construit dynamiquement  </li> <li><code>optimizer : torch.optim.Optimizer</code> \u2014 Optimiseur associ\u00e9 au MLP  </li> <li><code>name : str</code> \u2014 Nom du r\u00e9seau  </li> </ul>"},{"location":"mlp/#methods","title":"Methods","text":"<ul> <li><code>__init__(...)</code> \u2014 Initialise le r\u00e9seau, configure l\u2019encodage, la fonction de perte et l\u2019optimiseur  </li> <li><code>__repr__()</code> \u2014 Affiche un sch\u00e9ma visuel du MLP et ses dimensions (avec compression si n\u00e9cessaire)  </li> <li><code>__call__(x)</code> \u2014 Applique l\u2019encodage et le MLP sur un input x, retourne la pr\u00e9diction en <code>ndarray</code> </li> <li><code>Create_MLP(layers)</code> \u2014 Construit un <code>nn.Sequential</code> avec les couches lin\u00e9aires, activations et normalisations  </li> <li><code>plot(inputs, img_array)</code> \u2014 Affiche l\u2019image originale, l\u2019image pr\u00e9dite et la courbe des pertes  </li> <li><code>train(inputs, outputs, num_epochs=1500, batch_size=1024)</code> \u2014 Entra\u00eene le MLP avec mini-batchs et AMP, stocke les pertes  </li> <li><code>params()</code> \u2014 Retourne tous les poids du MLP sous forme de liste d\u2019<code>ndarray</code> </li> <li><code>neurons()</code> \u2014 Retourne tous les biais du MLP sous forme de liste d\u2019<code>ndarray</code> </li> <li><code>nb_params()</code> \u2014 Calcule le nombre total de param\u00e8tres (poids uniquement) du r\u00e9seau  </li> </ul>"},{"location":"mlp/#notes","title":"Notes","text":"<ul> <li>La classe supporte un entra\u00eenement sur GPU via <code>device</code> </li> <li>Les fonctions de visualisation utilisent matplotlib et visualtorch  </li> <li>Les sorties sont compatibles avec des images normalis\u00e9es entre 0 et 1  </li> <li>Le suivi des pertes permet d\u2019afficher l\u2019\u00e9volution du training loss</li> </ul>"},{"location":"mlp/Create_MLP/","title":"Construction du MLP (<code>Create_MLP</code>)","text":"<p>Construit un Multi-Layer Perceptron (MLP) standard compos\u00e9 : - d'une succession de couches Lin\u00e9aires - suivies d'une normalisation (<code>self.norm</code>) apr\u00e8s chaque couche cach\u00e9e - et d'une activation Sigmoid sur la couche de sortie</p>"},{"location":"mlp/Create_MLP/#parametres","title":"Param\u00e8tres","text":"Param\u00e8tre Type Description <code>layers</code> list[int] Liste des dimensions successives du r\u00e9seau. Exemple : <code>[in_features, hidden1, hidden2, ..., out_features]</code>"},{"location":"mlp/Create_MLP/#retour","title":"Retour","text":"Type Description <code>nn.Sequential</code> Le MLP complet sous forme de s\u00e9quence PyTorch"},{"location":"mlp/Create_MLP/#notes","title":"Notes","text":"<ul> <li>La couche finale applique syst\u00e9matiquement une Sigmoid, adapt\u00e9e \u00e0 des sorties dans <code>[0, 1]</code>.</li> </ul>"},{"location":"mlp/__call__/","title":"Inf\u00e9rence avec le MLP (<code>__call__</code>)","text":"<p>Effectue une inf\u00e9rence compl\u00e8te : encodage \u00e9ventuel, passage dans le MLP, puis retour en <code>numpy</code>.</p> <p>Cette m\u00e9thode permet d\u2019utiliser l\u2019objet comme une fonction directement.</p>"},{"location":"mlp/__call__/#parametres","title":"Param\u00e8tres","text":"Param\u00e8tre Type Description <code>x</code> array-like Entr\u00e9e(s) \u00e0 pr\u00e9dire. Peut \u00eatre un tableau numpy, une liste, ou d\u00e9j\u00e0 un tenseur compatible."},{"location":"mlp/__call__/#retour","title":"Retour","text":"Type Description <code>np.ndarray</code> Sortie du MLP apr\u00e8s encodage et propagation avant, convertie en tableau numpy sur CPU."},{"location":"mlp/__init__/","title":"Initialisation du r\u00e9seau MLP","text":"<p>Initialise un r\u00e9seau MLP avec options avanc\u00e9es : encodage Fourier, normalisation, choix d\u2019optimiseur et de fonction de perte, et compilation.</p>"},{"location":"mlp/__init__/#parameters","title":"Parameters","text":"Parameter Type Optional Description <code>layers</code> list[int] Yes Dimensions successives du r\u00e9seau (entr\u00e9e \u2192 couches cach\u00e9es \u2192 sortie). Default: <code>[1, 1, 1]</code> <code>learning_rate</code> float Yes Taux d\u2019apprentissage pour l\u2019optimiseur. Default: <code>1e-3</code> <code>Fourier</code> bool Yes Si True, applique un encodage RFF (Random Fourier Features) sur les entr\u00e9es. Default: <code>True</code> <code>optimizer</code> str Yes Nom de l\u2019optimiseur \u00e0 utiliser (doit \u00eatre pr\u00e9sent dans <code>Optim_list</code>). Default: <code>\"ADAM\"</code> <code>criterion</code> str Yes Nom de la fonction de perte \u00e0 utiliser (doit \u00eatre pr\u00e9sent dans <code>Criterion_list</code>). Default: <code>\"MSE\"</code> <code>normalizer</code> str Yes Type de normalisation / activation \u00e0 appliquer entre les couches cach\u00e9es. Default: <code>\"Relu\"</code> <code>name</code> str Yes Nom du r\u00e9seau (pour identification et affichage). Default: <code>\"Net\"</code> <code>Iscompiled</code> bool Yes Si True, compile le mod\u00e8le avec <code>torch.compile</code> pour acc\u00e9l\u00e9rer l\u2019inf\u00e9rence. Default: <code>True</code>"},{"location":"mlp/__init__/#attributes","title":"Attributes","text":"<ul> <li><code>losses : list</code> \u2014 Historique des pertes durant l\u2019entra\u00eenement  </li> <li><code>layers : list[int]</code> \u2014 Dimensions du r\u00e9seau apr\u00e8s ajustement pour encodage Fourier  </li> <li><code>encoding : nn.Module</code> \u2014 Module appliquant l\u2019encodage des entr\u00e9es (RFF ou identit\u00e9)  </li> <li><code>norm : nn.Module</code> \u2014 Normalisation / activation utilis\u00e9e entre les couches cach\u00e9es  </li> <li><code>criterion : nn.Module</code> \u2014 Fonction de perte PyTorch sur GPU  </li> <li><code>model : nn.Sequential</code> \u2014 MLP complet construit dynamiquement  </li> <li><code>optimizer : torch.optim.Optimizer</code> \u2014 Optimiseur associ\u00e9 au MLP  </li> <li><code>name : str</code> \u2014 Nom du r\u00e9seau</li> </ul>"},{"location":"mlp/__repr__/","title":"Visualisation du MLP","text":"<p>G\u00e9n\u00e8re un aper\u00e7u visuel du MLP et affiche ses dimensions.</p> <p>Cette m\u00e9thode :</p> <ul> <li>Cr\u00e9e une version \u00e9ventuellement \"compress\u00e9e\" des dimensions du r\u00e9seau   (utile lorsque certaines couches d\u00e9passent 30 neurones, afin de conserver une visualisation lisible)</li> <li>Utilise <code>visualtorch.graph_view</code> pour afficher un sch\u00e9ma du MLP</li> <li>Imprime la liste r\u00e9elle des dimensions du r\u00e9seau</li> <li>Retourne une cha\u00eene indiquant si un redimensionnement a \u00e9t\u00e9 appliqu\u00e9</li> </ul>"},{"location":"mlp/__repr__/#notes","title":"Notes","text":"<ul> <li>Le redimensionnement ne modifie pas le MLP r\u00e9el. Il ne sert qu'\u00e0 am\u00e9liorer la lisibilit\u00e9 du graphe affich\u00e9.</li> <li>Si Fourier Features sont activ\u00e9es, seule la premi\u00e8re dimension est recalcul\u00e9e en cons\u00e9quence.</li> </ul>"},{"location":"mlp/nb_params/","title":"Nombre total de param\u00e8tres du MLP (<code>nb_params</code>)","text":"<p>Calcule le nombre total de param\u00e8tres (poids) du MLP.</p> <p>Cette m\u00e9thode parcourt les param\u00e8tres du mod\u00e8le et ne compte que les poids des couches lin\u00e9aires. Dans un <code>nn.Linear</code>, <code>parameters()</code> renvoie dans l'ordre : - les poids (indice pair) - puis les biais (indice impair)  </p> <p>On ignore donc les biais dans ce comptage.</p>"},{"location":"mlp/nb_params/#retour","title":"Retour","text":"Type Description <code>int</code> Nombre total de param\u00e8tres pond\u00e9r\u00e9s (weights) dans le MLP."},{"location":"mlp/neurons/","title":"Extraction des neurones du MLP (<code>neurons</code>)","text":"<p>Extrait l'ensemble des neurones (biais) du MLP couche par couche.</p> <p>Cette m\u00e9thode parcourt les param\u00e8tres du mod\u00e8le et r\u00e9cup\u00e8re uniquement les poids associ\u00e9s aux biais des couches lin\u00e9aires. Dans un <code>nn.Linear</code>, <code>parameters()</code> renvoie successivement : - les poids (weights) - puis les biais (bias)  </p> <p>Les indices impairs correspondent donc aux biais.</p>"},{"location":"mlp/neurons/#retour","title":"Retour","text":"Type Description <code>list[np.ndarray]</code> Liste contenant chaque neurone (chaque valeur de biais), converti en array NumPy sur CPU."},{"location":"mlp/params/","title":"Extraction des poids du MLP (<code>params</code>)","text":"<p>Retourne la liste de tous les poids (weights) du MLP.</p> <p>Cette fonction extrait uniquement les matrices de poids des couches lin\u00e9aires, en ignorant les biais. Dans un <code>nn.Linear</code>, <code>parameters()</code> renvoie dans l'ordre : - les poids (indice pair) - les biais (indice impair)</p> <p>La m\u00e9thode :</p> <ul> <li>Parcourt les param\u00e8tres du r\u00e9seau  </li> <li>S\u00e9lectionne uniquement ceux correspondant aux poids  </li> <li>S\u00e9pare chaque ligne de la matrice de poids  </li> <li>Convertit chaque ligne en <code>ndarray</code> CPU d\u00e9tach\u00e9</li> </ul>"},{"location":"mlp/params/#retour","title":"Retour","text":"Type Description <code>list[np.ndarray]</code> Liste contenant chaque ligne des matrices de poids (chaque \u00e9l\u00e9ment est un vecteur numpy)."},{"location":"mlp/plot/","title":"Affichage de l\u2019image et de la perte (<code>plot</code>)","text":"<p>Affiche c\u00f4te \u00e0 c\u00f4te : - l\u2019image originale - l\u2019image pr\u00e9dite par le MLP - l\u2019\u00e9volution de la fonction de perte (loss) au cours de l\u2019entra\u00eenement</p>"},{"location":"mlp/plot/#parametres","title":"Param\u00e8tres","text":"Param\u00e8tre Type Description <code>inputs</code> array-like ou torch.Tensor Tableau des coordonn\u00e9es (ou features) servant d\u2019entr\u00e9e au r\u00e9seau. Doit correspondre \u00e0 la grille permettant de reconstruire l\u2019image. <code>img_array</code> np.ndarray Image originale sous forme de tableau <code>(H, W, 3)</code> utilis\u00e9e comme r\u00e9f\u00e9rence."},{"location":"mlp/plot/#notes","title":"Notes","text":"<p>Cette m\u00e9thode : - Tensorise les entr\u00e9es puis les encode avant passage dans le MLP - Reshape la sortie du mod\u00e8le pour retrouver la forme <code>(H, W, 3)</code> - Trace \u00e9galement la courbe de pertes stock\u00e9e dans <code>self.losses</code></p>"},{"location":"mlp/train/","title":"Entra\u00eenement du MLP (<code>train</code>)","text":"<p>Entra\u00eene le MLP sur des paires (inputs \u2192 outputs) en utilisant un sch\u00e9ma de mini-batchs et l'AMP (Automatic Mixed Precision).</p>"},{"location":"mlp/train/#parametres","title":"Param\u00e8tres","text":"Param\u00e8tre Type Description <code>inputs</code> array-like ou tensor Donn\u00e9es d'entr\u00e9e du r\u00e9seau, de shape <code>(N, input_dim)</code>. <code>outputs</code> array-like ou tensor Cibles associ\u00e9es, de shape <code>(N, output_dim)</code>. <code>num_epochs</code> int, optionnel Nombre total d'\u00e9poques d'entra\u00eenement. Default: <code>1500</code>. <code>batch_size</code> int, optionnel Taille des mini-batchs utilis\u00e9s \u00e0 chaque it\u00e9ration. Default: <code>1024</code>."},{"location":"mlp/train/#notes","title":"Notes","text":"<ul> <li>Utilise <code>torch.amp.autocast</code> + <code>GradScaler</code> pour un entra\u00eenement acc\u00e9l\u00e9r\u00e9 en FP16  </li> <li>Les pertes par \u00e9poque sont stock\u00e9es dans <code>self.losses</code> </li> <li>Le r\u00e9seau doit poss\u00e9der :  </li> <li><code>self.model</code>      : module PyTorch (MLP)  </li> <li><code>self.encoding()</code> : encodage \u00e9ventuel (Fourier features)  </li> <li><code>self.criterion</code>  : fonction de perte  </li> <li><code>self.optimizer</code>  : optimiseur</li> </ul>"}]}