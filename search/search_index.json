{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Index","text":""},{"location":"#device","title":"device","text":"<p>Variable principale d'allocation des performances.</p>"},{"location":"#apple-silicon-macos","title":"Apple Silicon (macOS)","text":"<ul> <li>Si le syst\u00e8me d'exploitation est macOS (nomm\u00e9 <code>darwin</code> dans <code>platform.system()</code>), la fonction v\u00e9rifie si l'acc\u00e9l\u00e9rateur Metal Performance Shaders (MPS) est disponible sur l'appareil.</li> <li>Si MPS est disponible (<code>torch.backends.mps.is_available()</code>), l'appareil cible sera d\u00e9fini sur <code>'mps'</code> (c'est un \u00e9quivalent de CUDA pour les appareils Apple Silicon).</li> </ul>"},{"location":"#windows","title":"Windows","text":"<ul> <li>Si le syst\u00e8me d'exploitation est Windows, la fonction v\u00e9rifie d'abord si CUDA (NVIDIA) est disponible avec <code>torch.cuda.is_available()</code>. Si c'est le cas, le p\u00e9riph\u00e9rique sera d\u00e9fini sur CUDA.</li> </ul>"},{"location":"#linux","title":"Linux","text":"<ul> <li>Si le syst\u00e8me d'exploitation est Linux, plusieurs v\u00e9rifications sont effectu\u00e9es :</li> <li>CUDA (NVIDIA) : Si <code>torch.cuda.is_available()</code> renvoie <code>True</code>, le p\u00e9riph\u00e9rique sera d\u00e9fini sur <code>'cuda'</code>.</li> <li>ROCm (AMD) : Si le syst\u00e8me supporte ROCm via <code>torch.backends.hip.is_available()</code>, l'appareil sera d\u00e9fini sur <code>'cuda'</code> (ROCm est utilis\u00e9 pour les cartes AMD dans le cadre de l'API CUDA).</li> <li>Intel oneAPI / XPU : Si le syst\u00e8me prend en charge Intel oneAPI ou XPU via <code>torch.xpu.is_available()</code>, le p\u00e9riph\u00e9rique sera d\u00e9fini sur XPU.</li> </ul>"},{"location":"#systeme-non-reconnu","title":"Syst\u00e8me non reconnu","text":"<ul> <li>Si aucune des conditions ci-dessus n'est remplie, la fonction retourne <code>'cpu'</code> comme p\u00e9riph\u00e9rique par d\u00e9faut.</li> </ul>"},{"location":"Containers/concept/","title":"NeuralNetworks.Container","text":""},{"location":"Containers/crits/","title":"NeuralNetworks.crits","text":"Valeurs Module PyTorch Description <code>'MSE'</code> <code>nn.MSELoss</code> Perte utilis\u00e9e pour les r\u00e9gressions. <code>'L1'</code> <code>nn.L1Loss()</code> Perte utilis\u00e9e pour la r\u00e9gularisation. <code>'SmoothL1'</code> <code>nn.SmoothL1Loss()</code> Perte moins sensible aux outliers. <code>'Huber'</code> <code>nn.HuberLoss()</code> Perte moins affect\u00e9e par les grands \u00e9carts. <code>'CrossEntropy'</code> <code>nn.CrossEntropyLoss()</code> Perte utilis\u00e9e pour les probl\u00e8mes de classification multi-classes. <code>'KLDiv'</code> <code>nn.KLDivLoss()</code> Perte utilis\u00e9e pour des mod\u00e8les probabilistes. <code>'PoissonNLL'</code> <code>nn.PoissonNLLLoss()</code> Perte utilis\u00e9e pour la mod\u00e9lisation de comptages. <code>'MultiLabelSoftMargin'</code> <code>nn.MultiLabelSoftMarginLoss()</code> Perte utilis\u00e9e pour les probl\u00e8mes de classification multi-\u00e9tiquettes."},{"location":"Containers/norms/","title":"NeuralNetworks.norms","text":"Valeurs Module PyTorch Description <code>'ReLU'</code> <code>nn.ReLU()</code> Fonction d'activation ReLU classique (Rectified Linear Unit). <code>'LeakyReLU'</code> <code>nn.LeakyReLU()</code> ReLU avec un petit coefficient pour les valeurs n\u00e9gatives (param\u00e8tre <code>negative_slope</code>). <code>'ELU'</code> <code>nn.ELU()</code> Fonction d'activation ELU (Exponential Linear Unit), qui a une meilleure gestion des valeurs n\u00e9gatives. <code>'SELU'</code> <code>nn.SELU()</code> SELU (Scaled Exponential Linear Unit), une version am\u00e9lior\u00e9e de l'ELU pour des r\u00e9seaux auto-normalisants. <code>'GELU'</code> <code>nn.GELU()</code> GELU (Gaussian Error Linear Unit), une activation probabiliste bas\u00e9e sur une fonction gaussienne. <code>'Mish'</code> <code>nn.Mish()</code> ReLU diff\u00e9rentiable en tout points avec passage n\u00e9gatif. <code>'Softplus'</code> <code>nn.Softplus()</code> Fonction d'activation qui approxime ReLU mais de mani\u00e8re liss\u00e9e. <code>'Sigmoid'</code> <code>nn.Sigmoid()</code> Fonction d'activation Sigmoid, qui produit une sortie entre 0 et 1. <code>'Tanh'</code> <code>nn.Tanh()</code> Fonction d'activation Tanh, avec une sortie dans l'intervalle [-1, 1]. <code>'Hardtanh'</code> <code>nn.Hardtanh()</code> Variante de Tanh, avec des sorties limit\u00e9es entre une plage sp\u00e9cifi\u00e9e. <code>'Softsign'</code> <code>nn.Softsign()</code> Fonction d'activation similaire \u00e0 Tanh mais plus souple, avec des valeurs dans [-1, 1]."},{"location":"Containers/optims/","title":"NeuralNetworks.optims","text":"Valeurs Module PyTorch Description <code>'Adadelta'</code> <code>optim.Adadelta()</code> Optimiseur bas\u00e9 sur les gradients adaptatifs, sans n\u00e9cessit\u00e9 de r\u00e9glage du taux d'apprentissage. <code>'Adafactor'</code> <code>optim.Adafactor()</code> Optimiseur variant d'Adam avec une mise \u00e0 jour plus efficace de la m\u00e9moire pour de grands mod\u00e8les. <code>'Adam'</code> <code>optim.Adam()</code> Optimiseur utilisant un gradient stochastique adaptatif avec des moyennes mobiles des gradients. <code>'AdamW'</code> <code>optim.AdamW()</code> Optimiseur avec une r\u00e9gularisation L2 (weight decay) distincte. <code>'Adamax'</code> <code>optim.Adamax()</code> Optimiseur utilisant une norme infinie pour les gradients, plus stable pour certaines configurations. <code>'ASGD'</code> <code>optim.ASGD()</code> Optimiseur utilis\u00e9 pour de grandes donn\u00e9es avec une moyenne des gradients. <code>'NAdam'</code> <code>optim.NAdam()</code> Optimiseur avec une adaptation des moments de second ordre. <code>'RAdam'</code> <code>optim.RAdam()</code> Optimiseur qui ajuste dynamiquement les moments pour stabiliser l'entra\u00eenement. <code>'RMSprop'</code> <code>optim.RMSprop()</code> Optimiseur utilisant une moyenne mobile des carr\u00e9s des gradients pour r\u00e9duire les oscillations. <code>'Rprop'</code> <code>optim.Rprop()</code> Optimiseur bas\u00e9 sur les mises \u00e0 jour des poids ind\u00e9pendantes des gradients. <code>'SGD'</code> <code>optim.SGD()</code> Optimiseur souvent utilis\u00e9e avec un taux d'apprentissage constant ou ajust\u00e9."},{"location":"Trainer/Trainer/","title":"NeuralNetworks.Trainer","text":"<p><code>NeuralNetworks.Trainer (*nets, inputs, outputs, init_train_size, final_train_size, optim, init_lr, final_lr, crit, batch_size)</code> [source]</p> <p>Classe pour entra\u00eener des r\u00e9seaux avec mini-batchs et Automatic Mixed Precision.</p> Param\u00e8tres Type Optionnel Description <code>*nets</code> <code>Module</code> Non R\u00e9seaux pour lesquels le trainer va entrainer. <code>inputs</code> <code>torch.Tensor([float])</code> Non Donn\u00e9es en entr\u00e9e au r\u00e9seau. <code>outputs</code> <code>torch.Tensor([float])</code> Non Donn\u00e9es en sortie au r\u00e9seau. <code>init_train_size</code> <code>float</code> Oui Fraction du dataset initiale.  Default: <code>0.01</code> <code>final_train_size</code> <code>float</code> Oui Fraction du dataset finale.  Default: <code>1</code> <code>optim</code> <code>optim</code> Oui Nom de l\u2019optimiseur \u00e0 utiliser (doit exister dans <code>optims()</code>). Default: <code>'Adam'</code> <code>init_lr</code> <code>float</code> Oui Taux d\u2019apprentissage initial pour l\u2019optimiseur. Default: <code>1e-3</code> <code>final_lr</code> <code>float</code> Oui Taux d\u2019apprentissage final pour l\u2019optimiseur. Default: <code>1e-5</code> <code>crit</code> <code>crit</code> Oui Fonction de perte \u00e0 utiliser (doit exister dans <code>crits()</code>). Default: <code>'MSE'</code> <code>batch_size</code> <code>int</code> Oui Taille des minibatchs. Default: <code>1024</code> Initialisation d'un trainer Trainer_exemple.py<pre><code>from NeuralNetworks import Trainer\n\nT = Trainer (\n    net                       , # (1)!\n    inputs           = inputs , # (2)!\n    outputs          = outputs, # (3)!\n    init_train_size  = 0.001  , \n    final_train_size = 1      , \n    init_lr          = 1e-3   , \n    final_lr         = 1e-5   , \n    optim            = 'Adam' , \n    crit             = 'MSE'  , \n    batch_size       = 163840   \n)\n</code></pre> <ol> <li>Voir <code>Module</code></li> <li>La d\u00e9finition de inputs n'est pas explicit\u00e9e ici</li> <li>La d\u00e9finition de outputs n'est pas explicit\u00e9e ici</li> </ol>"},{"location":"Trainer/lr/","title":"Dynamique du learning rate","text":"<p>Le learning rate a une double d\u00e9pendance, il varie en fonction de l'\u00e9poque mais aussi en fonction du r\u00e9sidu de l'\u00e9poque. Cela permet d'avoir un learning rate adapt\u00e9 en fonction de la taille de l'entrainement et qu'il continu \u00e0 apprendre dans le cas ou les erreurs sont grandes.</p> <p>La fusion des deux dynamiques de fait de la mani\u00e8re suivante:</p> \\[ lr = (\\text{init_lr} - \\text{final_lr}) \\cdot \\max (\\text{lr_epoch} , \\text{lr_loss}) - \\text{final_lr} \\] Dynamique en fonction de l'\u00e9poque \\[ \\text{lr_epoch} = \\begin{cases}         1 - \\frac{1}{2} (6 \\cdot (\\frac{epoch}{0.1 \\cdot \\text{Nb_epochs}})^5 - 15 (\\cdot \\frac{epoch}{0.1 \\cdot \\text{Nb_epochs}})^4 + 10 \\cdot (\\frac{epoch}{0.1 \\cdot \\text{Nb_epochs}})^3) \\quad &amp;\\text{si} \\, epoch &lt;= 0.1 \\cdot \\text{Nb_epochs}\\\\         \\frac{1}{2} (1 - 6 \\cdot (\\frac{epoch - 0.1 \\cdot \\text{Nb_epochs}}{0.9 \\cdot \\text{Nb_epochs}})^5 + 15 \\cdot (\\frac{epoch - 0.1 \\cdot \\text{Nb_epochs}}{0.9 \\cdot \\text{Nb_epochs}})^4 - 10 \\cdot (\\frac{epoch - 0.1 \\cdot \\text{Nb_epochs}}{0.9 \\cdot \\text{Nb_epochs}})^3) \\quad &amp;\\text{si} \\, epoch &gt;= 0.1 \\cdot \\text{Nb_epochs}\\\\     \\end{cases} \\] \u00c9chelle lin\u00e9aire\u00c9chelle logarithmique <p> </p> <p> </p> Dynamique en fonction des r\u00e9sidus \\[ \\text{lr_loss} =\\frac{1}{2} (1 + loss^4 + (loss^2 -2 \\cdot loss +1)^4  ) \\] \u00c9chelle lin\u00e9aire\u00c9chelle logarithmique <p> </p> <p> </p>"},{"location":"Trainer/train/","title":"NeuralNetworks.Trainer.train","text":"<p><code>NeuralNetworks.Trainer.train (num_epochs, disable_tqdm, benchmark)</code> [source]</p> <p>Lancement d'un entrainement avec le trainer d\u00e9finit.</p> Param\u00e8tres Type Optionnel Description <code>num_epochs</code> <code>int</code> Oui Nombres d'it\u00e9rations \u00e0 effectuer. Default: <code>1500</code> <code>disable_tqdm</code> <code>boolean</code> Oui D\u00e9sactive la barre de progression. Default: <code>False</code> <code>benchmark</code> <code>boolean</code> Oui Active le mode benchmark. Default: <code>False</code> Lancement d'un entrainement train_exemple.py<pre><code>from NeuralNetworks import Trainer\n\nT = Trainer (\n    net              , # (1)!\n    inputs  = inputs , # (2)!\n    outputs = outputs, # (3)!\n)\n\nT.train (\n    num_epochs   = 1000 ,\n    disable_tqdm = False,\n    benchmark    = False\n)\n</code></pre> <ol> <li>Voir <code>Module</code></li> <li>La d\u00e9finition de inputs n'est pas explicit\u00e9e ici</li> <li>La d\u00e9finition de outputs n'est pas explicit\u00e9e ici</li> </ol>"},{"location":"UI/learnings/","title":"NeuralNetworks.learnings","text":"<p><code>NeuralNetworks.learnings (*nets, fig_size, color)</code> [source]</p> <p>Affiche les taux d'apprentissage en fonction des \u00e9poques d'entrainement des r\u00e9seaux.</p> Param\u00e8tres Type Optionnel Description <code>*nets</code> <code>Module</code> Non R\u00e9seaux pour lesquels afficher les learning rates. <code>fig_size</code> <code>int</code> oui Taille de la figure. <code>color</code> <code>str</code> oui Couleur des axes. Example learnings.py<pre><code>from NeuralNetworks import MLP, Trainer, learnings\n\nhidden_layers = [256,256,256,256,256,256,256,256,256,256]\nnet = MLP(2, hidden_layers, 3)\n\nT = Trainer (net, inputs = inputs, outputs = outputs) # (1)!\nT.train(1500)\n\nlearnings (net, fig_size = 10)\n</code></pre> <ol> <li>La d\u00e9finition de inputs et outputs n'est pas explicit\u00e9e ici</li> </ol> <p> </p>"},{"location":"UI/losses/","title":"NeuralNetworks.losses","text":"<p><code>NeuralNetworks.losses (*nets, fuse_losses, names, fig_size, color)</code> [source]</p> <p>Affiche les r\u00e9sidus en fonction des \u00e9poques d'entrainement des r\u00e9seaux.</p> Param\u00e8tres Type Optionnel Description <code>*nets</code> <code>Module</code> Non R\u00e9seaux pour lesquels afficher les r\u00e9sidus. <code>fuse_losses</code> <code>boolean</code> oui Fusionner les r\u00e9sidus des sorties de chaques r\u00e9seaux en un r\u00e9sidu moyen. <code>names</code> <code>boolean</code> oui Noms des sorties des r\u00e9seaux. <code>fig_size</code> <code>boolean</code> oui Taille de la figure. <code>color</code> <code>boolean</code> oui Couleur des axes. Example Sans fusion des r\u00e9sidusAvec fusion des r\u00e9sidus losses_exemple.py<pre><code>from NeuralNetworks import MLP, Trainer, learnings\n\nhidden_layers = [256,256,256,256,256,256,256,256,256,256]\nnet = MLP(2, hidden_layers, 3)\n\nT = Trainer (net, inputs = inputs, outputs = outputs) # (1)!\nT.train(1500)\n\nlosses (net, fuse_losses = False, names = [\"Sortie 1\", \"Sortie 2\", \"Sortie 3\"], fig_size = 10)\n</code></pre> <ol> <li>La d\u00e9finition de inputs et outputs n'est pas explicit\u00e9e ici</li> </ol> <p> </p> fused_losses_exemple.py<pre><code>from NeuralNetworks import MLP, Trainer, learnings\n\nhidden_layers = [256,256,256,256,256,256,256,256,256,256]\nnet = MLP(2, hidden_layers, 3)\n\nT = Trainer (net, inputs = inputs, outputs = outputs) # (1)!\nT.train(1500)\n\nlosses (net, fuse_losses = True, fig_size = 10)\n</code></pre> <ol> <li>La d\u00e9finition de inputs et outputs n'est pas explicit\u00e9e ici</li> </ol> <p> </p>"},{"location":"dependences/","title":"NeuralNetworks Module test","text":"<p>Module complet pour faciliter la cr\u00e9ation et l'entra\u00eenement de mod\u00e8les pytorch en fournissant des classes pr\u00e9configur\u00e9s.</p> <p>Module complet pour la cr\u00e9ation et l'entra\u00eenement de MultiLayer Perceptrons (MLP) avec encodage optionnel Fourier Features et gestion automatique des pertes.</p>"},{"location":"dependences/#classes","title":"Classes","text":""},{"location":"dependences/#module","title":"Module","text":"Attributs Type Description <code>MLP.losses</code> <code>list[float]</code> Historique des pertes cumul\u00e9es lors de l'entra\u00eenement <code>MLP.learnings</code> <code>list[float]</code> Historique des taux d'apprentissage utilis\u00e9es lors de l'entra\u00eenement <code>MLP.model</code> <code>nn.Sequential</code> MLP complet construit dynamiquement <code>MLP.name</code> <code>str</code> Nom du r\u00e9seau <code>MLP.save</code> <code>method</code> Enregistre le r\u00e9seau dans un format propri\u00e9taire s\u00e9curis\u00e9 <code>MLP.load</code> <code>method</code> Charge un r\u00e9seau du format propri\u00e9taire s\u00e9curis\u00e9, peut continuer l'entrainement <code>MLP.onnx_save</code> <code>method</code> Enregistre le r\u00e9seau dans le format <code>onnx</code>"},{"location":"dependences/#mlp","title":"MLP","text":"<p>inh\u00e9rite des propri\u00e9t\u00e9s de la classe <code>Module</code> </p> <p>Cette classe fournit :</p> <ul> <li>Un MLP enti\u00e8rement configurable (dimensions, activation).</li> <li>Option d'encodage Fourier Features sur les entr\u00e9es.</li> </ul> Param\u00e8tres Type Optionnel Description <code>input_size</code> <code>int</code> Oui Taille des donn\u00e9es en entr\u00e9e au r\u00e9seau. Default: <code>1</code> <code>output_size</code> <code>int</code> Oui Taille des donn\u00e9es en sortie au r\u00e9seau. Default: <code>1</code> <code>hidden_layers</code> <code>list[int]</code> Oui Dimensions successives des couches interm\u00e9diaires du r\u00e9seau. Default: <code>[1]</code> <code>sigmas</code> <code>list[float]</code> Oui Liste de sigma pour encodages RFF. Si None : passthrough. Default: <code>None</code> <code>fourier_input_size</code> <code>int</code> Oui WIP. Default: <code>2</code> <code>nb_fourier</code> <code>int</code> Oui Nombre de fr\u00e9quences utilis\u00e9es pour les Fourier Features. Default: <code>8</code> <code>norm</code> <code>norm</code> Oui Type de normalisation / activation pour les couches cach\u00e9es. Default: <code>'Relu'</code> <code>name</code> <code>str</code> Oui Nom du r\u00e9seau pour identification ou affichage. Default: <code>'Net'</code>"},{"location":"dependences/#exemple-dutilisation","title":"Exemple d'utilisation","text":"<pre><code>from NeuralNetworks import MLP\n\nnet = MLP(\n    input_size    = 2,\n    hidden_layers = [512,512,512,512,512,512,512,512,512,512],\n    output_size   = 3,\n    nb_fourier    = 256,\n    sigmas        = [0.1,1],\n    norm          = \"Relu\",\n    name          = \"Net\"\n)\n</code></pre>"},{"location":"dependences/#vae","title":"VAE","text":"<p>inh\u00e9rite des propri\u00e9t\u00e9s de la classe <code>Module</code> </p> <p>Cette classe fournit :</p>"},{"location":"dependences/#-un-vae-entierement-configurable","title":"- Un VAE enti\u00e8rement configurable.","text":"Param\u00e8tres Type Optionnel Description"},{"location":"dependences/#trainer","title":"Trainer","text":"<p>Classe pour entra\u00eener des r\u00e9seaux avec mini-batchs et Automatic Mixed Precision.</p> Param\u00e8tres Type Optionnel Description <code>*nets</code> <code>Module</code> Non R\u00e9seaux pour lesquels le trainer va entrainer. <code>inputs</code> <code>torch.Tensor([float])</code> Non Donn\u00e9es en entr\u00e9e au r\u00e9seau. <code>outputs</code> <code>torch.Tensor([float])</code> Non Donn\u00e9es en sortie au r\u00e9seau. <code>init_train_size</code> <code>float</code> Oui Fraction du dataset initiale.  Default: <code>0.01</code> <code>final_train_size</code> <code>float</code> Oui Fraction du dataset finale.  Default: <code>1</code> <code>optim</code> <code>optim</code> Oui Nom de l\u2019optimiseur \u00e0 utiliser (doit exister dans <code>optims()</code>). Default: <code>'Adam'</code> <code>init_lr</code> <code>float</code> Oui Taux d\u2019apprentissage initial pour l\u2019optimiseur. Default: <code>1e-3</code> <code>final_lr</code> <code>float</code> Oui Taux d\u2019apprentissage final pour l\u2019optimiseur. Default: <code>1e-5</code> <code>crit</code> <code>crit</code> Oui Fonction de perte \u00e0 utiliser (doit exister dans <code>crits()</code>). Default: <code>'MSE'</code> <code>batch_size</code> <code>int</code> Oui Taille des minibatchs. Default: <code>1024</code>"},{"location":"dependences/#trainertrain","title":"Trainer.train","text":"<p>Lancement d'un entrainement avec le trainer d\u00e9finit.</p> Param\u00e8tres Type Optionnel Description <code>num_epochs</code> <code>int</code> Oui Nombres d'it\u00e9rations \u00e0 effectuer. Default: <code>1500</code> <code>disable_tqdm</code> <code>boolean</code> Oui D\u00e9sactive la barre de progression. Default: <code>False</code> <code>benchmark</code> <code>boolean</code> Oui Active le mode benchmark. Default: <code>False</code>"},{"location":"dependences/#exemple-dutilisation_1","title":"Exemple d'utilisation","text":"<p><pre><code>from NeuralNetworks import Trainer\n\nT = Trainer (\n    net                    , \n    inputs     = inputs    , \n    outputs    = outputs   , \n    init_train_size = 0.001, \n    final_train_size = 1   , \n    init_lr    = 1e-3      , \n    final_lr   = 1e-5      , \n    optim      = 'Adam'    , \n    crit       = 'MSE'     , \n    batch_size = 163840   \n)\n</code></pre> <pre><code>T.train (\n    num_epochs   = 1000 ,\n    disable_tqdm = False,\n    benchmark    = False\n)\n</code></pre></p>"},{"location":"dependences/#methodes","title":"M\u00e9thodes","text":""},{"location":"dependences/#losses","title":"losses","text":"<p>Affiche les r\u00e9sidus en fonction des \u00e9poques d'entrainement des r\u00e9seaux.</p> Param\u00e8tres Type Optionnel Description <code>*nets</code> <code>Module</code> Non R\u00e9seaux pour lesquels afficher les r\u00e9sidus. <code>fuse_losses</code> <code>boolean</code> oui Fusionner les r\u00e9sidus des sorties de chaques r\u00e9seaux en un r\u00e9sidu moyen. <code>names</code> <code>boolean</code> oui Noms des sorties des r\u00e9seaux. <code>fig_size</code> <code>boolean</code> oui Taille de la figure. <code>color</code> <code>boolean</code> oui Couleur des axes. <pre><code>losses (\n    net                     ,\n    fuse_losses = False     ,\n    names       = [\"x\", \"y\"],\n    fig_size    = 5         ,\n    color       = color\n)\n</code></pre>"},{"location":"dependences/#learnings","title":"learnings","text":"<p>Affiche les taux d'apprentissage en fonction des \u00e9poques d'entrainement des r\u00e9seaux.</p> Param\u00e8tres Type Optionnel Description <code>*nets</code> <code>Module</code> Non R\u00e9seaux pour lesquels afficher les learning rates. <code>fig_size</code> <code>int</code> oui Taille de la figure. <code>color</code> <code>str</code> oui Couleur des axes. <pre><code>learnings (\n    net             ,\n    fig_size = 5    ,\n    color    = color\n)\n</code></pre>"},{"location":"dependences/#dictionnaires","title":"Dictionnaires","text":""},{"location":"dependences/#norms","title":"norms","text":"Valeurs Module PyTorch Description <code>'ReLU'</code> <code>nn.ReLU()</code> Fonction d'activation ReLU classique (Rectified Linear Unit). <code>'LeakyReLU'</code> <code>nn.LeakyReLU()</code> ReLU avec un petit coefficient pour les valeurs n\u00e9gatives (param\u00e8tre <code>negative_slope</code>). <code>'ELU'</code> <code>nn.ELU()</code> Fonction d'activation ELU (Exponential Linear Unit), qui a une meilleure gestion des valeurs n\u00e9gatives. <code>'SELU'</code> <code>nn.SELU()</code> SELU (Scaled Exponential Linear Unit), une version am\u00e9lior\u00e9e de l'ELU pour des r\u00e9seaux auto-normalisants. <code>'GELU'</code> <code>nn.GELU()</code> GELU (Gaussian Error Linear Unit), une activation probabiliste bas\u00e9e sur une fonction gaussienne. <code>'Mish'</code> <code>nn.Mish()</code> ReLU diff\u00e9rentiable en tout points avec passage n\u00e9gatif. <code>'Softplus'</code> <code>nn.Softplus()</code> Fonction d'activation qui approxime ReLU mais de mani\u00e8re liss\u00e9e. <code>'Sigmoid'</code> <code>nn.Sigmoid()</code> Fonction d'activation Sigmoid, qui produit une sortie entre 0 et 1. <code>'Tanh'</code> <code>nn.Tanh()</code> Fonction d'activation Tanh, avec une sortie dans l'intervalle [-1, 1]. <code>'Hardtanh'</code> <code>nn.Hardtanh()</code> Variante de Tanh, avec des sorties limit\u00e9es entre une plage sp\u00e9cifi\u00e9e. <code>'Softsign'</code> <code>nn.Softsign()</code> Fonction d'activation similaire \u00e0 Tanh mais plus souple, avec des valeurs dans [-1, 1]."},{"location":"dependences/#optims","title":"optims","text":"Valeurs Module PyTorch Description <code>'Adadelta'</code> <code>optim.Adadelta()</code> Optimiseur bas\u00e9 sur les gradients adaptatifs, sans n\u00e9cessit\u00e9 de r\u00e9glage du taux d'apprentissage. <code>'Adafactor'</code> <code>optim.Adafactor()</code> Optimiseur variant d'Adam avec une mise \u00e0 jour plus efficace de la m\u00e9moire pour de grands mod\u00e8les. <code>'Adam'</code> <code>optim.Adam()</code> Optimiseur utilisant un gradient stochastique adaptatif avec des moyennes mobiles des gradients. <code>'AdamW'</code> <code>optim.AdamW()</code> Optimiseur avec une r\u00e9gularisation L2 (weight decay) distincte. <code>'Adamax'</code> <code>optim.Adamax()</code> Optimiseur utilisant une norme infinie pour les gradients, plus stable pour certaines configurations. <code>'ASGD'</code> <code>optim.ASGD()</code> Optimiseur utilis\u00e9 pour de grandes donn\u00e9es avec une moyenne des gradients. <code>'NAdam'</code> <code>optim.NAdam()</code> Optimiseur avec une adaptation des moments de second ordre. <code>'RAdam'</code> <code>optim.RAdam()</code> Optimiseur qui ajuste dynamiquement les moments pour stabiliser l'entra\u00eenement. <code>'RMSprop'</code> <code>optim.RMSprop()</code> Optimiseur utilisant une moyenne mobile des carr\u00e9s des gradients pour r\u00e9duire les oscillations. <code>'Rprop'</code> <code>optim.Rprop()</code> Optimiseur bas\u00e9 sur les mises \u00e0 jour des poids ind\u00e9pendantes des gradients. <code>'SGD'</code> <code>optim.SGD()</code> Optimiseur souvent utilis\u00e9e avec un taux d'apprentissage constant ou ajust\u00e9."},{"location":"dependences/#crits","title":"crits","text":"Valeurs Module PyTorch Description <code>'MSE'</code> <code>nn.MSELoss</code> Perte utilis\u00e9e pour les r\u00e9gressions. <code>'L1'</code> <code>nn.L1Loss()</code> Perte utilis\u00e9e pour la r\u00e9gularisation. <code>'SmoothL1'</code> <code>nn.SmoothL1Loss()</code> Perte moins sensible aux outliers. <code>'Huber'</code> <code>nn.HuberLoss()</code> Perte moins affect\u00e9e par les grands \u00e9carts. <code>'CrossEntropy'</code> <code>nn.CrossEntropyLoss()</code> Perte utilis\u00e9e pour les probl\u00e8mes de classification multi-classes. <code>'KLDiv'</code> <code>nn.KLDivLoss()</code> Perte utilis\u00e9e pour des mod\u00e8les probabilistes. <code>'PoissonNLL'</code> <code>nn.PoissonNLLLoss()</code> Perte utilis\u00e9e pour la mod\u00e9lisation de comptages. <code>'MultiLabelSoftMargin'</code> <code>nn.MultiLabelSoftMarginLoss()</code> Perte utilis\u00e9e pour les probl\u00e8mes de classification multi-\u00e9tiquettes."},{"location":"dependences/#device","title":"device","text":"<p>Variable principale d'allocation des performances.</p>"},{"location":"dependences/#apple-silicon-macos","title":"Apple Silicon (macOS)","text":"<ul> <li>Si le syst\u00e8me d'exploitation est macOS (nomm\u00e9 <code>darwin</code> dans <code>platform.system()</code>), la fonction v\u00e9rifie si l'acc\u00e9l\u00e9rateur Metal Performance Shaders (MPS) est disponible sur l'appareil.</li> <li>Si MPS est disponible (<code>torch.backends.mps.is_available()</code>), l'appareil cible sera d\u00e9fini sur <code>'mps'</code> (c'est un \u00e9quivalent de CUDA pour les appareils Apple Silicon).</li> </ul>"},{"location":"dependences/#windows","title":"Windows","text":"<ul> <li>Si le syst\u00e8me d'exploitation est Windows, la fonction v\u00e9rifie d'abord si CUDA (NVIDIA) est disponible avec <code>torch.cuda.is_available()</code>. Si c'est le cas, le p\u00e9riph\u00e9rique sera d\u00e9fini sur CUDA.</li> </ul>"},{"location":"dependences/#linux","title":"Linux","text":"<ul> <li>Si le syst\u00e8me d'exploitation est Linux, plusieurs v\u00e9rifications sont effectu\u00e9es :</li> <li>CUDA (NVIDIA) : Si <code>torch.cuda.is_available()</code> renvoie <code>True</code>, le p\u00e9riph\u00e9rique sera d\u00e9fini sur <code>'cuda'</code>.</li> <li>ROCm (AMD) : Si le syst\u00e8me supporte ROCm via <code>torch.backends.hip.is_available()</code>, l'appareil sera d\u00e9fini sur <code>'cuda'</code> (ROCm est utilis\u00e9 pour les cartes AMD dans le cadre de l'API CUDA).</li> <li>Intel oneAPI / XPU : Si le syst\u00e8me prend en charge Intel oneAPI ou XPU via <code>torch.xpu.is_available()</code>, le p\u00e9riph\u00e9rique sera d\u00e9fini sur XPU.</li> </ul>"},{"location":"dependences/#systeme-non-reconnu","title":"Syst\u00e8me non reconnu","text":"<ul> <li>Si aucune des conditions ci-dessus n'est remplie, la fonction retourne <code>'cpu'</code> comme p\u00e9riph\u00e9rique par d\u00e9faut.</li> </ul>"},{"location":"module/mlp/","title":"NeuralNetworks.MLP","text":"<p><code>NeuralNetworks.MLP (input_size, output_size, hidden_layers, sigmas, fourier_input_size, nb_fourier, norm, name)</code> [source]</p> <p>inh\u00e9rite des propri\u00e9t\u00e9s de la classe <code>Module</code> </p> <p>Cette classe fournit :</p> <ul> <li>Un MLP enti\u00e8rement configurable (dimensions, activation).</li> <li>Option d'encodage Fourier Features sur les entr\u00e9es.</li> </ul> <p>Permet de construire rapidement un r\u00e9seau de neurones multicouches rapidement sans connaisances profondes de pytorch.</p>"},{"location":"module/mlp/#description-des-parametres","title":"Description des param\u00e8tres","text":"Param\u00e8tres Type Optionnel Default value <code>input_size</code> <code>int</code> Oui <code>1</code> <code>output_size</code> <code>int</code> Oui <code>1</code> <code>hidden_layers</code> <code>list[int]</code> Oui <code>[1]</code> <code>sigmas</code> <code>list[float]</code> Oui <code>None</code> <code>fourier_input_size</code> <code>int</code> Oui <code>2</code> <code>nb_fourier</code> <code>int</code> Oui <code>8</code> <code>norm</code> <code>norm</code> Oui <code>'Relu'</code> <code>name</code> <code>str</code> Oui <code>'Net'</code> Example MLP_exemple.py<pre><code>from NeuralNetworks import MLP\n\nnet = MLP(\n    input_size    = 2,\n    hidden_layers = [512,512,512,512,512,512,512,512,512,512],\n    output_size   = 3,\n    nb_fourier    = 256,\n    sigmas        = [0.1,1],\n    norm          = \"Relu\",\n    name          = \"Net\"\n)\n</code></pre>"},{"location":"module/module/","title":"NeuralNetworks.Module","text":"<p><code>NeuralNetworks.Module (_name, **Reconstruction_data)</code> [source]</p> <p>Module complet pour faciliter la cr\u00e9ation et l'entra\u00eenement de mod\u00e8les pytorch en fournissant des classes pr\u00e9configur\u00e9es.</p> sous-classes Description <code>MLP</code> Classe pr\u00e9fabriqu\u00e9e de Multilayer Perceptron <code>VAE</code> Classe pr\u00e9fabriqu\u00e9e de Variational Autoencoder"},{"location":"module/module/#attributs","title":"Attributs","text":"Attributs Type Description <code>MLP.losses</code> <code>list[float]</code> Historique des pertes cumul\u00e9es lors de l'entra\u00eenement <code>MLP.learnings</code> <code>list[float]</code> Historique des taux d'apprentissage utilis\u00e9es lors de l'entra\u00eenement <code>MLP.model</code> <code>nn.Sequential</code> MLP complet construit dynamiquement <code>MLP.name</code> <code>str</code> Nom du r\u00e9seau <code>MLP.save</code> <code>method</code> Enregistre le r\u00e9seau dans un format propri\u00e9taire s\u00e9curis\u00e9 <code>MLP.load</code> <code>method</code> Charge un r\u00e9seau du format propri\u00e9taire s\u00e9curis\u00e9, peut continuer l'entrainement <code>MLP.onnx_save</code> <code>method</code> Enregistre le r\u00e9seau dans le format <code>onnx</code>"},{"location":"module/vae/","title":"NeuralNetworks.VAE","text":"<p>inh\u00e9rite des propri\u00e9t\u00e9s de la classe <code>Module</code> </p> <p>Cette classe fournit :</p>"},{"location":"module/vae/#-un-vae-entierement-configurable","title":"- Un VAE enti\u00e8rement configurable.","text":"Param\u00e8tres Type Optionnel Description"}]}