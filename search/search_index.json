{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"NeuralNetworks Module \u00b6 Module complet pour la cr\u00e9ation, l'entra\u00eenement et la visualisation de Multi-Layer Perceptrons (MLP) avec encodage optionnel Fourier, gestion automatique des pertes, compilation Torch et outils de traitement d'images pour l'apprentissage sur des images RGB. Contenu principal \u00b6 Classes \u00b6 MLP \u00b6 Multi-Layer Perceptron (MLP) avec options avanc\u00e9es : Encodage Fourier gaussien (RFF) optionnel Stockage automatique des pertes Compilation Torch optionnelle pour acc\u00e9l\u00e9rer l\u2019inf\u00e9rence Gestion flexible de l\u2019optimiseur, de la fonction de perte et de la normalisation M\u00e9thodes principales : MLP(layers, learning_rate, Fourier, optim, crit, norm, name, Iscompiled) Initialise le r\u00e9seau avec toutes les options. Les valeurs possibles de optim sont disponibles avec optims() Les valeurs possibles de crit sont disponibles avec crits() Les valeurs possibles de norm sont disponibles avec norms() train(inputs, outputs, num_epochs, batch_size) Entra\u00eene le MLP sur des donn\u00e9es ( inputs \u2192 outputs ) en utilisant AMP et mini-batchs. plot(inputs, img_array) Affiche l'image originale, la pr\u00e9diction du MLP et la courbe des pertes. params() Retourne tous les poids du MLP (ligne par ligne) sous forme de liste de numpy.ndarray . nb_params() Calcule le nombre total de poids dans le MLP. neurons() Retourne la liste des biais (neurones) de toutes les couches lin\u00e9aires. Fonctions utilitaires \u00b6 tensorise(obj) Convertit un objet array-like ou tensor en torch.Tensor float32 sur le device actif. rglen(list) Renvoie un range correspondant aux indices d'une liste. image_from_url(url, img_size) T\u00e9l\u00e9charge une image depuis une URL, la redimensionne et g\u00e9n\u00e8re : img_array : np.ndarray (H, W, 3) pour affichage. inputs : tenseur (H*W, 2) coordonn\u00e9es normalis\u00e9es. outputs : tenseur (H*W, 3) valeurs RGB cibles. Visualisation et comparaison \u00b6 plot(img_array, inputs, *nets) Affiche pour chaque r\u00e9seau l'image reconstruite \u00e0 partir des entr\u00e9es. compare(img_array, inputs, *nets) Affiche pour chaque r\u00e9seau l'erreur absolue entre l'image originale et la pr\u00e9diction, et trace \u00e9galement les pertes cumul\u00e9es. Chaque r\u00e9seau doit poss\u00e9der : Objets et dictionnaires \u00b6 norms() \u00b6 Valeurs Module PyTorch Description \"Relu\" nn.ReLU() Fonction d'activation ReLU classique (Rectified Linear Unit). \"LeakyRelu\" nn.LeakyReLU() ReLU avec un petit coefficient pour les valeurs n\u00e9gatives (param\u00e8tre negative_slope ). \"ELU\" nn.ELU() Fonction d'activation ELU (Exponential Linear Unit), qui a une meilleure gestion des valeurs n\u00e9gatives. \"SELU\" nn.SELU() SELU (Scaled Exponential Linear Unit), une version am\u00e9lior\u00e9e de l'ELU pour des r\u00e9seaux auto-normalisants. \"GELU\" nn.GELU() GELU (Gaussian Error Linear Unit), une activation probabiliste bas\u00e9e sur une fonction gaussienne. \"Sigmoid\" nn.Sigmoid() Fonction d'activation Sigmoid, qui produit une sortie entre 0 et 1. \"Tanh\" nn.Tanh() Fonction d'activation Tanh, avec une sortie dans l'intervalle [-1, 1]. \"Hardtanh\" nn.Hardtanh() Variante de Tanh, avec des sorties limit\u00e9es entre une plage sp\u00e9cifi\u00e9e. \"Softplus\" nn.Softplus() Fonction d'activation qui approxime ReLU mais de mani\u00e8re liss\u00e9e. \"Softsign\" nn.Softsign() Fonction d'activation similaire \u00e0 Tanh mais plus souple, avec des valeurs dans [-1, 1]. crits() \u00b6 Valeurs Module PyTorch Description \"MSE\" nn.MSELoss() Mean Squared Error Loss, utilis\u00e9e pour les r\u00e9gressions. \"L1\" nn.L1Loss() L1 Loss (erreur absolue), souvent utilis\u00e9e pour la r\u00e9gularisation. \"SmoothL1\" nn.SmoothL1Loss() Smooth L1 Loss, une combinaison de L1 et de MSE, moins sensible aux outliers. \"Huber\" nn.HuberLoss() Fonction de perte Huber, une version liss\u00e9e de L1 et MSE, moins affect\u00e9e par les grands \u00e9carts. \"CrossEntropy\" nn.CrossEntropyLoss() Perte de Cross-Entropy, utilis\u00e9e pour les probl\u00e8mes de classification multi-classes. \"KLDiv\" nn.KLDivLoss() Perte de divergence de Kullback-Leibler, souvent utilis\u00e9e pour des mod\u00e8les probabilistes. \"PoissonNLL\" nn.PoissonNLLLoss() Perte de log-vraisemblance pour une distribution de Poisson, utilis\u00e9e pour la mod\u00e9lisation de comptages. \"MultiLabelSoftMargin\" nn.MultiLabelSoftMarginLoss() Perte utilis\u00e9e pour les probl\u00e8mes de classification multi-\u00e9tiquettes. optims() \u00b6 Valeurs Module PyTorch Description \"Adadelta\" optim.Adadelta() Optimiseur Adadelta, bas\u00e9 sur les gradients adaptatifs, sans n\u00e9cessit\u00e9 de r\u00e9glage du taux d'apprentissage. \"Adafactor\" optim.Adafactor() Optimiseur Adafactor, variant d'Adam avec une mise \u00e0 jour plus efficace de la m\u00e9moire pour de grands mod\u00e8les. \"Adam\" optim.Adam() Optimiseur Adam, utilisant un gradient stochastique adaptatif avec des moyennes mobiles des gradients et des carr\u00e9s des gradients. \"AdamW\" optim.AdamW() Optimiseur Adam avec une r\u00e9gularisation L2 (weight decay) distincte, plus efficace que Adam avec weight_decay . \"Adamax\" optim.Adamax() Version d'Adam utilisant une norme infinie pour les gradients, plus stable pour certaines configurations. \"ASGD\" optim.ASGD() Optimiseur ASGD (Averaged Stochastic Gradient Descent), utilis\u00e9 pour de grandes donn\u00e9es avec une moyenne des gradients. \"NAdam\" optim.NAdam() Optimiseur NAdam, une version am\u00e9lior\u00e9e d'Adam avec une adaptation des moments de second ordre. \"RAdam\" optim.RAdam() Optimiseur RAdam, une version robuste de l'Adam qui ajuste dynamiquement les moments pour stabiliser l'entra\u00eenement. \"RMSprop\" optim.RMSprop() Optimiseur RMSprop, utilisant une moyenne mobile des carr\u00e9s des gradients pour r\u00e9duire les oscillations. \"Rprop\" optim.Rprop() Optimiseur Rprop, bas\u00e9 sur les mises \u00e0 jour des poids ind\u00e9pendantes des gradients. \"SGD\" optim.SGD() Descente de gradient stochastique classique, souvent utilis\u00e9e avec un taux d'apprentissage constant ou ajust\u00e9. Device et configuration \u00b6 Apple Silicon (macOS) \u00b6 Si le syst\u00e8me d'exploitation est macOS (nomm\u00e9 darwin dans platform.system() ), la fonction v\u00e9rifie si l'acc\u00e9l\u00e9rateur Metal Performance Shaders (MPS) est disponible sur l'appareil. Si MPS est disponible ( torch.backends.mps.is_available() ), l'appareil cible sera d\u00e9fini sur MPS (c'est un \u00e9quivalent de CUDA pour les appareils Apple Silicon). Windows \u00b6 Si le syst\u00e8me d'exploitation est Windows, la fonction v\u00e9rifie d'abord si CUDA (NVIDIA) est disponible avec torch.cuda.is_available() . Si c'est le cas, le p\u00e9riph\u00e9rique sera d\u00e9fini sur CUDA . Linux \u00b6 Si le syst\u00e8me d'exploitation est Linux, plusieurs v\u00e9rifications sont effectu\u00e9es : CUDA (NVIDIA) : Si torch.cuda.is_available() renvoie True , le p\u00e9riph\u00e9rique sera d\u00e9fini sur CUDA . ROCm (AMD) : Si le syst\u00e8me supporte ROCm via torch.backends.hip.is_available() , l'appareil sera d\u00e9fini sur CUDA (ROCm est utilis\u00e9 pour les cartes AMD dans le cadre de l'API CUDA). Intel oneAPI / XPU : Si le syst\u00e8me prend en charge Intel oneAPI ou XPU via torch.xpu.is_available() , le p\u00e9riph\u00e9rique sera d\u00e9fini sur XPU . Syst\u00e8me non reconnu \u00b6 Si aucune des conditions ci-dessus n'est remplie, la fonction retourne CPU comme p\u00e9riph\u00e9rique par d\u00e9faut. Param\u00e8tres matplotlib et PyTorch \u00b6 Style global pour fond transparent et texte gris Optimisations CUDA activ\u00e9es pour TF32, matmul et convolutions Autograd configur\u00e9 pour privil\u00e9gier les performances Notes g\u00e9n\u00e9rales \u00b6 Toutes les m\u00e9thodes de MLP utilisent les tenseurs sur le device global (CPU ou GPU) Les images doivent \u00eatre normalis\u00e9es entre 0 et 1 Les fonctions interactives ( plot , compare ) utilisent matplotlib en mode interactif Le module est con\u00e7u pour fonctionner dans Jupyter et scripts Python classiques","title":"Accueil"},{"location":"#neuralnetworks-module","text":"Module complet pour la cr\u00e9ation, l'entra\u00eenement et la visualisation de Multi-Layer Perceptrons (MLP) avec encodage optionnel Fourier, gestion automatique des pertes, compilation Torch et outils de traitement d'images pour l'apprentissage sur des images RGB.","title":"NeuralNetworks Module"},{"location":"#contenu-principal","text":"","title":"Contenu principal"},{"location":"#classes","text":"","title":"Classes"},{"location":"#mlp","text":"Multi-Layer Perceptron (MLP) avec options avanc\u00e9es : Encodage Fourier gaussien (RFF) optionnel Stockage automatique des pertes Compilation Torch optionnelle pour acc\u00e9l\u00e9rer l\u2019inf\u00e9rence Gestion flexible de l\u2019optimiseur, de la fonction de perte et de la normalisation M\u00e9thodes principales : MLP(layers, learning_rate, Fourier, optim, crit, norm, name, Iscompiled) Initialise le r\u00e9seau avec toutes les options. Les valeurs possibles de optim sont disponibles avec optims() Les valeurs possibles de crit sont disponibles avec crits() Les valeurs possibles de norm sont disponibles avec norms() train(inputs, outputs, num_epochs, batch_size) Entra\u00eene le MLP sur des donn\u00e9es ( inputs \u2192 outputs ) en utilisant AMP et mini-batchs. plot(inputs, img_array) Affiche l'image originale, la pr\u00e9diction du MLP et la courbe des pertes. params() Retourne tous les poids du MLP (ligne par ligne) sous forme de liste de numpy.ndarray . nb_params() Calcule le nombre total de poids dans le MLP. neurons() Retourne la liste des biais (neurones) de toutes les couches lin\u00e9aires.","title":"MLP"},{"location":"#fonctions-utilitaires","text":"tensorise(obj) Convertit un objet array-like ou tensor en torch.Tensor float32 sur le device actif. rglen(list) Renvoie un range correspondant aux indices d'une liste. image_from_url(url, img_size) T\u00e9l\u00e9charge une image depuis une URL, la redimensionne et g\u00e9n\u00e8re : img_array : np.ndarray (H, W, 3) pour affichage. inputs : tenseur (H*W, 2) coordonn\u00e9es normalis\u00e9es. outputs : tenseur (H*W, 3) valeurs RGB cibles.","title":"Fonctions utilitaires"},{"location":"#visualisation-et-comparaison","text":"plot(img_array, inputs, *nets) Affiche pour chaque r\u00e9seau l'image reconstruite \u00e0 partir des entr\u00e9es. compare(img_array, inputs, *nets) Affiche pour chaque r\u00e9seau l'erreur absolue entre l'image originale et la pr\u00e9diction, et trace \u00e9galement les pertes cumul\u00e9es. Chaque r\u00e9seau doit poss\u00e9der :","title":"Visualisation et comparaison"},{"location":"#objets-et-dictionnaires","text":"","title":"Objets et dictionnaires"},{"location":"#norms","text":"Valeurs Module PyTorch Description \"Relu\" nn.ReLU() Fonction d'activation ReLU classique (Rectified Linear Unit). \"LeakyRelu\" nn.LeakyReLU() ReLU avec un petit coefficient pour les valeurs n\u00e9gatives (param\u00e8tre negative_slope ). \"ELU\" nn.ELU() Fonction d'activation ELU (Exponential Linear Unit), qui a une meilleure gestion des valeurs n\u00e9gatives. \"SELU\" nn.SELU() SELU (Scaled Exponential Linear Unit), une version am\u00e9lior\u00e9e de l'ELU pour des r\u00e9seaux auto-normalisants. \"GELU\" nn.GELU() GELU (Gaussian Error Linear Unit), une activation probabiliste bas\u00e9e sur une fonction gaussienne. \"Sigmoid\" nn.Sigmoid() Fonction d'activation Sigmoid, qui produit une sortie entre 0 et 1. \"Tanh\" nn.Tanh() Fonction d'activation Tanh, avec une sortie dans l'intervalle [-1, 1]. \"Hardtanh\" nn.Hardtanh() Variante de Tanh, avec des sorties limit\u00e9es entre une plage sp\u00e9cifi\u00e9e. \"Softplus\" nn.Softplus() Fonction d'activation qui approxime ReLU mais de mani\u00e8re liss\u00e9e. \"Softsign\" nn.Softsign() Fonction d'activation similaire \u00e0 Tanh mais plus souple, avec des valeurs dans [-1, 1].","title":"norms()"},{"location":"#crits","text":"Valeurs Module PyTorch Description \"MSE\" nn.MSELoss() Mean Squared Error Loss, utilis\u00e9e pour les r\u00e9gressions. \"L1\" nn.L1Loss() L1 Loss (erreur absolue), souvent utilis\u00e9e pour la r\u00e9gularisation. \"SmoothL1\" nn.SmoothL1Loss() Smooth L1 Loss, une combinaison de L1 et de MSE, moins sensible aux outliers. \"Huber\" nn.HuberLoss() Fonction de perte Huber, une version liss\u00e9e de L1 et MSE, moins affect\u00e9e par les grands \u00e9carts. \"CrossEntropy\" nn.CrossEntropyLoss() Perte de Cross-Entropy, utilis\u00e9e pour les probl\u00e8mes de classification multi-classes. \"KLDiv\" nn.KLDivLoss() Perte de divergence de Kullback-Leibler, souvent utilis\u00e9e pour des mod\u00e8les probabilistes. \"PoissonNLL\" nn.PoissonNLLLoss() Perte de log-vraisemblance pour une distribution de Poisson, utilis\u00e9e pour la mod\u00e9lisation de comptages. \"MultiLabelSoftMargin\" nn.MultiLabelSoftMarginLoss() Perte utilis\u00e9e pour les probl\u00e8mes de classification multi-\u00e9tiquettes.","title":"crits()"},{"location":"#optims","text":"Valeurs Module PyTorch Description \"Adadelta\" optim.Adadelta() Optimiseur Adadelta, bas\u00e9 sur les gradients adaptatifs, sans n\u00e9cessit\u00e9 de r\u00e9glage du taux d'apprentissage. \"Adafactor\" optim.Adafactor() Optimiseur Adafactor, variant d'Adam avec une mise \u00e0 jour plus efficace de la m\u00e9moire pour de grands mod\u00e8les. \"Adam\" optim.Adam() Optimiseur Adam, utilisant un gradient stochastique adaptatif avec des moyennes mobiles des gradients et des carr\u00e9s des gradients. \"AdamW\" optim.AdamW() Optimiseur Adam avec une r\u00e9gularisation L2 (weight decay) distincte, plus efficace que Adam avec weight_decay . \"Adamax\" optim.Adamax() Version d'Adam utilisant une norme infinie pour les gradients, plus stable pour certaines configurations. \"ASGD\" optim.ASGD() Optimiseur ASGD (Averaged Stochastic Gradient Descent), utilis\u00e9 pour de grandes donn\u00e9es avec une moyenne des gradients. \"NAdam\" optim.NAdam() Optimiseur NAdam, une version am\u00e9lior\u00e9e d'Adam avec une adaptation des moments de second ordre. \"RAdam\" optim.RAdam() Optimiseur RAdam, une version robuste de l'Adam qui ajuste dynamiquement les moments pour stabiliser l'entra\u00eenement. \"RMSprop\" optim.RMSprop() Optimiseur RMSprop, utilisant une moyenne mobile des carr\u00e9s des gradients pour r\u00e9duire les oscillations. \"Rprop\" optim.Rprop() Optimiseur Rprop, bas\u00e9 sur les mises \u00e0 jour des poids ind\u00e9pendantes des gradients. \"SGD\" optim.SGD() Descente de gradient stochastique classique, souvent utilis\u00e9e avec un taux d'apprentissage constant ou ajust\u00e9.","title":"optims()"},{"location":"#device-et-configuration","text":"","title":"Device et configuration"},{"location":"#apple-silicon-macos","text":"Si le syst\u00e8me d'exploitation est macOS (nomm\u00e9 darwin dans platform.system() ), la fonction v\u00e9rifie si l'acc\u00e9l\u00e9rateur Metal Performance Shaders (MPS) est disponible sur l'appareil. Si MPS est disponible ( torch.backends.mps.is_available() ), l'appareil cible sera d\u00e9fini sur MPS (c'est un \u00e9quivalent de CUDA pour les appareils Apple Silicon).","title":"Apple Silicon (macOS)"},{"location":"#windows","text":"Si le syst\u00e8me d'exploitation est Windows, la fonction v\u00e9rifie d'abord si CUDA (NVIDIA) est disponible avec torch.cuda.is_available() . Si c'est le cas, le p\u00e9riph\u00e9rique sera d\u00e9fini sur CUDA .","title":"Windows"},{"location":"#linux","text":"Si le syst\u00e8me d'exploitation est Linux, plusieurs v\u00e9rifications sont effectu\u00e9es : CUDA (NVIDIA) : Si torch.cuda.is_available() renvoie True , le p\u00e9riph\u00e9rique sera d\u00e9fini sur CUDA . ROCm (AMD) : Si le syst\u00e8me supporte ROCm via torch.backends.hip.is_available() , l'appareil sera d\u00e9fini sur CUDA (ROCm est utilis\u00e9 pour les cartes AMD dans le cadre de l'API CUDA). Intel oneAPI / XPU : Si le syst\u00e8me prend en charge Intel oneAPI ou XPU via torch.xpu.is_available() , le p\u00e9riph\u00e9rique sera d\u00e9fini sur XPU .","title":"Linux"},{"location":"#systeme-non-reconnu","text":"Si aucune des conditions ci-dessus n'est remplie, la fonction retourne CPU comme p\u00e9riph\u00e9rique par d\u00e9faut.","title":"Syst\u00e8me non reconnu"},{"location":"#parametres-matplotlib-et-pytorch","text":"Style global pour fond transparent et texte gris Optimisations CUDA activ\u00e9es pour TF32, matmul et convolutions Autograd configur\u00e9 pour privil\u00e9gier les performances","title":"Param\u00e8tres matplotlib et PyTorch"},{"location":"#notes-generales","text":"Toutes les m\u00e9thodes de MLP utilisent les tenseurs sur le device global (CPU ou GPU) Les images doivent \u00eatre normalis\u00e9es entre 0 et 1 Les fonctions interactives ( plot , compare ) utilisent matplotlib en mode interactif Le module est con\u00e7u pour fonctionner dans Jupyter et scripts Python classiques","title":"Notes g\u00e9n\u00e9rales"},{"location":"Image/image_from_url/","text":"image_from_url \u00b6 T\u00e9l\u00e9charge une image depuis une URL, la redimensionne et pr\u00e9pare les donn\u00e9es pour l'entra\u00eenement d'un MLP pixel-wise. Cette fonction retourne\u202f: img_array : image RGB sous forme de tableau NumPy (H, W, 3), pour affichage. inputs : coordonn\u00e9es normalis\u00e9es (x, y) de chaque pixel, sous forme de tenseur (H*W, 2). outputs : valeurs RGB cibles pour chaque pixel, sous forme de tenseur (H*W, 3). Param\u00e8tres \u00b6 Param\u00e8tre Type Description url str URL de l'image \u00e0 t\u00e9l\u00e9charger. img_size int Taille finale carr\u00e9e de l'image (img_size x img_size). Par d\u00e9faut 256. Retours \u00b6 Nom Type Description img_array numpy.ndarray (H, W, 3) Image sous forme de tableau NumPy, valeurs normalis\u00e9es entre 0 et 1. inputs torch.Tensor (H*W, 2) Coordonn\u00e9es normalis\u00e9es des pixels pour l'entr\u00e9e du MLP. outputs torch.Tensor (H*W, 3) Valeurs RGB cibles pour chaque pixel, pour la sortie du MLP. Notes \u00b6 Utilise PIL pour le traitement de l'image et torchvision.transforms pour la conversion en tenseur normalis\u00e9. Les coordonn\u00e9es sont normalis\u00e9es dans [0, 1] pour une utilisation optimale avec des MLP utilisant Fourier Features ou activations standard. Les tenseurs inputs et outputs sont pr\u00eats \u00e0 \u00eatre envoy\u00e9s sur GPU si n\u00e9cessaire.","title":"image_from_url"},{"location":"Image/image_from_url/#image_from_url","text":"T\u00e9l\u00e9charge une image depuis une URL, la redimensionne et pr\u00e9pare les donn\u00e9es pour l'entra\u00eenement d'un MLP pixel-wise. Cette fonction retourne\u202f: img_array : image RGB sous forme de tableau NumPy (H, W, 3), pour affichage. inputs : coordonn\u00e9es normalis\u00e9es (x, y) de chaque pixel, sous forme de tenseur (H*W, 2). outputs : valeurs RGB cibles pour chaque pixel, sous forme de tenseur (H*W, 3).","title":"image_from_url"},{"location":"Image/image_from_url/#parametres","text":"Param\u00e8tre Type Description url str URL de l'image \u00e0 t\u00e9l\u00e9charger. img_size int Taille finale carr\u00e9e de l'image (img_size x img_size). Par d\u00e9faut 256.","title":"Param\u00e8tres"},{"location":"Image/image_from_url/#retours","text":"Nom Type Description img_array numpy.ndarray (H, W, 3) Image sous forme de tableau NumPy, valeurs normalis\u00e9es entre 0 et 1. inputs torch.Tensor (H*W, 2) Coordonn\u00e9es normalis\u00e9es des pixels pour l'entr\u00e9e du MLP. outputs torch.Tensor (H*W, 3) Valeurs RGB cibles pour chaque pixel, pour la sortie du MLP.","title":"Retours"},{"location":"Image/image_from_url/#notes","text":"Utilise PIL pour le traitement de l'image et torchvision.transforms pour la conversion en tenseur normalis\u00e9. Les coordonn\u00e9es sont normalis\u00e9es dans [0, 1] pour une utilisation optimale avec des MLP utilisant Fourier Features ou activations standard. Les tenseurs inputs et outputs sont pr\u00eats \u00e0 \u00eatre envoy\u00e9s sur GPU si n\u00e9cessaire.","title":"Notes"},{"location":"MLP/","text":"Multi-Layer Perceptron (MLP) \u00b6 Multi-Layer Perceptron (MLP) avec encodage optionnel Fourier (RFF), suivi automatique des pertes, visualisation et compilation PyTorch. Cette classe fournit : Un MLP enti\u00e8rement configurable (dimensions, normalisation, activation) Option d'encodage Fourier (Random Fourier Features) sur les entr\u00e9es M\u00e9thodes pour entra\u00eener le r\u00e9seau avec mini-batchs et AMP (Automatic Mixed Precision) Visualisation de l'architecture via visualtorch Suivi et affichage de la perte d'entra\u00eenement Acc\u00e8s aux poids, biais et nombre de param\u00e8tres Compilation du mod\u00e8le via torch.compile pour acc\u00e9l\u00e9rer l'inf\u00e9rence Parameters \u00b6 Parameter Type Optional Description layers list[int] Yes Dimensions successives du r\u00e9seau (entr\u00e9e \u2192 couches cach\u00e9es \u2192 sortie). Exemple : [in_features, hidden1, hidden2, ..., out_features] . Default: [1, 1, 1] learning_rate float Yes Taux d\u2019apprentissage pour l\u2019optimiseur. Default: 1e-3 Fourier bool Yes Si True, applique un encodage Fourier gaussien (RFF) sur les entr\u00e9es. Default: True optim str Yes Nom de l\u2019optimiseur \u00e0 utiliser (doit exister dans optims() ). Default: \"Adam\" crit str Yes Fonction de perte \u00e0 utiliser (doit exister dans crits() ). Default: \"MSE\" norm str Yes Type de normalisation / activation pour les couches cach\u00e9es (ex: \"Relu\" ). Default: \"Relu\" name str Yes Nom du r\u00e9seau pour identification ou affichage. Default: \"Net\" Iscompiled bool Yes Si True, compile le mod\u00e8le via torch.compile pour acc\u00e9l\u00e9rer l\u2019inf\u00e9rence. Default: True Attributes \u00b6 losses : list[torch.Tensor] \u2014 Historique des pertes cumul\u00e9es lors de l'entra\u00eenement layers : list[int] \u2014 Dimensions du r\u00e9seau, ajust\u00e9es si encodage Fourier actif encoding : nn.Module \u2014 Module appliquant l'encodage des entr\u00e9es (RFF ou identity) norm : nn.Module \u2014 Normalisation ou activation utilis\u00e9e dans les couches cach\u00e9es crit : nn.Module \u2014 Fonction de perte PyTorch sur le device sp\u00e9cifi\u00e9 model : nn.Sequential \u2014 MLP complet construit dynamiquement optim : torch.optim.Optimizer \u2014 Optimiseur associ\u00e9 au MLP name : str \u2014 Nom du r\u00e9seau Methods \u00b6 plot(inputs, img_array) \u2014 Affiche l\u2019image originale, l\u2019image pr\u00e9dite et la courbe des pertes train(inputs, outputs, num_epochs=1500, batch_size=1024) \u2014 Entra\u00eene le MLP avec mini-batchs et AMP, stocke les pertes params() \u2014 Retourne tous les poids du MLP sous forme de liste d\u2019 ndarray neurons() \u2014 Retourne tous les biais du MLP sous forme de liste d\u2019 ndarray nb_params() \u2014 Calcule le nombre total de param\u00e8tres (poids uniquement) du r\u00e9seau Objets et dictionnaires \u00b6 norms() \u00b6 Valeurs Module PyTorch Description \"Relu\" nn.ReLU() Fonction d'activation ReLU classique (Rectified Linear Unit). \"LeakyRelu\" nn.LeakyReLU() ReLU avec un petit coefficient pour les valeurs n\u00e9gatives (param\u00e8tre negative_slope ). \"ELU\" nn.ELU() Fonction d'activation ELU (Exponential Linear Unit), qui a une meilleure gestion des valeurs n\u00e9gatives. \"SELU\" nn.SELU() SELU (Scaled Exponential Linear Unit), une version am\u00e9lior\u00e9e de l'ELU pour des r\u00e9seaux auto-normalisants. \"GELU\" nn.GELU() GELU (Gaussian Error Linear Unit), une activation probabiliste bas\u00e9e sur une fonction gaussienne. \"Sigmoid\" nn.Sigmoid() Fonction d'activation Sigmoid, qui produit une sortie entre 0 et 1. \"Tanh\" nn.Tanh() Fonction d'activation Tanh, avec une sortie dans l'intervalle [-1, 1]. \"Hardtanh\" nn.Hardtanh() Variante de Tanh, avec des sorties limit\u00e9es entre une plage sp\u00e9cifi\u00e9e. \"Softplus\" nn.Softplus() Fonction d'activation qui approxime ReLU mais de mani\u00e8re liss\u00e9e. \"Softsign\" nn.Softsign() Fonction d'activation similaire \u00e0 Tanh mais plus souple, avec des valeurs dans [-1, 1]. crits() \u00b6 Valeurs Module PyTorch Description \"MSE\" nn.MSELoss() Mean Squared Error Loss, utilis\u00e9e pour les r\u00e9gressions. \"L1\" nn.L1Loss() L1 Loss (erreur absolue), souvent utilis\u00e9e pour la r\u00e9gularisation. \"SmoothL1\" nn.SmoothL1Loss() Smooth L1 Loss, une combinaison de L1 et de MSE, moins sensible aux outliers. \"Huber\" nn.HuberLoss() Fonction de perte Huber, une version liss\u00e9e de L1 et MSE, moins affect\u00e9e par les grands \u00e9carts. \"CrossEntropy\" nn.CrossEntropyLoss() Perte de Cross-Entropy, utilis\u00e9e pour les probl\u00e8mes de classification multi-classes. \"KLDiv\" nn.KLDivLoss() Perte de divergence de Kullback-Leibler, souvent utilis\u00e9e pour des mod\u00e8les probabilistes. \"PoissonNLL\" nn.PoissonNLLLoss() Perte de log-vraisemblance pour une distribution de Poisson, utilis\u00e9e pour la mod\u00e9lisation de comptages. \"MultiLabelSoftMargin\" nn.MultiLabelSoftMarginLoss() Perte utilis\u00e9e pour les probl\u00e8mes de classification multi-\u00e9tiquettes. optims() \u00b6 Valeurs Module PyTorch Description \"Adadelta\" optim.Adadelta() Optimiseur Adadelta, bas\u00e9 sur les gradients adaptatifs, sans n\u00e9cessit\u00e9 de r\u00e9glage du taux d'apprentissage. \"Adafactor\" optim.Adafactor() Optimiseur Adafactor, variant d'Adam avec une mise \u00e0 jour plus efficace de la m\u00e9moire pour de grands mod\u00e8les. \"Adam\" optim.Adam() Optimiseur Adam, utilisant un gradient stochastique adaptatif avec des moyennes mobiles des gradients et des carr\u00e9s des gradients. \"AdamW\" optim.AdamW() Optimiseur Adam avec une r\u00e9gularisation L2 (weight decay) distincte, plus efficace que Adam avec weight_decay . \"Adamax\" optim.Adamax() Version d'Adam utilisant une norme infinie pour les gradients, plus stable pour certaines configurations. \"ASGD\" optim.ASGD() Optimiseur ASGD (Averaged Stochastic Gradient Descent), utilis\u00e9 pour de grandes donn\u00e9es avec une moyenne des gradients. \"NAdam\" optim.NAdam() Optimiseur NAdam, une version am\u00e9lior\u00e9e d'Adam avec une adaptation des moments de second ordre. \"RAdam\" optim.RAdam() Optimiseur RAdam, une version robuste de l'Adam qui ajuste dynamiquement les moments pour stabiliser l'entra\u00eenement. \"RMSprop\" optim.RMSprop() Optimiseur RMSprop, utilisant une moyenne mobile des carr\u00e9s des gradients pour r\u00e9duire les oscillations. \"Rprop\" optim.Rprop() Optimiseur Rprop, bas\u00e9 sur les mises \u00e0 jour des poids ind\u00e9pendantes des gradients. \"SGD\" optim.SGD() Descente de gradient stochastique classique, souvent utilis\u00e9e avec un taux d'apprentissage constant ou ajust\u00e9. Notes \u00b6 La classe supporte un entra\u00eenement sur GPU via device Les fonctions de visualisation utilisent matplotlib et visualtorch Les sorties sont compatibles avec des images normalis\u00e9es entre 0 et 1 Le suivi des pertes permet d\u2019afficher l\u2019\u00e9volution du training loss","title":"MLP"},{"location":"MLP/#multi-layer-perceptron-mlp","text":"Multi-Layer Perceptron (MLP) avec encodage optionnel Fourier (RFF), suivi automatique des pertes, visualisation et compilation PyTorch. Cette classe fournit : Un MLP enti\u00e8rement configurable (dimensions, normalisation, activation) Option d'encodage Fourier (Random Fourier Features) sur les entr\u00e9es M\u00e9thodes pour entra\u00eener le r\u00e9seau avec mini-batchs et AMP (Automatic Mixed Precision) Visualisation de l'architecture via visualtorch Suivi et affichage de la perte d'entra\u00eenement Acc\u00e8s aux poids, biais et nombre de param\u00e8tres Compilation du mod\u00e8le via torch.compile pour acc\u00e9l\u00e9rer l'inf\u00e9rence","title":"Multi-Layer Perceptron (MLP)"},{"location":"MLP/#parameters","text":"Parameter Type Optional Description layers list[int] Yes Dimensions successives du r\u00e9seau (entr\u00e9e \u2192 couches cach\u00e9es \u2192 sortie). Exemple : [in_features, hidden1, hidden2, ..., out_features] . Default: [1, 1, 1] learning_rate float Yes Taux d\u2019apprentissage pour l\u2019optimiseur. Default: 1e-3 Fourier bool Yes Si True, applique un encodage Fourier gaussien (RFF) sur les entr\u00e9es. Default: True optim str Yes Nom de l\u2019optimiseur \u00e0 utiliser (doit exister dans optims() ). Default: \"Adam\" crit str Yes Fonction de perte \u00e0 utiliser (doit exister dans crits() ). Default: \"MSE\" norm str Yes Type de normalisation / activation pour les couches cach\u00e9es (ex: \"Relu\" ). Default: \"Relu\" name str Yes Nom du r\u00e9seau pour identification ou affichage. Default: \"Net\" Iscompiled bool Yes Si True, compile le mod\u00e8le via torch.compile pour acc\u00e9l\u00e9rer l\u2019inf\u00e9rence. Default: True","title":"Parameters"},{"location":"MLP/#attributes","text":"losses : list[torch.Tensor] \u2014 Historique des pertes cumul\u00e9es lors de l'entra\u00eenement layers : list[int] \u2014 Dimensions du r\u00e9seau, ajust\u00e9es si encodage Fourier actif encoding : nn.Module \u2014 Module appliquant l'encodage des entr\u00e9es (RFF ou identity) norm : nn.Module \u2014 Normalisation ou activation utilis\u00e9e dans les couches cach\u00e9es crit : nn.Module \u2014 Fonction de perte PyTorch sur le device sp\u00e9cifi\u00e9 model : nn.Sequential \u2014 MLP complet construit dynamiquement optim : torch.optim.Optimizer \u2014 Optimiseur associ\u00e9 au MLP name : str \u2014 Nom du r\u00e9seau","title":"Attributes"},{"location":"MLP/#methods","text":"plot(inputs, img_array) \u2014 Affiche l\u2019image originale, l\u2019image pr\u00e9dite et la courbe des pertes train(inputs, outputs, num_epochs=1500, batch_size=1024) \u2014 Entra\u00eene le MLP avec mini-batchs et AMP, stocke les pertes params() \u2014 Retourne tous les poids du MLP sous forme de liste d\u2019 ndarray neurons() \u2014 Retourne tous les biais du MLP sous forme de liste d\u2019 ndarray nb_params() \u2014 Calcule le nombre total de param\u00e8tres (poids uniquement) du r\u00e9seau","title":"Methods"},{"location":"MLP/#objets-et-dictionnaires","text":"","title":"Objets et dictionnaires"},{"location":"MLP/#norms","text":"Valeurs Module PyTorch Description \"Relu\" nn.ReLU() Fonction d'activation ReLU classique (Rectified Linear Unit). \"LeakyRelu\" nn.LeakyReLU() ReLU avec un petit coefficient pour les valeurs n\u00e9gatives (param\u00e8tre negative_slope ). \"ELU\" nn.ELU() Fonction d'activation ELU (Exponential Linear Unit), qui a une meilleure gestion des valeurs n\u00e9gatives. \"SELU\" nn.SELU() SELU (Scaled Exponential Linear Unit), une version am\u00e9lior\u00e9e de l'ELU pour des r\u00e9seaux auto-normalisants. \"GELU\" nn.GELU() GELU (Gaussian Error Linear Unit), une activation probabiliste bas\u00e9e sur une fonction gaussienne. \"Sigmoid\" nn.Sigmoid() Fonction d'activation Sigmoid, qui produit une sortie entre 0 et 1. \"Tanh\" nn.Tanh() Fonction d'activation Tanh, avec une sortie dans l'intervalle [-1, 1]. \"Hardtanh\" nn.Hardtanh() Variante de Tanh, avec des sorties limit\u00e9es entre une plage sp\u00e9cifi\u00e9e. \"Softplus\" nn.Softplus() Fonction d'activation qui approxime ReLU mais de mani\u00e8re liss\u00e9e. \"Softsign\" nn.Softsign() Fonction d'activation similaire \u00e0 Tanh mais plus souple, avec des valeurs dans [-1, 1].","title":"norms()"},{"location":"MLP/#crits","text":"Valeurs Module PyTorch Description \"MSE\" nn.MSELoss() Mean Squared Error Loss, utilis\u00e9e pour les r\u00e9gressions. \"L1\" nn.L1Loss() L1 Loss (erreur absolue), souvent utilis\u00e9e pour la r\u00e9gularisation. \"SmoothL1\" nn.SmoothL1Loss() Smooth L1 Loss, une combinaison de L1 et de MSE, moins sensible aux outliers. \"Huber\" nn.HuberLoss() Fonction de perte Huber, une version liss\u00e9e de L1 et MSE, moins affect\u00e9e par les grands \u00e9carts. \"CrossEntropy\" nn.CrossEntropyLoss() Perte de Cross-Entropy, utilis\u00e9e pour les probl\u00e8mes de classification multi-classes. \"KLDiv\" nn.KLDivLoss() Perte de divergence de Kullback-Leibler, souvent utilis\u00e9e pour des mod\u00e8les probabilistes. \"PoissonNLL\" nn.PoissonNLLLoss() Perte de log-vraisemblance pour une distribution de Poisson, utilis\u00e9e pour la mod\u00e9lisation de comptages. \"MultiLabelSoftMargin\" nn.MultiLabelSoftMarginLoss() Perte utilis\u00e9e pour les probl\u00e8mes de classification multi-\u00e9tiquettes.","title":"crits()"},{"location":"MLP/#optims","text":"Valeurs Module PyTorch Description \"Adadelta\" optim.Adadelta() Optimiseur Adadelta, bas\u00e9 sur les gradients adaptatifs, sans n\u00e9cessit\u00e9 de r\u00e9glage du taux d'apprentissage. \"Adafactor\" optim.Adafactor() Optimiseur Adafactor, variant d'Adam avec une mise \u00e0 jour plus efficace de la m\u00e9moire pour de grands mod\u00e8les. \"Adam\" optim.Adam() Optimiseur Adam, utilisant un gradient stochastique adaptatif avec des moyennes mobiles des gradients et des carr\u00e9s des gradients. \"AdamW\" optim.AdamW() Optimiseur Adam avec une r\u00e9gularisation L2 (weight decay) distincte, plus efficace que Adam avec weight_decay . \"Adamax\" optim.Adamax() Version d'Adam utilisant une norme infinie pour les gradients, plus stable pour certaines configurations. \"ASGD\" optim.ASGD() Optimiseur ASGD (Averaged Stochastic Gradient Descent), utilis\u00e9 pour de grandes donn\u00e9es avec une moyenne des gradients. \"NAdam\" optim.NAdam() Optimiseur NAdam, une version am\u00e9lior\u00e9e d'Adam avec une adaptation des moments de second ordre. \"RAdam\" optim.RAdam() Optimiseur RAdam, une version robuste de l'Adam qui ajuste dynamiquement les moments pour stabiliser l'entra\u00eenement. \"RMSprop\" optim.RMSprop() Optimiseur RMSprop, utilisant une moyenne mobile des carr\u00e9s des gradients pour r\u00e9duire les oscillations. \"Rprop\" optim.Rprop() Optimiseur Rprop, bas\u00e9 sur les mises \u00e0 jour des poids ind\u00e9pendantes des gradients. \"SGD\" optim.SGD() Descente de gradient stochastique classique, souvent utilis\u00e9e avec un taux d'apprentissage constant ou ajust\u00e9.","title":"optims()"},{"location":"MLP/#notes","text":"La classe supporte un entra\u00eenement sur GPU via device Les fonctions de visualisation utilisent matplotlib et visualtorch Les sorties sont compatibles avec des images normalis\u00e9es entre 0 et 1 Le suivi des pertes permet d\u2019afficher l\u2019\u00e9volution du training loss","title":"Notes"},{"location":"MLP/nb_params/","text":"MLP.nb_params \u00b6 Calcule le nombre total de param\u00e8tres (poids) du MLP. Cette m\u00e9thode parcourt les param\u00e8tres du mod\u00e8le et ne compte que les poids des couches lin\u00e9aires. Dans un nn.Linear , parameters() renvoie dans l'ordre : - les poids (indice pair) - puis les biais (indice impair) On ignore donc les biais dans ce comptage. Retour \u00b6 Type Description int Nombre total de param\u00e8tres pond\u00e9r\u00e9s (weights) dans le MLP.","title":"MLP.nb_params"},{"location":"MLP/nb_params/#mlpnb_params","text":"Calcule le nombre total de param\u00e8tres (poids) du MLP. Cette m\u00e9thode parcourt les param\u00e8tres du mod\u00e8le et ne compte que les poids des couches lin\u00e9aires. Dans un nn.Linear , parameters() renvoie dans l'ordre : - les poids (indice pair) - puis les biais (indice impair) On ignore donc les biais dans ce comptage.","title":"MLP.nb_params"},{"location":"MLP/nb_params/#retour","text":"Type Description int Nombre total de param\u00e8tres pond\u00e9r\u00e9s (weights) dans le MLP.","title":"Retour"},{"location":"MLP/neurons/","text":"MLP.neurons \u00b6 Extrait l'ensemble des neurones (biais) du MLP couche par couche. Cette m\u00e9thode parcourt les param\u00e8tres du mod\u00e8le et r\u00e9cup\u00e8re uniquement les poids associ\u00e9s aux biais des couches lin\u00e9aires. Dans un nn.Linear , parameters() renvoie successivement : - les poids (weights) - puis les biais (bias) Les indices impairs correspondent donc aux biais. Retour \u00b6 Type Description list[np.ndarray] Liste contenant chaque neurone (chaque valeur de biais), converti en array NumPy sur CPU.","title":"MLP.neurons"},{"location":"MLP/neurons/#mlpneurons","text":"Extrait l'ensemble des neurones (biais) du MLP couche par couche. Cette m\u00e9thode parcourt les param\u00e8tres du mod\u00e8le et r\u00e9cup\u00e8re uniquement les poids associ\u00e9s aux biais des couches lin\u00e9aires. Dans un nn.Linear , parameters() renvoie successivement : - les poids (weights) - puis les biais (bias) Les indices impairs correspondent donc aux biais.","title":"MLP.neurons"},{"location":"MLP/neurons/#retour","text":"Type Description list[np.ndarray] Liste contenant chaque neurone (chaque valeur de biais), converti en array NumPy sur CPU.","title":"Retour"},{"location":"MLP/params/","text":"MLP.params \u00b6 Retourne la liste de tous les poids (weights) du MLP. Cette fonction extrait uniquement les matrices de poids des couches lin\u00e9aires, en ignorant les biais. Dans un nn.Linear , parameters() renvoie dans l'ordre : - les poids (indice pair) - les biais (indice impair) La m\u00e9thode : Parcourt les param\u00e8tres du r\u00e9seau S\u00e9lectionne uniquement ceux correspondant aux poids S\u00e9pare chaque ligne de la matrice de poids Convertit chaque ligne en ndarray CPU d\u00e9tach\u00e9 Retour \u00b6 Type Description list[np.ndarray] Liste contenant chaque ligne des matrices de poids (chaque \u00e9l\u00e9ment est un vecteur numpy).","title":"MLP.params"},{"location":"MLP/params/#mlpparams","text":"Retourne la liste de tous les poids (weights) du MLP. Cette fonction extrait uniquement les matrices de poids des couches lin\u00e9aires, en ignorant les biais. Dans un nn.Linear , parameters() renvoie dans l'ordre : - les poids (indice pair) - les biais (indice impair) La m\u00e9thode : Parcourt les param\u00e8tres du r\u00e9seau S\u00e9lectionne uniquement ceux correspondant aux poids S\u00e9pare chaque ligne de la matrice de poids Convertit chaque ligne en ndarray CPU d\u00e9tach\u00e9","title":"MLP.params"},{"location":"MLP/params/#retour","text":"Type Description list[np.ndarray] Liste contenant chaque ligne des matrices de poids (chaque \u00e9l\u00e9ment est un vecteur numpy).","title":"Retour"},{"location":"MLP/plot/","text":"MLP.plot \u00b6 Affiche c\u00f4te \u00e0 c\u00f4te : - l\u2019image originale - l\u2019image pr\u00e9dite par le MLP - l\u2019\u00e9volution de la fonction de perte (loss) au cours de l\u2019entra\u00eenement Param\u00e8tres \u00b6 Param\u00e8tre Type Description img_array np.ndarray Image originale sous forme de tableau (H, W, 3) utilis\u00e9e comme r\u00e9f\u00e9rence. inputs array-like ou torch.Tensor Tableau des coordonn\u00e9es (ou features) servant d\u2019entr\u00e9e au r\u00e9seau. Doit correspondre \u00e0 la grille permettant de reconstruire l\u2019image. Notes \u00b6 Cette m\u00e9thode : - Tensorise les entr\u00e9es puis les encode avant passage dans le MLP - Reshape la sortie du mod\u00e8le pour retrouver la forme (H, W, 3) - Trace \u00e9galement la courbe de pertes stock\u00e9e dans self.losses","title":"MLP.plot"},{"location":"MLP/plot/#mlpplot","text":"Affiche c\u00f4te \u00e0 c\u00f4te : - l\u2019image originale - l\u2019image pr\u00e9dite par le MLP - l\u2019\u00e9volution de la fonction de perte (loss) au cours de l\u2019entra\u00eenement","title":"MLP.plot"},{"location":"MLP/plot/#parametres","text":"Param\u00e8tre Type Description img_array np.ndarray Image originale sous forme de tableau (H, W, 3) utilis\u00e9e comme r\u00e9f\u00e9rence. inputs array-like ou torch.Tensor Tableau des coordonn\u00e9es (ou features) servant d\u2019entr\u00e9e au r\u00e9seau. Doit correspondre \u00e0 la grille permettant de reconstruire l\u2019image.","title":"Param\u00e8tres"},{"location":"MLP/plot/#notes","text":"Cette m\u00e9thode : - Tensorise les entr\u00e9es puis les encode avant passage dans le MLP - Reshape la sortie du mod\u00e8le pour retrouver la forme (H, W, 3) - Trace \u00e9galement la courbe de pertes stock\u00e9e dans self.losses","title":"Notes"},{"location":"MLP/train/","text":"MLP.train \u00b6 Entra\u00eene le MLP sur des paires (inputs \u2192 outputs) en utilisant un sch\u00e9ma de mini-batchs et l'AMP (Automatic Mixed Precision). Param\u00e8tres \u00b6 Param\u00e8tre Type Description inputs array-like ou tensor Donn\u00e9es d'entr\u00e9e du r\u00e9seau, de shape (N, input_dim) . outputs array-like ou tensor Cibles associ\u00e9es, de shape (N, output_dim) . num_epochs int, optionnel Nombre total d'\u00e9poques d'entra\u00eenement. Default: 1500 . batch_size int, optionnel Taille des mini-batchs utilis\u00e9s \u00e0 chaque it\u00e9ration. Default: 1024 . Notes \u00b6 Utilise torch.amp.autocast + GradScaler pour un entra\u00eenement acc\u00e9l\u00e9r\u00e9 en FP16 Les pertes par \u00e9poque sont stock\u00e9es dans self.losses","title":"MLP.train"},{"location":"MLP/train/#mlptrain","text":"Entra\u00eene le MLP sur des paires (inputs \u2192 outputs) en utilisant un sch\u00e9ma de mini-batchs et l'AMP (Automatic Mixed Precision).","title":"MLP.train"},{"location":"MLP/train/#parametres","text":"Param\u00e8tre Type Description inputs array-like ou tensor Donn\u00e9es d'entr\u00e9e du r\u00e9seau, de shape (N, input_dim) . outputs array-like ou tensor Cibles associ\u00e9es, de shape (N, output_dim) . num_epochs int, optionnel Nombre total d'\u00e9poques d'entra\u00eenement. Default: 1500 . batch_size int, optionnel Taille des mini-batchs utilis\u00e9s \u00e0 chaque it\u00e9ration. Default: 1024 .","title":"Param\u00e8tres"},{"location":"MLP/train/#notes","text":"Utilise torch.amp.autocast + GradScaler pour un entra\u00eenement acc\u00e9l\u00e9r\u00e9 en FP16 Les pertes par \u00e9poque sont stock\u00e9es dans self.losses","title":"Notes"},{"location":"Plots/compare/","text":"compare \u00b6 Affiche, pour chaque r\u00e9seau, l\u2019erreur absolue entre l\u2019image originale et l\u2019image reconstruite par le r\u00e9seau. Chaque r\u00e9seau doit poss\u00e9der : - une m\u00e9thode encoding(x) (si RFF activ\u00e9), - un module model retournant un tenseur de shape (N, 3), - une reconstruction compatible avec (H, W, 3). Param\u00e8tres \u00b6 Param\u00e8tre Type Description img_array np.ndarray (H, W, 3) Image originale servant de r\u00e9f\u00e9rence. inputs tensor-like (H*W, 2) Coordonn\u00e9es normalis\u00e9es des pixels correspondant \u00e0 chaque point de l'image. nets MLP ou liste de MLP Un ou plusieurs r\u00e9seaux poss\u00e9dant les m\u00e9thodes .encoding() et .model() , et l\u2019attribut .losses . Notes \u00b6 Affiche la diff\u00e9rence absolue entre l\u2019image originale et la pr\u00e9diction du r\u00e9seau. Les pertes cumul\u00e9es sont \u00e9galement trac\u00e9es pour chaque r\u00e9seau. Utilise matplotlib en mode interactif.","title":"compare"},{"location":"Plots/compare/#compare","text":"Affiche, pour chaque r\u00e9seau, l\u2019erreur absolue entre l\u2019image originale et l\u2019image reconstruite par le r\u00e9seau. Chaque r\u00e9seau doit poss\u00e9der : - une m\u00e9thode encoding(x) (si RFF activ\u00e9), - un module model retournant un tenseur de shape (N, 3), - une reconstruction compatible avec (H, W, 3).","title":"compare"},{"location":"Plots/compare/#parametres","text":"Param\u00e8tre Type Description img_array np.ndarray (H, W, 3) Image originale servant de r\u00e9f\u00e9rence. inputs tensor-like (H*W, 2) Coordonn\u00e9es normalis\u00e9es des pixels correspondant \u00e0 chaque point de l'image. nets MLP ou liste de MLP Un ou plusieurs r\u00e9seaux poss\u00e9dant les m\u00e9thodes .encoding() et .model() , et l\u2019attribut .losses .","title":"Param\u00e8tres"},{"location":"Plots/compare/#notes","text":"Affiche la diff\u00e9rence absolue entre l\u2019image originale et la pr\u00e9diction du r\u00e9seau. Les pertes cumul\u00e9es sont \u00e9galement trac\u00e9es pour chaque r\u00e9seau. Utilise matplotlib en mode interactif.","title":"Notes"},{"location":"Plots/losses/","text":"losses \u00b6 Affiche les courbes de pertes (training loss) de plusieurs r\u00e9seaux MLP. Param\u00e8tres \u00b6 Param\u00e8tre Type Description *nets MLP ou liste de MLP Un ou plusieurs r\u00e9seaux poss\u00e9dant un attribut .losses contenant l'historique des pertes (liste de float). Notes \u00b6 L\u2019axe X correspond aux it\u00e9rations (epochs ou steps). L\u2019axe Y correspond \u00e0 la valeur de la perte. Utilise matplotlib en mode interactif pour un affichage dynamique.","title":"losses"},{"location":"Plots/losses/#losses","text":"Affiche les courbes de pertes (training loss) de plusieurs r\u00e9seaux MLP.","title":"losses"},{"location":"Plots/losses/#parametres","text":"Param\u00e8tre Type Description *nets MLP ou liste de MLP Un ou plusieurs r\u00e9seaux poss\u00e9dant un attribut .losses contenant l'historique des pertes (liste de float).","title":"Param\u00e8tres"},{"location":"Plots/losses/#notes","text":"L\u2019axe X correspond aux it\u00e9rations (epochs ou steps). L\u2019axe Y correspond \u00e0 la valeur de la perte. Utilise matplotlib en mode interactif pour un affichage dynamique.","title":"Notes"},{"location":"Plots/plot/","text":"plot \u00b6 Affiche, pour chaque r\u00e9seau, l\u2019image reconstruite \u00e0 partir de ses pr\u00e9dictions. Param\u00e8tres \u00b6 Param\u00e8tre Type Description img_array np.ndarray (H, W, 3) Image originale, utilis\u00e9e pour conna\u00eetre les dimensions de reconstruction. inputs tensor-like (H*W, 2) Coordonn\u00e9es normalis\u00e9es des pixels correspondant \u00e0 chaque point de l'image. *nets MLP ou liste de MLP Un ou plusieurs r\u00e9seaux poss\u00e9dant les m\u00e9thodes .encoding() et .model() , et l\u2019attribut .losses . Notes \u00b6 Affiche la pr\u00e9diction brute de chaque r\u00e9seau. Les pertes cumul\u00e9es sont \u00e9galement trac\u00e9es. Utilise matplotlib en mode interactif.","title":"plot"},{"location":"Plots/plot/#plot","text":"Affiche, pour chaque r\u00e9seau, l\u2019image reconstruite \u00e0 partir de ses pr\u00e9dictions.","title":"plot"},{"location":"Plots/plot/#parametres","text":"Param\u00e8tre Type Description img_array np.ndarray (H, W, 3) Image originale, utilis\u00e9e pour conna\u00eetre les dimensions de reconstruction. inputs tensor-like (H*W, 2) Coordonn\u00e9es normalis\u00e9es des pixels correspondant \u00e0 chaque point de l'image. *nets MLP ou liste de MLP Un ou plusieurs r\u00e9seaux poss\u00e9dant les m\u00e9thodes .encoding() et .model() , et l\u2019attribut .losses .","title":"Param\u00e8tres"},{"location":"Plots/plot/#notes","text":"Affiche la pr\u00e9diction brute de chaque r\u00e9seau. Les pertes cumul\u00e9es sont \u00e9galement trac\u00e9es. Utilise matplotlib en mode interactif.","title":"Notes"},{"location":"Plots/train/","text":"train \u00b6 Entra\u00eene un ou plusieurs MLP sur des paires (inputs, outputs) avec gestion optionnelle de l'affichage interactif. Affiche dynamiquement si img_array est fourni : - L'image originale (r\u00e9f\u00e9rence) - Les pr\u00e9dictions des MLP - L'\u00e9volution des pertes au fil des \u00e9poques Param\u00e8tres \u00b6 Param\u00e8tre Type Description inputs array-like Entr\u00e9es du ou des MLP (shape: [n_samples, n_features]). outputs array-like Sorties cibles correspondantes (shape: [n_samples, output_dim]). num_epochs int, optional Nombre d\u2019\u00e9poques pour l\u2019entra\u00eenement (default=1500). batch_size int, optional Taille des mini-batchs pour la descente de gradient (default=1024). *nets MLP ou liste de MLP Un ou plusieurs objets MLP \u00e0 entra\u00eener. img_array np.ndarray (H, W, 3), optional Image de r\u00e9f\u00e9rence pour visualisation des pr\u00e9dictions (default=None). Notes \u00b6 Les MLP sont entra\u00een\u00e9s ind\u00e9pendamment mais avec le m\u00eame ordre al\u00e9atoire d'\u00e9chantillons. Utilise torch.amp.GradScaler pour l'entra\u00eenement en FP16. La visualisation interactive utilise clear_output() et plt.ion() .","title":"train"},{"location":"Plots/train/#train","text":"Entra\u00eene un ou plusieurs MLP sur des paires (inputs, outputs) avec gestion optionnelle de l'affichage interactif. Affiche dynamiquement si img_array est fourni : - L'image originale (r\u00e9f\u00e9rence) - Les pr\u00e9dictions des MLP - L'\u00e9volution des pertes au fil des \u00e9poques","title":"train"},{"location":"Plots/train/#parametres","text":"Param\u00e8tre Type Description inputs array-like Entr\u00e9es du ou des MLP (shape: [n_samples, n_features]). outputs array-like Sorties cibles correspondantes (shape: [n_samples, output_dim]). num_epochs int, optional Nombre d\u2019\u00e9poques pour l\u2019entra\u00eenement (default=1500). batch_size int, optional Taille des mini-batchs pour la descente de gradient (default=1024). *nets MLP ou liste de MLP Un ou plusieurs objets MLP \u00e0 entra\u00eener. img_array np.ndarray (H, W, 3), optional Image de r\u00e9f\u00e9rence pour visualisation des pr\u00e9dictions (default=None).","title":"Param\u00e8tres"},{"location":"Plots/train/#notes","text":"Les MLP sont entra\u00een\u00e9s ind\u00e9pendamment mais avec le m\u00eame ordre al\u00e9atoire d'\u00e9chantillons. Utilise torch.amp.GradScaler pour l'entra\u00eenement en FP16. La visualisation interactive utilise clear_output() et plt.ion() .","title":"Notes"}]}