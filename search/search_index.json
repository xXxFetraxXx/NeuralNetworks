{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"NeuralNetworks Module \u00b6 Module complet pour la cr\u00e9ation, l'entra\u00eenement et la visualisation de Multi-Layer Perceptrons (MLP) avec encodage optionnel Fourier, gestion automatique des pertes, compilation Torch et outils de traitement d'images pour l'apprentissage sur des images RGB. Contenu principal \u00b6 Classes \u00b6 MLP \u00b6 Multi-Layer Perceptron (MLP) avec encodage optionnel Fourier (RFF), suivi automatique des pertes, visualisation et compilation PyTorch. Cette classe fournit : Un MLP enti\u00e8rement configurable (dimensions, normalisation, activation) Option d'encodage Fourier (Random Fourier Features) sur les entr\u00e9es M\u00e9thodes pour entra\u00eener le r\u00e9seau avec mini-batchs et AMP (Automatic Mixed Precision) Visualisation de l'architecture via visualtorch Suivi et affichage de la perte d'entra\u00eenement Acc\u00e8s aux poids, biais et nombre de param\u00e8tres Parameters \u00b6 Parameter Type Optional Description input_size int Yes Taille des donn\u00e9es en entr\u00e9e au r\u00e9seau. Default: 1 output_size int Yes Taille des donn\u00e9es en sortie au r\u00e9seau. Default: 1 hidden_layers list[int] Yes Dimensions successives des couches interm\u00e9diaires du r\u00e9seau. Default: [1] sigmas list[float] Yes Liste de sigma pour encodages RFF. Si None : passthrough. Default: None fourier_input_size int Yes WIP. Default: 2 nb_fourier int Yes Nombre de fr\u00e9quences utilis\u00e9es pour les Fourier Features. Default: 8 norm str Yes Type de normalisation / activation pour les couches cach\u00e9es (ex: \"Relu\" ). Default: \"Relu\" name str Yes Nom du r\u00e9seau pour identification ou affichage. Default: \"Net\" Attributes \u00b6 losses : list[float] \u2014 Historique des pertes cumul\u00e9es lors de l'entra\u00eenement learnings : list[float] \u2014 Historique des taux d'apprentissage utilis\u00e9es lors de l'entra\u00eenement model : nn.Sequential \u2014 MLP complet construit dynamiquement name : str \u2014 Nom du r\u00e9seau Trainer \u00b6 Parameters \u00b6 Parameter Type Optional Description *nets *MLP No R\u00e9seaux pour lesquels le trainer va entrainer. inputs np.array(float) No Donn\u00e9es en entr\u00e9e au r\u00e9seau. outputs np.array(float) No Donn\u00e9es en sortie au r\u00e9seau. test_size float Yes Proportion des donn\u00e9es \u00e0 utiliser pendant l'entrainement. Si None : utilise toutes les donn\u00e9es. Default: None optim str Yes Nom de l\u2019optimiseur \u00e0 utiliser (doit exister dans optims() ). Default: \"Adam\" init_lr float Yes Taux d\u2019apprentissage initial pour l\u2019optimiseur. Default: 1e-3 crit str Yes Fonction de perte \u00e0 utiliser (doit exister dans crits() ). Default: \"MSE\" batch_size float Yes Taille des minibatchs. Default: 1024 Trainer.train \u00b6 Lancement d'un entrainement avec le trainer d\u00e9finit Parameter Type Optional Description num_epochs int Yes Nombres d'it\u00e9rations \u00e0 effectuer. activate_tqdm boolean Yes Utilisation d'une barre de progression. Dictionnaires \u00b6 norms() \u00b6 Valeurs Module PyTorch Description \"ReLU\" nn.ReLU() Fonction d'activation ReLU classique (Rectified Linear Unit). \"LeakyReLU\" nn.LeakyReLU() ReLU avec un petit coefficient pour les valeurs n\u00e9gatives (param\u00e8tre negative_slope ). \"ELU\" nn.ELU() Fonction d'activation ELU (Exponential Linear Unit), qui a une meilleure gestion des valeurs n\u00e9gatives. \"SELU\" nn.SELU() SELU (Scaled Exponential Linear Unit), une version am\u00e9lior\u00e9e de l'ELU pour des r\u00e9seaux auto-normalisants. \"GELU\" nn.GELU() GELU (Gaussian Error Linear Unit), une activation probabiliste bas\u00e9e sur une fonction gaussienne. \"Mish\" nn.Mish() ReLU diff\u00e9rentiable en tout points avec passage n\u00e9gatif. \"Softplus\" nn.Softplus() Fonction d'activation qui approxime ReLU mais de mani\u00e8re liss\u00e9e. \"Sigmoid\" nn.Sigmoid() Fonction d'activation Sigmoid, qui produit une sortie entre 0 et 1. \"Tanh\" nn.Tanh() Fonction d'activation Tanh, avec une sortie dans l'intervalle [-1, 1]. \"Hardtanh\" nn.Hardtanh() Variante de Tanh, avec des sorties limit\u00e9es entre une plage sp\u00e9cifi\u00e9e. \"Softsign\" nn.Softsign() Fonction d'activation similaire \u00e0 Tanh mais plus souple, avec des valeurs dans [-1, 1]. crits() \u00b6 Valeurs Module PyTorch Description \"MSE\" nn.MSELoss Mean Squared Error Loss, utilis\u00e9e pour les r\u00e9gressions. \"L1\" nn.L1Loss() L1 Loss (erreur absolue), souvent utilis\u00e9e pour la r\u00e9gularisation. \"SmoothL1\" nn.SmoothL1Loss() Smooth L1 Loss, une combinaison de L1 et de MSE, moins sensible aux outliers. \"Huber\" nn.HuberLoss() Fonction de perte Huber, une version liss\u00e9e de L1 et MSE, moins affect\u00e9e par les grands \u00e9carts. \"CrossEntropy\" nn.CrossEntropyLoss() Perte de Cross-Entropy, utilis\u00e9e pour les probl\u00e8mes de classification multi-classes. \"KLDiv\" nn.KLDivLoss() Perte de divergence de Kullback-Leibler, souvent utilis\u00e9e pour des mod\u00e8les probabilistes. \"PoissonNLL\" nn.PoissonNLLLoss() Perte de log-vraisemblance pour une distribution de Poisson, utilis\u00e9e pour la mod\u00e9lisation de comptages. \"MultiLabelSoftMargin\" nn.MultiLabelSoftMarginLoss() Perte utilis\u00e9e pour les probl\u00e8mes de classification multi-\u00e9tiquettes. optims() \u00b6 Valeurs Module PyTorch Description \"Adadelta\" optim.Adadelta() Optimiseur Adadelta, bas\u00e9 sur les gradients adaptatifs, sans n\u00e9cessit\u00e9 de r\u00e9glage du taux d'apprentissage. \"Adafactor\" optim.Adafactor() Optimiseur Adafactor, variant d'Adam avec une mise \u00e0 jour plus efficace de la m\u00e9moire pour de grands mod\u00e8les. \"Adam\" optim.Adam() Optimiseur Adam, utilisant un gradient stochastique adaptatif avec des moyennes mobiles des gradients et des carr\u00e9s des gradients. \"AdamW\" optim.AdamW() Optimiseur Adam avec une r\u00e9gularisation L2 (weight decay) distincte, plus efficace que Adam avec weight_decay . \"Adamax\" optim.Adamax() Version d'Adam utilisant une norme infinie pour les gradients, plus stable pour certaines configurations. \"ASGD\" optim.ASGD() Optimiseur ASGD (Averaged Stochastic Gradient Descent), utilis\u00e9 pour de grandes donn\u00e9es avec une moyenne des gradients. \"NAdam\" optim.NAdam() Optimiseur NAdam, une version am\u00e9lior\u00e9e d'Adam avec une adaptation des moments de second ordre. \"RAdam\" optim.RAdam() Optimiseur RAdam, une version robuste de l'Adam qui ajuste dynamiquement les moments pour stabiliser l'entra\u00eenement. \"RMSprop\" optim.RMSprop() Optimiseur RMSprop, utilisant une moyenne mobile des carr\u00e9s des gradients pour r\u00e9duire les oscillations. \"Rprop\" optim.Rprop() Optimiseur Rprop, bas\u00e9 sur les mises \u00e0 jour des poids ind\u00e9pendantes des gradients. \"SGD\" optim.SGD() Descente de gradient stochastique classique, souvent utilis\u00e9e avec un taux d'apprentissage constant ou ajust\u00e9. device \u00b6 variable principale d'allocation des performances Apple Silicon (macOS) \u00b6 Si le syst\u00e8me d'exploitation est macOS (nomm\u00e9 darwin dans platform.system() ), la fonction v\u00e9rifie si l'acc\u00e9l\u00e9rateur Metal Performance Shaders (MPS) est disponible sur l'appareil. Si MPS est disponible ( torch.backends.mps.is_available() ), l'appareil cible sera d\u00e9fini sur MPS (c'est un \u00e9quivalent de CUDA pour les appareils Apple Silicon). Windows \u00b6 Si le syst\u00e8me d'exploitation est Windows, la fonction v\u00e9rifie d'abord si CUDA (NVIDIA) est disponible avec torch.cuda.is_available() . Si c'est le cas, le p\u00e9riph\u00e9rique sera d\u00e9fini sur CUDA . Linux \u00b6 Si le syst\u00e8me d'exploitation est Linux, plusieurs v\u00e9rifications sont effectu\u00e9es : CUDA (NVIDIA) : Si torch.cuda.is_available() renvoie True , le p\u00e9riph\u00e9rique sera d\u00e9fini sur CUDA . ROCm (AMD) : Si le syst\u00e8me supporte ROCm via torch.backends.hip.is_available() , l'appareil sera d\u00e9fini sur CUDA (ROCm est utilis\u00e9 pour les cartes AMD dans le cadre de l'API CUDA). Intel oneAPI / XPU : Si le syst\u00e8me prend en charge Intel oneAPI ou XPU via torch.xpu.is_available() , le p\u00e9riph\u00e9rique sera d\u00e9fini sur XPU . Syst\u00e8me non reconnu \u00b6 Si aucune des conditions ci-dessus n'est remplie, la fonction retourne CPU comme p\u00e9riph\u00e9rique par d\u00e9faut. Param\u00e8tres matplotlib et PyTorch \u00b6 Style global pour fond transparent et texte gris Optimisations CUDA activ\u00e9es pour TF32, matmul et convolutions Autograd configur\u00e9 pour privil\u00e9gier les performances","title":"Accueil"},{"location":"#neuralnetworks-module","text":"Module complet pour la cr\u00e9ation, l'entra\u00eenement et la visualisation de Multi-Layer Perceptrons (MLP) avec encodage optionnel Fourier, gestion automatique des pertes, compilation Torch et outils de traitement d'images pour l'apprentissage sur des images RGB.","title":"NeuralNetworks Module"},{"location":"#contenu-principal","text":"","title":"Contenu principal"},{"location":"#classes","text":"","title":"Classes"},{"location":"#mlp","text":"Multi-Layer Perceptron (MLP) avec encodage optionnel Fourier (RFF), suivi automatique des pertes, visualisation et compilation PyTorch. Cette classe fournit : Un MLP enti\u00e8rement configurable (dimensions, normalisation, activation) Option d'encodage Fourier (Random Fourier Features) sur les entr\u00e9es M\u00e9thodes pour entra\u00eener le r\u00e9seau avec mini-batchs et AMP (Automatic Mixed Precision) Visualisation de l'architecture via visualtorch Suivi et affichage de la perte d'entra\u00eenement Acc\u00e8s aux poids, biais et nombre de param\u00e8tres","title":"MLP"},{"location":"#parameters","text":"Parameter Type Optional Description input_size int Yes Taille des donn\u00e9es en entr\u00e9e au r\u00e9seau. Default: 1 output_size int Yes Taille des donn\u00e9es en sortie au r\u00e9seau. Default: 1 hidden_layers list[int] Yes Dimensions successives des couches interm\u00e9diaires du r\u00e9seau. Default: [1] sigmas list[float] Yes Liste de sigma pour encodages RFF. Si None : passthrough. Default: None fourier_input_size int Yes WIP. Default: 2 nb_fourier int Yes Nombre de fr\u00e9quences utilis\u00e9es pour les Fourier Features. Default: 8 norm str Yes Type de normalisation / activation pour les couches cach\u00e9es (ex: \"Relu\" ). Default: \"Relu\" name str Yes Nom du r\u00e9seau pour identification ou affichage. Default: \"Net\"","title":"Parameters"},{"location":"#attributes","text":"losses : list[float] \u2014 Historique des pertes cumul\u00e9es lors de l'entra\u00eenement learnings : list[float] \u2014 Historique des taux d'apprentissage utilis\u00e9es lors de l'entra\u00eenement model : nn.Sequential \u2014 MLP complet construit dynamiquement name : str \u2014 Nom du r\u00e9seau","title":"Attributes"},{"location":"#trainer","text":"","title":"Trainer"},{"location":"#parameters_1","text":"Parameter Type Optional Description *nets *MLP No R\u00e9seaux pour lesquels le trainer va entrainer. inputs np.array(float) No Donn\u00e9es en entr\u00e9e au r\u00e9seau. outputs np.array(float) No Donn\u00e9es en sortie au r\u00e9seau. test_size float Yes Proportion des donn\u00e9es \u00e0 utiliser pendant l'entrainement. Si None : utilise toutes les donn\u00e9es. Default: None optim str Yes Nom de l\u2019optimiseur \u00e0 utiliser (doit exister dans optims() ). Default: \"Adam\" init_lr float Yes Taux d\u2019apprentissage initial pour l\u2019optimiseur. Default: 1e-3 crit str Yes Fonction de perte \u00e0 utiliser (doit exister dans crits() ). Default: \"MSE\" batch_size float Yes Taille des minibatchs. Default: 1024","title":"Parameters"},{"location":"#trainertrain","text":"Lancement d'un entrainement avec le trainer d\u00e9finit Parameter Type Optional Description num_epochs int Yes Nombres d'it\u00e9rations \u00e0 effectuer. activate_tqdm boolean Yes Utilisation d'une barre de progression.","title":"Trainer.train"},{"location":"#dictionnaires","text":"","title":"Dictionnaires"},{"location":"#norms","text":"Valeurs Module PyTorch Description \"ReLU\" nn.ReLU() Fonction d'activation ReLU classique (Rectified Linear Unit). \"LeakyReLU\" nn.LeakyReLU() ReLU avec un petit coefficient pour les valeurs n\u00e9gatives (param\u00e8tre negative_slope ). \"ELU\" nn.ELU() Fonction d'activation ELU (Exponential Linear Unit), qui a une meilleure gestion des valeurs n\u00e9gatives. \"SELU\" nn.SELU() SELU (Scaled Exponential Linear Unit), une version am\u00e9lior\u00e9e de l'ELU pour des r\u00e9seaux auto-normalisants. \"GELU\" nn.GELU() GELU (Gaussian Error Linear Unit), une activation probabiliste bas\u00e9e sur une fonction gaussienne. \"Mish\" nn.Mish() ReLU diff\u00e9rentiable en tout points avec passage n\u00e9gatif. \"Softplus\" nn.Softplus() Fonction d'activation qui approxime ReLU mais de mani\u00e8re liss\u00e9e. \"Sigmoid\" nn.Sigmoid() Fonction d'activation Sigmoid, qui produit une sortie entre 0 et 1. \"Tanh\" nn.Tanh() Fonction d'activation Tanh, avec une sortie dans l'intervalle [-1, 1]. \"Hardtanh\" nn.Hardtanh() Variante de Tanh, avec des sorties limit\u00e9es entre une plage sp\u00e9cifi\u00e9e. \"Softsign\" nn.Softsign() Fonction d'activation similaire \u00e0 Tanh mais plus souple, avec des valeurs dans [-1, 1].","title":"norms()"},{"location":"#crits","text":"Valeurs Module PyTorch Description \"MSE\" nn.MSELoss Mean Squared Error Loss, utilis\u00e9e pour les r\u00e9gressions. \"L1\" nn.L1Loss() L1 Loss (erreur absolue), souvent utilis\u00e9e pour la r\u00e9gularisation. \"SmoothL1\" nn.SmoothL1Loss() Smooth L1 Loss, une combinaison de L1 et de MSE, moins sensible aux outliers. \"Huber\" nn.HuberLoss() Fonction de perte Huber, une version liss\u00e9e de L1 et MSE, moins affect\u00e9e par les grands \u00e9carts. \"CrossEntropy\" nn.CrossEntropyLoss() Perte de Cross-Entropy, utilis\u00e9e pour les probl\u00e8mes de classification multi-classes. \"KLDiv\" nn.KLDivLoss() Perte de divergence de Kullback-Leibler, souvent utilis\u00e9e pour des mod\u00e8les probabilistes. \"PoissonNLL\" nn.PoissonNLLLoss() Perte de log-vraisemblance pour une distribution de Poisson, utilis\u00e9e pour la mod\u00e9lisation de comptages. \"MultiLabelSoftMargin\" nn.MultiLabelSoftMarginLoss() Perte utilis\u00e9e pour les probl\u00e8mes de classification multi-\u00e9tiquettes.","title":"crits()"},{"location":"#optims","text":"Valeurs Module PyTorch Description \"Adadelta\" optim.Adadelta() Optimiseur Adadelta, bas\u00e9 sur les gradients adaptatifs, sans n\u00e9cessit\u00e9 de r\u00e9glage du taux d'apprentissage. \"Adafactor\" optim.Adafactor() Optimiseur Adafactor, variant d'Adam avec une mise \u00e0 jour plus efficace de la m\u00e9moire pour de grands mod\u00e8les. \"Adam\" optim.Adam() Optimiseur Adam, utilisant un gradient stochastique adaptatif avec des moyennes mobiles des gradients et des carr\u00e9s des gradients. \"AdamW\" optim.AdamW() Optimiseur Adam avec une r\u00e9gularisation L2 (weight decay) distincte, plus efficace que Adam avec weight_decay . \"Adamax\" optim.Adamax() Version d'Adam utilisant une norme infinie pour les gradients, plus stable pour certaines configurations. \"ASGD\" optim.ASGD() Optimiseur ASGD (Averaged Stochastic Gradient Descent), utilis\u00e9 pour de grandes donn\u00e9es avec une moyenne des gradients. \"NAdam\" optim.NAdam() Optimiseur NAdam, une version am\u00e9lior\u00e9e d'Adam avec une adaptation des moments de second ordre. \"RAdam\" optim.RAdam() Optimiseur RAdam, une version robuste de l'Adam qui ajuste dynamiquement les moments pour stabiliser l'entra\u00eenement. \"RMSprop\" optim.RMSprop() Optimiseur RMSprop, utilisant une moyenne mobile des carr\u00e9s des gradients pour r\u00e9duire les oscillations. \"Rprop\" optim.Rprop() Optimiseur Rprop, bas\u00e9 sur les mises \u00e0 jour des poids ind\u00e9pendantes des gradients. \"SGD\" optim.SGD() Descente de gradient stochastique classique, souvent utilis\u00e9e avec un taux d'apprentissage constant ou ajust\u00e9.","title":"optims()"},{"location":"#device","text":"variable principale d'allocation des performances","title":"device"},{"location":"#apple-silicon-macos","text":"Si le syst\u00e8me d'exploitation est macOS (nomm\u00e9 darwin dans platform.system() ), la fonction v\u00e9rifie si l'acc\u00e9l\u00e9rateur Metal Performance Shaders (MPS) est disponible sur l'appareil. Si MPS est disponible ( torch.backends.mps.is_available() ), l'appareil cible sera d\u00e9fini sur MPS (c'est un \u00e9quivalent de CUDA pour les appareils Apple Silicon).","title":"Apple Silicon (macOS)"},{"location":"#windows","text":"Si le syst\u00e8me d'exploitation est Windows, la fonction v\u00e9rifie d'abord si CUDA (NVIDIA) est disponible avec torch.cuda.is_available() . Si c'est le cas, le p\u00e9riph\u00e9rique sera d\u00e9fini sur CUDA .","title":"Windows"},{"location":"#linux","text":"Si le syst\u00e8me d'exploitation est Linux, plusieurs v\u00e9rifications sont effectu\u00e9es : CUDA (NVIDIA) : Si torch.cuda.is_available() renvoie True , le p\u00e9riph\u00e9rique sera d\u00e9fini sur CUDA . ROCm (AMD) : Si le syst\u00e8me supporte ROCm via torch.backends.hip.is_available() , l'appareil sera d\u00e9fini sur CUDA (ROCm est utilis\u00e9 pour les cartes AMD dans le cadre de l'API CUDA). Intel oneAPI / XPU : Si le syst\u00e8me prend en charge Intel oneAPI ou XPU via torch.xpu.is_available() , le p\u00e9riph\u00e9rique sera d\u00e9fini sur XPU .","title":"Linux"},{"location":"#systeme-non-reconnu","text":"Si aucune des conditions ci-dessus n'est remplie, la fonction retourne CPU comme p\u00e9riph\u00e9rique par d\u00e9faut.","title":"Syst\u00e8me non reconnu"},{"location":"#parametres-matplotlib-et-pytorch","text":"Style global pour fond transparent et texte gris Optimisations CUDA activ\u00e9es pour TF32, matmul et convolutions Autograd configur\u00e9 pour privil\u00e9gier les performances","title":"Param\u00e8tres matplotlib et PyTorch"}]}