{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"NeuralNetworks Module \u00b6 Module complet pour la cr\u00e9ation, l'entra\u00eenement et la visualisation de Multi-Layer Perceptrons (MLP) avec encodage optionnel Fourier, gestion automatique des pertes, compilation Torch et outils de traitement d'images pour l'apprentissage sur des images RGB. Contenu principal \u00b6 Classes \u00b6 MLP \u00b6 Multi-Layer Perceptron (MLP) avec options avanc\u00e9es : Multi encodage Fourier gaussien (RFF) optionnel Stockage automatique des pertes. Compilation Torch optionnelle pour acc\u00e9l\u00e9rer l\u2019inf\u00e9rence. Gestion flexible de l\u2019optimiseur, de la fonction de perte et de la normalisation. Adaptation du learning rate en fonction des r\u00e9sidus. M\u00e9thodes principales : MLP(layers, learning_rate, Fourier, optim, crit, norm, name, Iscompiled) Initialise le r\u00e9seau avec toutes les options. Les valeurs possibles de optim sont disponibles avec optims() Les valeurs possibles de crit sont disponibles avec crits() Les valeurs possibles de norm sont disponibles avec norms() train(inputs, outputs, num_epochs, batch_size) Entra\u00eene le MLP sur des donn\u00e9es ( inputs \u2192 outputs ). params() Retourne tous les poids du MLP (ligne par ligne) sous forme de liste de numpy.ndarray . nb_params() Calcule le nombre total de poids dans le MLP. neurons() Retourne la liste des biais (neurones) de toutes les couches lin\u00e9aires. Objets et dictionnaires \u00b6 norms() \u00b6 Valeurs Module PyTorch Description \"Relu\" nn.ReLU() Fonction d'activation ReLU classique (Rectified Linear Unit). \"LeakyRelu\" nn.LeakyReLU() ReLU avec un petit coefficient pour les valeurs n\u00e9gatives (param\u00e8tre negative_slope ). \"ELU\" nn.ELU() Fonction d'activation ELU (Exponential Linear Unit), qui a une meilleure gestion des valeurs n\u00e9gatives. \"SELU\" nn.SELU() SELU (Scaled Exponential Linear Unit), une version am\u00e9lior\u00e9e de l'ELU pour des r\u00e9seaux auto-normalisants. \"GELU\" nn.GELU() GELU (Gaussian Error Linear Unit), une activation probabiliste bas\u00e9e sur une fonction gaussienne. \"Sigmoid\" nn.Sigmoid() Fonction d'activation Sigmoid, qui produit une sortie entre 0 et 1. \"Tanh\" nn.Tanh() Fonction d'activation Tanh, avec une sortie dans l'intervalle [-1, 1]. \"Hardtanh\" nn.Hardtanh() Variante de Tanh, avec des sorties limit\u00e9es entre une plage sp\u00e9cifi\u00e9e. \"Softplus\" nn.Softplus() Fonction d'activation qui approxime ReLU mais de mani\u00e8re liss\u00e9e. \"Softsign\" nn.Softsign() Fonction d'activation similaire \u00e0 Tanh mais plus souple, avec des valeurs dans [-1, 1]. crits() \u00b6 Valeurs Module PyTorch Description \"MSE\" nn.MSELoss() Mean Squared Error Loss, utilis\u00e9e pour les r\u00e9gressions. \"L1\" nn.L1Loss() L1 Loss (erreur absolue), souvent utilis\u00e9e pour la r\u00e9gularisation. \"SmoothL1\" nn.SmoothL1Loss() Smooth L1 Loss, une combinaison de L1 et de MSE, moins sensible aux outliers. \"Huber\" nn.HuberLoss() Fonction de perte Huber, une version liss\u00e9e de L1 et MSE, moins affect\u00e9e par les grands \u00e9carts. \"CrossEntropy\" nn.CrossEntropyLoss() Perte de Cross-Entropy, utilis\u00e9e pour les probl\u00e8mes de classification multi-classes. \"KLDiv\" nn.KLDivLoss() Perte de divergence de Kullback-Leibler, souvent utilis\u00e9e pour des mod\u00e8les probabilistes. \"PoissonNLL\" nn.PoissonNLLLoss() Perte de log-vraisemblance pour une distribution de Poisson, utilis\u00e9e pour la mod\u00e9lisation de comptages. \"MultiLabelSoftMargin\" nn.MultiLabelSoftMarginLoss() Perte utilis\u00e9e pour les probl\u00e8mes de classification multi-\u00e9tiquettes. optims() \u00b6 Valeurs Module PyTorch Description \"Adadelta\" optim.Adadelta() Optimiseur Adadelta, bas\u00e9 sur les gradients adaptatifs, sans n\u00e9cessit\u00e9 de r\u00e9glage du taux d'apprentissage. \"Adafactor\" optim.Adafactor() Optimiseur Adafactor, variant d'Adam avec une mise \u00e0 jour plus efficace de la m\u00e9moire pour de grands mod\u00e8les. \"Adam\" optim.Adam() Optimiseur Adam, utilisant un gradient stochastique adaptatif avec des moyennes mobiles des gradients et des carr\u00e9s des gradients. \"AdamW\" optim.AdamW() Optimiseur Adam avec une r\u00e9gularisation L2 (weight decay) distincte, plus efficace que Adam avec weight_decay . \"Adamax\" optim.Adamax() Version d'Adam utilisant une norme infinie pour les gradients, plus stable pour certaines configurations. \"ASGD\" optim.ASGD() Optimiseur ASGD (Averaged Stochastic Gradient Descent), utilis\u00e9 pour de grandes donn\u00e9es avec une moyenne des gradients. \"NAdam\" optim.NAdam() Optimiseur NAdam, une version am\u00e9lior\u00e9e d'Adam avec une adaptation des moments de second ordre. \"RAdam\" optim.RAdam() Optimiseur RAdam, une version robuste de l'Adam qui ajuste dynamiquement les moments pour stabiliser l'entra\u00eenement. \"RMSprop\" optim.RMSprop() Optimiseur RMSprop, utilisant une moyenne mobile des carr\u00e9s des gradients pour r\u00e9duire les oscillations. \"Rprop\" optim.Rprop() Optimiseur Rprop, bas\u00e9 sur les mises \u00e0 jour des poids ind\u00e9pendantes des gradients. \"SGD\" optim.SGD() Descente de gradient stochastique classique, souvent utilis\u00e9e avec un taux d'apprentissage constant ou ajust\u00e9. Device et configuration \u00b6 Apple Silicon (macOS) \u00b6 Si le syst\u00e8me d'exploitation est macOS (nomm\u00e9 darwin dans platform.system() ), la fonction v\u00e9rifie si l'acc\u00e9l\u00e9rateur Metal Performance Shaders (MPS) est disponible sur l'appareil. Si MPS est disponible ( torch.backends.mps.is_available() ), l'appareil cible sera d\u00e9fini sur MPS (c'est un \u00e9quivalent de CUDA pour les appareils Apple Silicon). Windows \u00b6 Si le syst\u00e8me d'exploitation est Windows, la fonction v\u00e9rifie d'abord si CUDA (NVIDIA) est disponible avec torch.cuda.is_available() . Si c'est le cas, le p\u00e9riph\u00e9rique sera d\u00e9fini sur CUDA . Linux \u00b6 Si le syst\u00e8me d'exploitation est Linux, plusieurs v\u00e9rifications sont effectu\u00e9es : CUDA (NVIDIA) : Si torch.cuda.is_available() renvoie True , le p\u00e9riph\u00e9rique sera d\u00e9fini sur CUDA . ROCm (AMD) : Si le syst\u00e8me supporte ROCm via torch.backends.hip.is_available() , l'appareil sera d\u00e9fini sur CUDA (ROCm est utilis\u00e9 pour les cartes AMD dans le cadre de l'API CUDA). Intel oneAPI / XPU : Si le syst\u00e8me prend en charge Intel oneAPI ou XPU via torch.xpu.is_available() , le p\u00e9riph\u00e9rique sera d\u00e9fini sur XPU . Syst\u00e8me non reconnu \u00b6 Si aucune des conditions ci-dessus n'est remplie, la fonction retourne CPU comme p\u00e9riph\u00e9rique par d\u00e9faut. Param\u00e8tres matplotlib et PyTorch \u00b6 Style global pour fond transparent et texte gris Optimisations CUDA activ\u00e9es pour TF32, matmul et convolutions Autograd configur\u00e9 pour privil\u00e9gier les performances","title":"Accueil"},{"location":"#neuralnetworks-module","text":"Module complet pour la cr\u00e9ation, l'entra\u00eenement et la visualisation de Multi-Layer Perceptrons (MLP) avec encodage optionnel Fourier, gestion automatique des pertes, compilation Torch et outils de traitement d'images pour l'apprentissage sur des images RGB.","title":"NeuralNetworks Module"},{"location":"#contenu-principal","text":"","title":"Contenu principal"},{"location":"#classes","text":"","title":"Classes"},{"location":"#mlp","text":"Multi-Layer Perceptron (MLP) avec options avanc\u00e9es : Multi encodage Fourier gaussien (RFF) optionnel Stockage automatique des pertes. Compilation Torch optionnelle pour acc\u00e9l\u00e9rer l\u2019inf\u00e9rence. Gestion flexible de l\u2019optimiseur, de la fonction de perte et de la normalisation. Adaptation du learning rate en fonction des r\u00e9sidus. M\u00e9thodes principales : MLP(layers, learning_rate, Fourier, optim, crit, norm, name, Iscompiled) Initialise le r\u00e9seau avec toutes les options. Les valeurs possibles de optim sont disponibles avec optims() Les valeurs possibles de crit sont disponibles avec crits() Les valeurs possibles de norm sont disponibles avec norms() train(inputs, outputs, num_epochs, batch_size) Entra\u00eene le MLP sur des donn\u00e9es ( inputs \u2192 outputs ). params() Retourne tous les poids du MLP (ligne par ligne) sous forme de liste de numpy.ndarray . nb_params() Calcule le nombre total de poids dans le MLP. neurons() Retourne la liste des biais (neurones) de toutes les couches lin\u00e9aires.","title":"MLP"},{"location":"#objets-et-dictionnaires","text":"","title":"Objets et dictionnaires"},{"location":"#norms","text":"Valeurs Module PyTorch Description \"Relu\" nn.ReLU() Fonction d'activation ReLU classique (Rectified Linear Unit). \"LeakyRelu\" nn.LeakyReLU() ReLU avec un petit coefficient pour les valeurs n\u00e9gatives (param\u00e8tre negative_slope ). \"ELU\" nn.ELU() Fonction d'activation ELU (Exponential Linear Unit), qui a une meilleure gestion des valeurs n\u00e9gatives. \"SELU\" nn.SELU() SELU (Scaled Exponential Linear Unit), une version am\u00e9lior\u00e9e de l'ELU pour des r\u00e9seaux auto-normalisants. \"GELU\" nn.GELU() GELU (Gaussian Error Linear Unit), une activation probabiliste bas\u00e9e sur une fonction gaussienne. \"Sigmoid\" nn.Sigmoid() Fonction d'activation Sigmoid, qui produit une sortie entre 0 et 1. \"Tanh\" nn.Tanh() Fonction d'activation Tanh, avec une sortie dans l'intervalle [-1, 1]. \"Hardtanh\" nn.Hardtanh() Variante de Tanh, avec des sorties limit\u00e9es entre une plage sp\u00e9cifi\u00e9e. \"Softplus\" nn.Softplus() Fonction d'activation qui approxime ReLU mais de mani\u00e8re liss\u00e9e. \"Softsign\" nn.Softsign() Fonction d'activation similaire \u00e0 Tanh mais plus souple, avec des valeurs dans [-1, 1].","title":"norms()"},{"location":"#crits","text":"Valeurs Module PyTorch Description \"MSE\" nn.MSELoss() Mean Squared Error Loss, utilis\u00e9e pour les r\u00e9gressions. \"L1\" nn.L1Loss() L1 Loss (erreur absolue), souvent utilis\u00e9e pour la r\u00e9gularisation. \"SmoothL1\" nn.SmoothL1Loss() Smooth L1 Loss, une combinaison de L1 et de MSE, moins sensible aux outliers. \"Huber\" nn.HuberLoss() Fonction de perte Huber, une version liss\u00e9e de L1 et MSE, moins affect\u00e9e par les grands \u00e9carts. \"CrossEntropy\" nn.CrossEntropyLoss() Perte de Cross-Entropy, utilis\u00e9e pour les probl\u00e8mes de classification multi-classes. \"KLDiv\" nn.KLDivLoss() Perte de divergence de Kullback-Leibler, souvent utilis\u00e9e pour des mod\u00e8les probabilistes. \"PoissonNLL\" nn.PoissonNLLLoss() Perte de log-vraisemblance pour une distribution de Poisson, utilis\u00e9e pour la mod\u00e9lisation de comptages. \"MultiLabelSoftMargin\" nn.MultiLabelSoftMarginLoss() Perte utilis\u00e9e pour les probl\u00e8mes de classification multi-\u00e9tiquettes.","title":"crits()"},{"location":"#optims","text":"Valeurs Module PyTorch Description \"Adadelta\" optim.Adadelta() Optimiseur Adadelta, bas\u00e9 sur les gradients adaptatifs, sans n\u00e9cessit\u00e9 de r\u00e9glage du taux d'apprentissage. \"Adafactor\" optim.Adafactor() Optimiseur Adafactor, variant d'Adam avec une mise \u00e0 jour plus efficace de la m\u00e9moire pour de grands mod\u00e8les. \"Adam\" optim.Adam() Optimiseur Adam, utilisant un gradient stochastique adaptatif avec des moyennes mobiles des gradients et des carr\u00e9s des gradients. \"AdamW\" optim.AdamW() Optimiseur Adam avec une r\u00e9gularisation L2 (weight decay) distincte, plus efficace que Adam avec weight_decay . \"Adamax\" optim.Adamax() Version d'Adam utilisant une norme infinie pour les gradients, plus stable pour certaines configurations. \"ASGD\" optim.ASGD() Optimiseur ASGD (Averaged Stochastic Gradient Descent), utilis\u00e9 pour de grandes donn\u00e9es avec une moyenne des gradients. \"NAdam\" optim.NAdam() Optimiseur NAdam, une version am\u00e9lior\u00e9e d'Adam avec une adaptation des moments de second ordre. \"RAdam\" optim.RAdam() Optimiseur RAdam, une version robuste de l'Adam qui ajuste dynamiquement les moments pour stabiliser l'entra\u00eenement. \"RMSprop\" optim.RMSprop() Optimiseur RMSprop, utilisant une moyenne mobile des carr\u00e9s des gradients pour r\u00e9duire les oscillations. \"Rprop\" optim.Rprop() Optimiseur Rprop, bas\u00e9 sur les mises \u00e0 jour des poids ind\u00e9pendantes des gradients. \"SGD\" optim.SGD() Descente de gradient stochastique classique, souvent utilis\u00e9e avec un taux d'apprentissage constant ou ajust\u00e9.","title":"optims()"},{"location":"#device-et-configuration","text":"","title":"Device et configuration"},{"location":"#apple-silicon-macos","text":"Si le syst\u00e8me d'exploitation est macOS (nomm\u00e9 darwin dans platform.system() ), la fonction v\u00e9rifie si l'acc\u00e9l\u00e9rateur Metal Performance Shaders (MPS) est disponible sur l'appareil. Si MPS est disponible ( torch.backends.mps.is_available() ), l'appareil cible sera d\u00e9fini sur MPS (c'est un \u00e9quivalent de CUDA pour les appareils Apple Silicon).","title":"Apple Silicon (macOS)"},{"location":"#windows","text":"Si le syst\u00e8me d'exploitation est Windows, la fonction v\u00e9rifie d'abord si CUDA (NVIDIA) est disponible avec torch.cuda.is_available() . Si c'est le cas, le p\u00e9riph\u00e9rique sera d\u00e9fini sur CUDA .","title":"Windows"},{"location":"#linux","text":"Si le syst\u00e8me d'exploitation est Linux, plusieurs v\u00e9rifications sont effectu\u00e9es : CUDA (NVIDIA) : Si torch.cuda.is_available() renvoie True , le p\u00e9riph\u00e9rique sera d\u00e9fini sur CUDA . ROCm (AMD) : Si le syst\u00e8me supporte ROCm via torch.backends.hip.is_available() , l'appareil sera d\u00e9fini sur CUDA (ROCm est utilis\u00e9 pour les cartes AMD dans le cadre de l'API CUDA). Intel oneAPI / XPU : Si le syst\u00e8me prend en charge Intel oneAPI ou XPU via torch.xpu.is_available() , le p\u00e9riph\u00e9rique sera d\u00e9fini sur XPU .","title":"Linux"},{"location":"#systeme-non-reconnu","text":"Si aucune des conditions ci-dessus n'est remplie, la fonction retourne CPU comme p\u00e9riph\u00e9rique par d\u00e9faut.","title":"Syst\u00e8me non reconnu"},{"location":"#parametres-matplotlib-et-pytorch","text":"Style global pour fond transparent et texte gris Optimisations CUDA activ\u00e9es pour TF32, matmul et convolutions Autograd configur\u00e9 pour privil\u00e9gier les performances","title":"Param\u00e8tres matplotlib et PyTorch"},{"location":"placeholder/","text":"WIP \u00b6","title":"Latent"},{"location":"placeholder/#wip","text":"","title":"WIP"},{"location":"MLP/","text":"Multi-Layer Perceptron (MLP) \u00b6 Multi-Layer Perceptron (MLP) avec encodage optionnel Fourier (RFF), suivi automatique des pertes, visualisation et compilation PyTorch. Cette classe fournit : Un MLP enti\u00e8rement configurable (dimensions, normalisation, activation) Option d'encodage Fourier (Random Fourier Features) sur les entr\u00e9es M\u00e9thodes pour entra\u00eener le r\u00e9seau avec mini-batchs et AMP (Automatic Mixed Precision) Visualisation de l'architecture via visualtorch Suivi et affichage de la perte d'entra\u00eenement Acc\u00e8s aux poids, biais et nombre de param\u00e8tres Compilation du mod\u00e8le via torch.compile pour acc\u00e9l\u00e9rer l'inf\u00e9rence Parameters \u00b6 Parameter Type Optional Description layers list[int] Yes Dimensions successives du r\u00e9seau (entr\u00e9e \u2192 couches cach\u00e9es \u2192 sortie). Exemple : [in_features, hidden1, hidden2, ..., out_features] . Default: [1, 1, 1] init_lr float Yes Taux d\u2019apprentissage initial pour l\u2019optimiseur. Default: 1e-3 Fourier list[float] Oui Liste de sigma pour encodages RFF. Si None : passthrough. Default: None optim str Yes Nom de l\u2019optimiseur \u00e0 utiliser (doit exister dans optims() ). Default: \"Adam\" crit str Yes Fonction de perte \u00e0 utiliser (doit exister dans crits() ). Default: \"MSE\" norm str Yes Type de normalisation / activation pour les couches cach\u00e9es (ex: \"Relu\" ). Default: \"Relu\" name str Yes Nom du r\u00e9seau pour identification ou affichage. Default: \"Net\" Iscompiled bool Yes Si True, compile le mod\u00e8le via torch.compile pour acc\u00e9l\u00e9rer l\u2019inf\u00e9rence. Default: True Attributes \u00b6 losses : list[torch.Tensor] \u2014 Historique des pertes cumul\u00e9es lors de l'entra\u00eenement layers : list[int] \u2014 Dimensions du r\u00e9seau, ajust\u00e9es si encodage Fourier actif encodings : liste de modules d'encodage ( RFF ou Identity ) norm : nn.Module \u2014 Normalisation ou activation utilis\u00e9e dans les couches cach\u00e9es crit : nn.Module \u2014 Fonction de perte PyTorch sur le device sp\u00e9cifi\u00e9 model : nn.Sequential \u2014 MLP complet construit dynamiquement optim : torch.optim.Optimizer \u2014 Optimiseur associ\u00e9 au MLP name : str \u2014 Nom du r\u00e9seau f : couche lin\u00e9aire finale combinant sorties encodages Methods \u00b6 plot(inputs, img_array) \u2014 Affiche l\u2019image originale, l\u2019image pr\u00e9dite et la courbe des pertes train(inputs, outputs, num_epochs=1500, batch_size=1024) \u2014 Entra\u00eene le MLP avec mini-batchs et AMP, stocke les pertes params() \u2014 Retourne tous les poids du MLP sous forme de liste d\u2019 ndarray neurons() \u2014 Retourne tous les biais du MLP sous forme de liste d\u2019 ndarray nb_params() \u2014 Calcule le nombre total de param\u00e8tres (poids uniquement) du r\u00e9seau Objets et dictionnaires \u00b6 norms() \u00b6 Valeurs Module PyTorch Description \"Relu\" nn.ReLU() Fonction d'activation ReLU classique (Rectified Linear Unit). \"LeakyRelu\" nn.LeakyReLU() ReLU avec un petit coefficient pour les valeurs n\u00e9gatives (param\u00e8tre negative_slope ). \"ELU\" nn.ELU() Fonction d'activation ELU (Exponential Linear Unit), qui a une meilleure gestion des valeurs n\u00e9gatives. \"SELU\" nn.SELU() SELU (Scaled Exponential Linear Unit), une version am\u00e9lior\u00e9e de l'ELU pour des r\u00e9seaux auto-normalisants. \"GELU\" nn.GELU() GELU (Gaussian Error Linear Unit), une activation probabiliste bas\u00e9e sur une fonction gaussienne. \"Sigmoid\" nn.Sigmoid() Fonction d'activation Sigmoid, qui produit une sortie entre 0 et 1. \"Tanh\" nn.Tanh() Fonction d'activation Tanh, avec une sortie dans l'intervalle [-1, 1]. \"Hardtanh\" nn.Hardtanh() Variante de Tanh, avec des sorties limit\u00e9es entre une plage sp\u00e9cifi\u00e9e. \"Softplus\" nn.Softplus() Fonction d'activation qui approxime ReLU mais de mani\u00e8re liss\u00e9e. \"Softsign\" nn.Softsign() Fonction d'activation similaire \u00e0 Tanh mais plus souple, avec des valeurs dans [-1, 1]. crits() \u00b6 Valeurs Module PyTorch Description \"MSE\" nn.MSELoss() Mean Squared Error Loss, utilis\u00e9e pour les r\u00e9gressions. \"L1\" nn.L1Loss() L1 Loss (erreur absolue), souvent utilis\u00e9e pour la r\u00e9gularisation. \"SmoothL1\" nn.SmoothL1Loss() Smooth L1 Loss, une combinaison de L1 et de MSE, moins sensible aux outliers. \"Huber\" nn.HuberLoss() Fonction de perte Huber, une version liss\u00e9e de L1 et MSE, moins affect\u00e9e par les grands \u00e9carts. \"CrossEntropy\" nn.CrossEntropyLoss() Perte de Cross-Entropy, utilis\u00e9e pour les probl\u00e8mes de classification multi-classes. \"KLDiv\" nn.KLDivLoss() Perte de divergence de Kullback-Leibler, souvent utilis\u00e9e pour des mod\u00e8les probabilistes. \"PoissonNLL\" nn.PoissonNLLLoss() Perte de log-vraisemblance pour une distribution de Poisson, utilis\u00e9e pour la mod\u00e9lisation de comptages. \"MultiLabelSoftMargin\" nn.MultiLabelSoftMarginLoss() Perte utilis\u00e9e pour les probl\u00e8mes de classification multi-\u00e9tiquettes. optims() \u00b6 Valeurs Module PyTorch Description \"Adadelta\" optim.Adadelta() Optimiseur Adadelta, bas\u00e9 sur les gradients adaptatifs, sans n\u00e9cessit\u00e9 de r\u00e9glage du taux d'apprentissage. \"Adafactor\" optim.Adafactor() Optimiseur Adafactor, variant d'Adam avec une mise \u00e0 jour plus efficace de la m\u00e9moire pour de grands mod\u00e8les. \"Adam\" optim.Adam() Optimiseur Adam, utilisant un gradient stochastique adaptatif avec des moyennes mobiles des gradients et des carr\u00e9s des gradients. \"AdamW\" optim.AdamW() Optimiseur Adam avec une r\u00e9gularisation L2 (weight decay) distincte, plus efficace que Adam avec weight_decay . \"Adamax\" optim.Adamax() Version d'Adam utilisant une norme infinie pour les gradients, plus stable pour certaines configurations. \"ASGD\" optim.ASGD() Optimiseur ASGD (Averaged Stochastic Gradient Descent), utilis\u00e9 pour de grandes donn\u00e9es avec une moyenne des gradients. \"NAdam\" optim.NAdam() Optimiseur NAdam, une version am\u00e9lior\u00e9e d'Adam avec une adaptation des moments de second ordre. \"RAdam\" optim.RAdam() Optimiseur RAdam, une version robuste de l'Adam qui ajuste dynamiquement les moments pour stabiliser l'entra\u00eenement. \"RMSprop\" optim.RMSprop() Optimiseur RMSprop, utilisant une moyenne mobile des carr\u00e9s des gradients pour r\u00e9duire les oscillations. \"Rprop\" optim.Rprop() Optimiseur Rprop, bas\u00e9 sur les mises \u00e0 jour des poids ind\u00e9pendantes des gradients. \"SGD\" optim.SGD() Descente de gradient stochastique classique, souvent utilis\u00e9e avec un taux d'apprentissage constant ou ajust\u00e9.","title":"MLP"},{"location":"MLP/#multi-layer-perceptron-mlp","text":"Multi-Layer Perceptron (MLP) avec encodage optionnel Fourier (RFF), suivi automatique des pertes, visualisation et compilation PyTorch. Cette classe fournit : Un MLP enti\u00e8rement configurable (dimensions, normalisation, activation) Option d'encodage Fourier (Random Fourier Features) sur les entr\u00e9es M\u00e9thodes pour entra\u00eener le r\u00e9seau avec mini-batchs et AMP (Automatic Mixed Precision) Visualisation de l'architecture via visualtorch Suivi et affichage de la perte d'entra\u00eenement Acc\u00e8s aux poids, biais et nombre de param\u00e8tres Compilation du mod\u00e8le via torch.compile pour acc\u00e9l\u00e9rer l'inf\u00e9rence","title":"Multi-Layer Perceptron (MLP)"},{"location":"MLP/#parameters","text":"Parameter Type Optional Description layers list[int] Yes Dimensions successives du r\u00e9seau (entr\u00e9e \u2192 couches cach\u00e9es \u2192 sortie). Exemple : [in_features, hidden1, hidden2, ..., out_features] . Default: [1, 1, 1] init_lr float Yes Taux d\u2019apprentissage initial pour l\u2019optimiseur. Default: 1e-3 Fourier list[float] Oui Liste de sigma pour encodages RFF. Si None : passthrough. Default: None optim str Yes Nom de l\u2019optimiseur \u00e0 utiliser (doit exister dans optims() ). Default: \"Adam\" crit str Yes Fonction de perte \u00e0 utiliser (doit exister dans crits() ). Default: \"MSE\" norm str Yes Type de normalisation / activation pour les couches cach\u00e9es (ex: \"Relu\" ). Default: \"Relu\" name str Yes Nom du r\u00e9seau pour identification ou affichage. Default: \"Net\" Iscompiled bool Yes Si True, compile le mod\u00e8le via torch.compile pour acc\u00e9l\u00e9rer l\u2019inf\u00e9rence. Default: True","title":"Parameters"},{"location":"MLP/#attributes","text":"losses : list[torch.Tensor] \u2014 Historique des pertes cumul\u00e9es lors de l'entra\u00eenement layers : list[int] \u2014 Dimensions du r\u00e9seau, ajust\u00e9es si encodage Fourier actif encodings : liste de modules d'encodage ( RFF ou Identity ) norm : nn.Module \u2014 Normalisation ou activation utilis\u00e9e dans les couches cach\u00e9es crit : nn.Module \u2014 Fonction de perte PyTorch sur le device sp\u00e9cifi\u00e9 model : nn.Sequential \u2014 MLP complet construit dynamiquement optim : torch.optim.Optimizer \u2014 Optimiseur associ\u00e9 au MLP name : str \u2014 Nom du r\u00e9seau f : couche lin\u00e9aire finale combinant sorties encodages","title":"Attributes"},{"location":"MLP/#methods","text":"plot(inputs, img_array) \u2014 Affiche l\u2019image originale, l\u2019image pr\u00e9dite et la courbe des pertes train(inputs, outputs, num_epochs=1500, batch_size=1024) \u2014 Entra\u00eene le MLP avec mini-batchs et AMP, stocke les pertes params() \u2014 Retourne tous les poids du MLP sous forme de liste d\u2019 ndarray neurons() \u2014 Retourne tous les biais du MLP sous forme de liste d\u2019 ndarray nb_params() \u2014 Calcule le nombre total de param\u00e8tres (poids uniquement) du r\u00e9seau","title":"Methods"},{"location":"MLP/#objets-et-dictionnaires","text":"","title":"Objets et dictionnaires"},{"location":"MLP/#norms","text":"Valeurs Module PyTorch Description \"Relu\" nn.ReLU() Fonction d'activation ReLU classique (Rectified Linear Unit). \"LeakyRelu\" nn.LeakyReLU() ReLU avec un petit coefficient pour les valeurs n\u00e9gatives (param\u00e8tre negative_slope ). \"ELU\" nn.ELU() Fonction d'activation ELU (Exponential Linear Unit), qui a une meilleure gestion des valeurs n\u00e9gatives. \"SELU\" nn.SELU() SELU (Scaled Exponential Linear Unit), une version am\u00e9lior\u00e9e de l'ELU pour des r\u00e9seaux auto-normalisants. \"GELU\" nn.GELU() GELU (Gaussian Error Linear Unit), une activation probabiliste bas\u00e9e sur une fonction gaussienne. \"Sigmoid\" nn.Sigmoid() Fonction d'activation Sigmoid, qui produit une sortie entre 0 et 1. \"Tanh\" nn.Tanh() Fonction d'activation Tanh, avec une sortie dans l'intervalle [-1, 1]. \"Hardtanh\" nn.Hardtanh() Variante de Tanh, avec des sorties limit\u00e9es entre une plage sp\u00e9cifi\u00e9e. \"Softplus\" nn.Softplus() Fonction d'activation qui approxime ReLU mais de mani\u00e8re liss\u00e9e. \"Softsign\" nn.Softsign() Fonction d'activation similaire \u00e0 Tanh mais plus souple, avec des valeurs dans [-1, 1].","title":"norms()"},{"location":"MLP/#crits","text":"Valeurs Module PyTorch Description \"MSE\" nn.MSELoss() Mean Squared Error Loss, utilis\u00e9e pour les r\u00e9gressions. \"L1\" nn.L1Loss() L1 Loss (erreur absolue), souvent utilis\u00e9e pour la r\u00e9gularisation. \"SmoothL1\" nn.SmoothL1Loss() Smooth L1 Loss, une combinaison de L1 et de MSE, moins sensible aux outliers. \"Huber\" nn.HuberLoss() Fonction de perte Huber, une version liss\u00e9e de L1 et MSE, moins affect\u00e9e par les grands \u00e9carts. \"CrossEntropy\" nn.CrossEntropyLoss() Perte de Cross-Entropy, utilis\u00e9e pour les probl\u00e8mes de classification multi-classes. \"KLDiv\" nn.KLDivLoss() Perte de divergence de Kullback-Leibler, souvent utilis\u00e9e pour des mod\u00e8les probabilistes. \"PoissonNLL\" nn.PoissonNLLLoss() Perte de log-vraisemblance pour une distribution de Poisson, utilis\u00e9e pour la mod\u00e9lisation de comptages. \"MultiLabelSoftMargin\" nn.MultiLabelSoftMarginLoss() Perte utilis\u00e9e pour les probl\u00e8mes de classification multi-\u00e9tiquettes.","title":"crits()"},{"location":"MLP/#optims","text":"Valeurs Module PyTorch Description \"Adadelta\" optim.Adadelta() Optimiseur Adadelta, bas\u00e9 sur les gradients adaptatifs, sans n\u00e9cessit\u00e9 de r\u00e9glage du taux d'apprentissage. \"Adafactor\" optim.Adafactor() Optimiseur Adafactor, variant d'Adam avec une mise \u00e0 jour plus efficace de la m\u00e9moire pour de grands mod\u00e8les. \"Adam\" optim.Adam() Optimiseur Adam, utilisant un gradient stochastique adaptatif avec des moyennes mobiles des gradients et des carr\u00e9s des gradients. \"AdamW\" optim.AdamW() Optimiseur Adam avec une r\u00e9gularisation L2 (weight decay) distincte, plus efficace que Adam avec weight_decay . \"Adamax\" optim.Adamax() Version d'Adam utilisant une norme infinie pour les gradients, plus stable pour certaines configurations. \"ASGD\" optim.ASGD() Optimiseur ASGD (Averaged Stochastic Gradient Descent), utilis\u00e9 pour de grandes donn\u00e9es avec une moyenne des gradients. \"NAdam\" optim.NAdam() Optimiseur NAdam, une version am\u00e9lior\u00e9e d'Adam avec une adaptation des moments de second ordre. \"RAdam\" optim.RAdam() Optimiseur RAdam, une version robuste de l'Adam qui ajuste dynamiquement les moments pour stabiliser l'entra\u00eenement. \"RMSprop\" optim.RMSprop() Optimiseur RMSprop, utilisant une moyenne mobile des carr\u00e9s des gradients pour r\u00e9duire les oscillations. \"Rprop\" optim.Rprop() Optimiseur Rprop, bas\u00e9 sur les mises \u00e0 jour des poids ind\u00e9pendantes des gradients. \"SGD\" optim.SGD() Descente de gradient stochastique classique, souvent utilis\u00e9e avec un taux d'apprentissage constant ou ajust\u00e9.","title":"optims()"},{"location":"MLP/losses/","text":"losses \u00b6 Affiche les courbes de pertes (training loss) de plusieurs r\u00e9seaux MLP. Param\u00e8tres \u00b6 Param\u00e8tre Type Description *nets MLP ou liste de MLP Un ou plusieurs r\u00e9seaux poss\u00e9dant un attribut .losses contenant l'historique des pertes (liste de float). Notes \u00b6 L\u2019axe X correspond aux it\u00e9rations (epochs ou steps). L\u2019axe Y correspond \u00e0 la valeur de la perte. Utilise matplotlib en mode interactif pour un affichage dynamique.","title":"losses"},{"location":"MLP/losses/#losses","text":"Affiche les courbes de pertes (training loss) de plusieurs r\u00e9seaux MLP.","title":"losses"},{"location":"MLP/losses/#parametres","text":"Param\u00e8tre Type Description *nets MLP ou liste de MLP Un ou plusieurs r\u00e9seaux poss\u00e9dant un attribut .losses contenant l'historique des pertes (liste de float).","title":"Param\u00e8tres"},{"location":"MLP/losses/#notes","text":"L\u2019axe X correspond aux it\u00e9rations (epochs ou steps). L\u2019axe Y correspond \u00e0 la valeur de la perte. Utilise matplotlib en mode interactif pour un affichage dynamique.","title":"Notes"},{"location":"MLP/nb_params/","text":"MLP.nb_params \u00b6 Calcule le nombre total de param\u00e8tres (poids) du MLP. Cette m\u00e9thode parcourt les param\u00e8tres du mod\u00e8le et ne compte que les poids des couches lin\u00e9aires. Dans un nn.Linear , parameters() renvoie dans l'ordre : - les poids (indice pair) - puis les biais (indice impair) On ignore donc les biais dans ce comptage. Retour \u00b6 Type Description int Nombre total de param\u00e8tres pond\u00e9r\u00e9s (weights) dans le MLP.","title":"MLP.nb_params"},{"location":"MLP/nb_params/#mlpnb_params","text":"Calcule le nombre total de param\u00e8tres (poids) du MLP. Cette m\u00e9thode parcourt les param\u00e8tres du mod\u00e8le et ne compte que les poids des couches lin\u00e9aires. Dans un nn.Linear , parameters() renvoie dans l'ordre : - les poids (indice pair) - puis les biais (indice impair) On ignore donc les biais dans ce comptage.","title":"MLP.nb_params"},{"location":"MLP/nb_params/#retour","text":"Type Description int Nombre total de param\u00e8tres pond\u00e9r\u00e9s (weights) dans le MLP.","title":"Retour"},{"location":"MLP/neurons/","text":"MLP.neurons \u00b6 Extrait l'ensemble des neurones (biais) du MLP couche par couche. Cette m\u00e9thode parcourt les param\u00e8tres du mod\u00e8le et r\u00e9cup\u00e8re uniquement les poids associ\u00e9s aux biais des couches lin\u00e9aires. Dans un nn.Linear , parameters() renvoie successivement : - les poids (weights) - puis les biais (bias) Les indices impairs correspondent donc aux biais. Retour \u00b6 Type Description list[np.ndarray] Liste contenant chaque neurone (chaque valeur de biais), converti en array NumPy sur CPU.","title":"MLP.neurons"},{"location":"MLP/neurons/#mlpneurons","text":"Extrait l'ensemble des neurones (biais) du MLP couche par couche. Cette m\u00e9thode parcourt les param\u00e8tres du mod\u00e8le et r\u00e9cup\u00e8re uniquement les poids associ\u00e9s aux biais des couches lin\u00e9aires. Dans un nn.Linear , parameters() renvoie successivement : - les poids (weights) - puis les biais (bias) Les indices impairs correspondent donc aux biais.","title":"MLP.neurons"},{"location":"MLP/neurons/#retour","text":"Type Description list[np.ndarray] Liste contenant chaque neurone (chaque valeur de biais), converti en array NumPy sur CPU.","title":"Retour"},{"location":"MLP/params/","text":"MLP.params \u00b6 Retourne la liste de tous les poids (weights) du MLP. Cette fonction extrait uniquement les matrices de poids des couches lin\u00e9aires, en ignorant les biais. Dans un nn.Linear , parameters() renvoie dans l'ordre : - les poids (indice pair) - les biais (indice impair) La m\u00e9thode : Parcourt les param\u00e8tres du r\u00e9seau S\u00e9lectionne uniquement ceux correspondant aux poids S\u00e9pare chaque ligne de la matrice de poids Convertit chaque ligne en ndarray CPU d\u00e9tach\u00e9 Retour \u00b6 Type Description list[np.ndarray] Liste contenant chaque ligne des matrices de poids (chaque \u00e9l\u00e9ment est un vecteur numpy).","title":"MLP.params"},{"location":"MLP/params/#mlpparams","text":"Retourne la liste de tous les poids (weights) du MLP. Cette fonction extrait uniquement les matrices de poids des couches lin\u00e9aires, en ignorant les biais. Dans un nn.Linear , parameters() renvoie dans l'ordre : - les poids (indice pair) - les biais (indice impair) La m\u00e9thode : Parcourt les param\u00e8tres du r\u00e9seau S\u00e9lectionne uniquement ceux correspondant aux poids S\u00e9pare chaque ligne de la matrice de poids Convertit chaque ligne en ndarray CPU d\u00e9tach\u00e9","title":"MLP.params"},{"location":"MLP/params/#retour","text":"Type Description list[np.ndarray] Liste contenant chaque ligne des matrices de poids (chaque \u00e9l\u00e9ment est un vecteur numpy).","title":"Retour"},{"location":"MLP/train/","text":"train \u00b6 Entra\u00eene un ou plusieurs MLP sur des paires (inputs, outputs) avec gestion optionnelle de l'affichage interactif. Affiche dynamiquement si img_array est fourni : - L'image originale (r\u00e9f\u00e9rence) - Les pr\u00e9dictions des MLP - L'\u00e9volution des pertes au fil des \u00e9poques Param\u00e8tres \u00b6 Param\u00e8tre Type Description inputs array-like Entr\u00e9es du ou des MLP (shape: [n_samples, n_features]). outputs array-like Sorties cibles correspondantes (shape: [n_samples, output_dim]). num_epochs int, optional Nombre d\u2019\u00e9poques pour l\u2019entra\u00eenement (default=1500). batch_size int, optional Taille des mini-batchs pour la descente de gradient (default=1024). *nets MLP ou liste de MLP Un ou plusieurs objets MLP \u00e0 entra\u00eener. img_array np.ndarray (H, W, 3), optional Image de r\u00e9f\u00e9rence pour visualisation des pr\u00e9dictions (default=None). Notes \u00b6 Les MLP sont entra\u00een\u00e9s ind\u00e9pendamment mais avec le m\u00eame ordre al\u00e9atoire d'\u00e9chantillons. Utilise torch.amp.GradScaler pour l'entra\u00eenement en FP16. La visualisation interactive utilise clear_output() et plt.ion() .","title":"MLP.train"},{"location":"MLP/train/#train","text":"Entra\u00eene un ou plusieurs MLP sur des paires (inputs, outputs) avec gestion optionnelle de l'affichage interactif. Affiche dynamiquement si img_array est fourni : - L'image originale (r\u00e9f\u00e9rence) - Les pr\u00e9dictions des MLP - L'\u00e9volution des pertes au fil des \u00e9poques","title":"train"},{"location":"MLP/train/#parametres","text":"Param\u00e8tre Type Description inputs array-like Entr\u00e9es du ou des MLP (shape: [n_samples, n_features]). outputs array-like Sorties cibles correspondantes (shape: [n_samples, output_dim]). num_epochs int, optional Nombre d\u2019\u00e9poques pour l\u2019entra\u00eenement (default=1500). batch_size int, optional Taille des mini-batchs pour la descente de gradient (default=1024). *nets MLP ou liste de MLP Un ou plusieurs objets MLP \u00e0 entra\u00eener. img_array np.ndarray (H, W, 3), optional Image de r\u00e9f\u00e9rence pour visualisation des pr\u00e9dictions (default=None).","title":"Param\u00e8tres"},{"location":"MLP/train/#notes","text":"Les MLP sont entra\u00een\u00e9s ind\u00e9pendamment mais avec le m\u00eame ordre al\u00e9atoire d'\u00e9chantillons. Utilise torch.amp.GradScaler pour l'entra\u00eenement en FP16. La visualisation interactive utilise clear_output() et plt.ion() .","title":"Notes"},{"location":"tools/image/compare/","text":"compare \u00b6 Affiche, pour chaque r\u00e9seau, l\u2019erreur absolue entre l\u2019image originale et l\u2019image reconstruite par le r\u00e9seau. Chaque r\u00e9seau doit poss\u00e9der : - une m\u00e9thode encoding(x) (si RFF activ\u00e9), - un module model retournant un tenseur de shape (N, 3), - une reconstruction compatible avec (H, W, 3). Param\u00e8tres \u00b6 Param\u00e8tre Type Description img_array np.ndarray (H, W, 3) Image originale servant de r\u00e9f\u00e9rence. inputs tensor-like (H*W, 2) Coordonn\u00e9es normalis\u00e9es des pixels correspondant \u00e0 chaque point de l'image. nets MLP ou liste de MLP Un ou plusieurs r\u00e9seaux poss\u00e9dant les m\u00e9thodes .encoding() et .model() , et l\u2019attribut .losses . Notes \u00b6 Affiche la diff\u00e9rence absolue entre l\u2019image originale et la pr\u00e9diction du r\u00e9seau. Les pertes cumul\u00e9es sont \u00e9galement trac\u00e9es pour chaque r\u00e9seau. Utilise matplotlib en mode interactif.","title":"compare"},{"location":"tools/image/compare/#compare","text":"Affiche, pour chaque r\u00e9seau, l\u2019erreur absolue entre l\u2019image originale et l\u2019image reconstruite par le r\u00e9seau. Chaque r\u00e9seau doit poss\u00e9der : - une m\u00e9thode encoding(x) (si RFF activ\u00e9), - un module model retournant un tenseur de shape (N, 3), - une reconstruction compatible avec (H, W, 3).","title":"compare"},{"location":"tools/image/compare/#parametres","text":"Param\u00e8tre Type Description img_array np.ndarray (H, W, 3) Image originale servant de r\u00e9f\u00e9rence. inputs tensor-like (H*W, 2) Coordonn\u00e9es normalis\u00e9es des pixels correspondant \u00e0 chaque point de l'image. nets MLP ou liste de MLP Un ou plusieurs r\u00e9seaux poss\u00e9dant les m\u00e9thodes .encoding() et .model() , et l\u2019attribut .losses .","title":"Param\u00e8tres"},{"location":"tools/image/compare/#notes","text":"Affiche la diff\u00e9rence absolue entre l\u2019image originale et la pr\u00e9diction du r\u00e9seau. Les pertes cumul\u00e9es sont \u00e9galement trac\u00e9es pour chaque r\u00e9seau. Utilise matplotlib en mode interactif.","title":"Notes"},{"location":"tools/image/image_from_url/","text":"image_from_url \u00b6 T\u00e9l\u00e9charge une image depuis une URL, la redimensionne et pr\u00e9pare les donn\u00e9es pour l'entra\u00eenement d'un MLP pixel-wise. Cette fonction retourne\u202f: img_array : image RGB sous forme de tableau NumPy (H, W, 3), pour affichage. inputs : coordonn\u00e9es normalis\u00e9es (x, y) de chaque pixel, sous forme de tenseur (H*W, 2). outputs : valeurs RGB cibles pour chaque pixel, sous forme de tenseur (H*W, 3). Param\u00e8tres \u00b6 Param\u00e8tre Type Description url str URL de l'image \u00e0 t\u00e9l\u00e9charger. img_size int Taille finale carr\u00e9e de l'image (img_size x img_size). Par d\u00e9faut 256. Retours \u00b6 Nom Type Description img_array numpy.ndarray (H, W, 3) Image sous forme de tableau NumPy, valeurs normalis\u00e9es entre 0 et 1. inputs torch.Tensor (H*W, 2) Coordonn\u00e9es normalis\u00e9es des pixels pour l'entr\u00e9e du MLP. outputs torch.Tensor (H*W, 3) Valeurs RGB cibles pour chaque pixel, pour la sortie du MLP. Notes \u00b6 Utilise PIL pour le traitement de l'image et torchvision.transforms pour la conversion en tenseur normalis\u00e9. Les coordonn\u00e9es sont normalis\u00e9es dans [0, 1] pour une utilisation optimale avec des MLP utilisant Fourier Features ou activations standard. Les tenseurs inputs et outputs sont pr\u00eats \u00e0 \u00eatre envoy\u00e9s sur GPU si n\u00e9cessaire.","title":"image_from_url"},{"location":"tools/image/image_from_url/#image_from_url","text":"T\u00e9l\u00e9charge une image depuis une URL, la redimensionne et pr\u00e9pare les donn\u00e9es pour l'entra\u00eenement d'un MLP pixel-wise. Cette fonction retourne\u202f: img_array : image RGB sous forme de tableau NumPy (H, W, 3), pour affichage. inputs : coordonn\u00e9es normalis\u00e9es (x, y) de chaque pixel, sous forme de tenseur (H*W, 2). outputs : valeurs RGB cibles pour chaque pixel, sous forme de tenseur (H*W, 3).","title":"image_from_url"},{"location":"tools/image/image_from_url/#parametres","text":"Param\u00e8tre Type Description url str URL de l'image \u00e0 t\u00e9l\u00e9charger. img_size int Taille finale carr\u00e9e de l'image (img_size x img_size). Par d\u00e9faut 256.","title":"Param\u00e8tres"},{"location":"tools/image/image_from_url/#retours","text":"Nom Type Description img_array numpy.ndarray (H, W, 3) Image sous forme de tableau NumPy, valeurs normalis\u00e9es entre 0 et 1. inputs torch.Tensor (H*W, 2) Coordonn\u00e9es normalis\u00e9es des pixels pour l'entr\u00e9e du MLP. outputs torch.Tensor (H*W, 3) Valeurs RGB cibles pour chaque pixel, pour la sortie du MLP.","title":"Retours"},{"location":"tools/image/image_from_url/#notes","text":"Utilise PIL pour le traitement de l'image et torchvision.transforms pour la conversion en tenseur normalis\u00e9. Les coordonn\u00e9es sont normalis\u00e9es dans [0, 1] pour une utilisation optimale avec des MLP utilisant Fourier Features ou activations standard. Les tenseurs inputs et outputs sont pr\u00eats \u00e0 \u00eatre envoy\u00e9s sur GPU si n\u00e9cessaire.","title":"Notes"},{"location":"tools/image/plot/","text":"plot \u00b6 Affiche, pour chaque r\u00e9seau, l\u2019image reconstruite \u00e0 partir de ses pr\u00e9dictions. Param\u00e8tres \u00b6 Param\u00e8tre Type Description img_array np.ndarray (H, W, 3) Image originale, utilis\u00e9e pour conna\u00eetre les dimensions de reconstruction. inputs tensor-like (H*W, 2) Coordonn\u00e9es normalis\u00e9es des pixels correspondant \u00e0 chaque point de l'image. *nets MLP ou liste de MLP Un ou plusieurs r\u00e9seaux poss\u00e9dant les m\u00e9thodes .encoding() et .model() , et l\u2019attribut .losses . Notes \u00b6 Affiche la pr\u00e9diction brute de chaque r\u00e9seau. Les pertes cumul\u00e9es sont \u00e9galement trac\u00e9es. Utilise matplotlib en mode interactif.","title":"plot"},{"location":"tools/image/plot/#plot","text":"Affiche, pour chaque r\u00e9seau, l\u2019image reconstruite \u00e0 partir de ses pr\u00e9dictions.","title":"plot"},{"location":"tools/image/plot/#parametres","text":"Param\u00e8tre Type Description img_array np.ndarray (H, W, 3) Image originale, utilis\u00e9e pour conna\u00eetre les dimensions de reconstruction. inputs tensor-like (H*W, 2) Coordonn\u00e9es normalis\u00e9es des pixels correspondant \u00e0 chaque point de l'image. *nets MLP ou liste de MLP Un ou plusieurs r\u00e9seaux poss\u00e9dant les m\u00e9thodes .encoding() et .model() , et l\u2019attribut .losses .","title":"Param\u00e8tres"},{"location":"tools/image/plot/#notes","text":"Affiche la pr\u00e9diction brute de chaque r\u00e9seau. Les pertes cumul\u00e9es sont \u00e9galement trac\u00e9es. Utilise matplotlib en mode interactif.","title":"Notes"}]}