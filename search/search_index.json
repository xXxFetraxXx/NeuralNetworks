{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"NeuralNetworks Module \u00b6 Module complet pour la cr\u00e9ation et l'entra\u00eenement de MultiLayer Perceptrons (MLP) avec encodage optionnel Fourier Features et gestion automatique des pertes. Contenu principal \u00b6 Classes \u00b6 MLP {#MLP} \u00b6 Cette classe fournit : Un MLP enti\u00e8rement configurable (dimensions, activation). Option d'encodage Fourier Features sur les entr\u00e9es. Param\u00e8tres \u00b6 Param\u00e8tres Type Optionnel Description input_size int Oui Taille des donn\u00e9es en entr\u00e9e au r\u00e9seau. Default: 1 output_size int Oui Taille des donn\u00e9es en sortie au r\u00e9seau. Default: 1 hidden_layers list[int] Oui Dimensions successives des couches interm\u00e9diaires du r\u00e9seau. Default: [1] sigmas list[float] Oui Liste de sigma pour encodages RFF. Si None : passthrough. Default: None fourier_input_size int Oui WIP. Default: 2 nb_fourier int Oui Nombre de fr\u00e9quences utilis\u00e9es pour les Fourier Features. Default: 8 norm norm Oui Type de normalisation / activation pour les couches cach\u00e9es. Default: 'Relu' name str Oui Nom du r\u00e9seau pour identification ou affichage. Default: 'Net' Attributs \u00b6 losses : list[float] \u2014 Historique des pertes cumul\u00e9es lors de l'entra\u00eenement learnings : list[float] \u2014 Historique des taux d'apprentissage utilis\u00e9es lors de l'entra\u00eenement model : nn.Sequential \u2014 MLP complet construit dynamiquement name : str \u2014 Nom du r\u00e9seau Trainer \u00b6 Cette classe fournit : M\u00e9thode pour entra\u00eener des r\u00e9seaux avec mini-batchs et Automatic Mixed Precision Param\u00e8tres \u00b6 Param\u00e8tres Type Optionnel Description *nets MLP Non R\u00e9seaux pour lesquels le trainer va entrainer. inputs numpy.array(list[float]) Non Donn\u00e9es en entr\u00e9e au r\u00e9seau. outputs numpy.array(list[float]) Non Donn\u00e9es en sortie au r\u00e9seau. test_size float Oui Proportion des donn\u00e9es \u00e0 utiliser pendant l'entrainement. Si None : utilise toutes les donn\u00e9es. Default: None optim optim Oui Nom de l\u2019optimiseur \u00e0 utiliser (doit exister dans optims() ). Default: 'Adam' init_lr float Oui Taux d\u2019apprentissage initial pour l\u2019optimiseur. Default: 1e-3 crit crit Oui Fonction de perte \u00e0 utiliser (doit exister dans crits() ). Default: MSE' batch_size int Oui Taille des minibatchs. Default: 1024 Trainer.train \u00b6 Lancement d'un entrainement avec le trainer d\u00e9finit Param\u00e8tres Type Optionnel Description num_epochs int Oui Nombres d'it\u00e9rations \u00e0 effectuer. activate_tqdm boolean Oui Utilisation d'une barre de progression. M\u00e9thodes \u00b6 losses \u00b6 Affiche les r\u00e9sidus en fonction des \u00e9poques d'entrainement des r\u00e9seaux. learnings \u00b6 Affiche les taux d'apprentissage en fonction des \u00e9poques d'entrainement des r\u00e9seaux. Dictionnaires \u00b6 norms() {#norms} \u00b6 Valeurs Module PyTorch Description 'ReLU' nn.ReLU() Fonction d'activation ReLU classique (Rectified Linear Unit). 'LeakyReLU' nn.LeakyReLU() ReLU avec un petit coefficient pour les valeurs n\u00e9gatives (param\u00e8tre negative_slope ). 'ELU' nn.ELU() Fonction d'activation ELU (Exponential Linear Unit), qui a une meilleure gestion des valeurs n\u00e9gatives. 'SELU' nn.SELU() SELU (Scaled Exponential Linear Unit), une version am\u00e9lior\u00e9e de l'ELU pour des r\u00e9seaux auto-normalisants. 'GELU' nn.GELU() GELU (Gaussian Error Linear Unit), une activation probabiliste bas\u00e9e sur une fonction gaussienne. 'Mish' nn.Mish() ReLU diff\u00e9rentiable en tout points avec passage n\u00e9gatif. 'Softplus' nn.Softplus() Fonction d'activation qui approxime ReLU mais de mani\u00e8re liss\u00e9e. 'Sigmoid' nn.Sigmoid() Fonction d'activation Sigmoid, qui produit une sortie entre 0 et 1. 'Tanh' nn.Tanh() Fonction d'activation Tanh, avec une sortie dans l'intervalle [-1, 1]. 'Hardtanh' nn.Hardtanh() Variante de Tanh, avec des sorties limit\u00e9es entre une plage sp\u00e9cifi\u00e9e. 'Softsign' nn.Softsign() Fonction d'activation similaire \u00e0 Tanh mais plus souple, avec des valeurs dans [-1, 1]. crits() {#crits} \u00b6 Valeurs Module PyTorch Description 'MSE' nn.MSELoss Perte utilis\u00e9e pour les r\u00e9gressions. 'L1' nn.L1Loss() Perte utilis\u00e9e pour la r\u00e9gularisation. 'SmoothL1' nn.SmoothL1Loss() Perte moins sensible aux outliers. 'Huber' nn.HuberLoss() Perte moins affect\u00e9e par les grands \u00e9carts. 'CrossEntropy' nn.CrossEntropyLoss() Perte utilis\u00e9e pour les probl\u00e8mes de classification multi-classes. 'KLDiv' nn.KLDivLoss() Perte utilis\u00e9e pour des mod\u00e8les probabilistes. 'PoissonNLL' nn.PoissonNLLLoss() Perte utilis\u00e9e pour la mod\u00e9lisation de comptages. 'MultiLabelSoftMargin' nn.MultiLabelSoftMarginLoss() Perte utilis\u00e9e pour les probl\u00e8mes de classification multi-\u00e9tiquettes. optims() {#optims} \u00b6 Valeurs Module PyTorch Description 'Adadelta' optim.Adadelta() Optimiseur bas\u00e9 sur les gradients adaptatifs, sans n\u00e9cessit\u00e9 de r\u00e9glage du taux d'apprentissage. 'Adafactor' optim.Adafactor() Optimiseur variant d'Adam avec une mise \u00e0 jour plus efficace de la m\u00e9moire pour de grands mod\u00e8les. 'Adam' optim.Adam() Optimiseur utilisant un gradient stochastique adaptatif avec des moyennes mobiles des gradients. 'AdamW' optim.AdamW() Optimiseur avec une r\u00e9gularisation L2 (weight decay) distincte. 'Adamax' optim.Adamax() Optimiseur utilisant une norme infinie pour les gradients, plus stable pour certaines configurations. 'ASGD' optim.ASGD() Optimiseur utilis\u00e9 pour de grandes donn\u00e9es avec une moyenne des gradients. 'NAdam' optim.NAdam() Optimiseur avec une adaptation des moments de second ordre. 'RAdam' optim.RAdam() Optimiseur qui ajuste dynamiquement les moments pour stabiliser l'entra\u00eenement. 'RMSprop' optim.RMSprop() Optimiseur utilisant une moyenne mobile des carr\u00e9s des gradients pour r\u00e9duire les oscillations. 'Rprop' optim.Rprop() Optimiseur bas\u00e9 sur les mises \u00e0 jour des poids ind\u00e9pendantes des gradients. 'SGD' optim.SGD() Optimiseur souvent utilis\u00e9e avec un taux d'apprentissage constant ou ajust\u00e9. device \u00b6 variable principale d'allocation des performances Apple Silicon (macOS) \u00b6 Si le syst\u00e8me d'exploitation est macOS (nomm\u00e9 darwin dans platform.system() ), la fonction v\u00e9rifie si l'acc\u00e9l\u00e9rateur Metal Performance Shaders (MPS) est disponible sur l'appareil. Si MPS est disponible ( torch.backends.mps.is_available() ), l'appareil cible sera d\u00e9fini sur 'MPS' (c'est un \u00e9quivalent de CUDA pour les appareils Apple Silicon). Windows \u00b6 Si le syst\u00e8me d'exploitation est Windows, la fonction v\u00e9rifie d'abord si CUDA (NVIDIA) est disponible avec torch.cuda.is_available() . Si c'est le cas, le p\u00e9riph\u00e9rique sera d\u00e9fini sur CUDA . Linux \u00b6 Si le syst\u00e8me d'exploitation est Linux, plusieurs v\u00e9rifications sont effectu\u00e9es : CUDA (NVIDIA) : Si torch.cuda.is_available() renvoie True , le p\u00e9riph\u00e9rique sera d\u00e9fini sur 'CUDA' . ROCm (AMD) : Si le syst\u00e8me supporte ROCm via torch.backends.hip.is_available() , l'appareil sera d\u00e9fini sur 'CUDA' (ROCm est utilis\u00e9 pour les cartes AMD dans le cadre de l'API CUDA). Intel oneAPI / XPU : Si le syst\u00e8me prend en charge Intel oneAPI ou XPU via torch.xpu.is_available() , le p\u00e9riph\u00e9rique sera d\u00e9fini sur XPU . Syst\u00e8me non reconnu \u00b6 Si aucune des conditions ci-dessus n'est remplie, la fonction retourne 'CPU' comme p\u00e9riph\u00e9rique par d\u00e9faut.","title":"Accueil"},{"location":"#neuralnetworks-module","text":"Module complet pour la cr\u00e9ation et l'entra\u00eenement de MultiLayer Perceptrons (MLP) avec encodage optionnel Fourier Features et gestion automatique des pertes.","title":"NeuralNetworks Module"},{"location":"#contenu-principal","text":"","title":"Contenu principal"},{"location":"#classes","text":"","title":"Classes"},{"location":"#mlp-mlp","text":"Cette classe fournit : Un MLP enti\u00e8rement configurable (dimensions, activation). Option d'encodage Fourier Features sur les entr\u00e9es.","title":"MLP {#MLP}"},{"location":"#parametres","text":"Param\u00e8tres Type Optionnel Description input_size int Oui Taille des donn\u00e9es en entr\u00e9e au r\u00e9seau. Default: 1 output_size int Oui Taille des donn\u00e9es en sortie au r\u00e9seau. Default: 1 hidden_layers list[int] Oui Dimensions successives des couches interm\u00e9diaires du r\u00e9seau. Default: [1] sigmas list[float] Oui Liste de sigma pour encodages RFF. Si None : passthrough. Default: None fourier_input_size int Oui WIP. Default: 2 nb_fourier int Oui Nombre de fr\u00e9quences utilis\u00e9es pour les Fourier Features. Default: 8 norm norm Oui Type de normalisation / activation pour les couches cach\u00e9es. Default: 'Relu' name str Oui Nom du r\u00e9seau pour identification ou affichage. Default: 'Net'","title":"Param\u00e8tres"},{"location":"#attributs","text":"losses : list[float] \u2014 Historique des pertes cumul\u00e9es lors de l'entra\u00eenement learnings : list[float] \u2014 Historique des taux d'apprentissage utilis\u00e9es lors de l'entra\u00eenement model : nn.Sequential \u2014 MLP complet construit dynamiquement name : str \u2014 Nom du r\u00e9seau","title":"Attributs"},{"location":"#trainer","text":"Cette classe fournit : M\u00e9thode pour entra\u00eener des r\u00e9seaux avec mini-batchs et Automatic Mixed Precision","title":"Trainer"},{"location":"#parametres_1","text":"Param\u00e8tres Type Optionnel Description *nets MLP Non R\u00e9seaux pour lesquels le trainer va entrainer. inputs numpy.array(list[float]) Non Donn\u00e9es en entr\u00e9e au r\u00e9seau. outputs numpy.array(list[float]) Non Donn\u00e9es en sortie au r\u00e9seau. test_size float Oui Proportion des donn\u00e9es \u00e0 utiliser pendant l'entrainement. Si None : utilise toutes les donn\u00e9es. Default: None optim optim Oui Nom de l\u2019optimiseur \u00e0 utiliser (doit exister dans optims() ). Default: 'Adam' init_lr float Oui Taux d\u2019apprentissage initial pour l\u2019optimiseur. Default: 1e-3 crit crit Oui Fonction de perte \u00e0 utiliser (doit exister dans crits() ). Default: MSE' batch_size int Oui Taille des minibatchs. Default: 1024","title":"Param\u00e8tres"},{"location":"#trainertrain","text":"Lancement d'un entrainement avec le trainer d\u00e9finit Param\u00e8tres Type Optionnel Description num_epochs int Oui Nombres d'it\u00e9rations \u00e0 effectuer. activate_tqdm boolean Oui Utilisation d'une barre de progression.","title":"Trainer.train"},{"location":"#methodes","text":"","title":"M\u00e9thodes"},{"location":"#losses","text":"Affiche les r\u00e9sidus en fonction des \u00e9poques d'entrainement des r\u00e9seaux.","title":"losses"},{"location":"#learnings","text":"Affiche les taux d'apprentissage en fonction des \u00e9poques d'entrainement des r\u00e9seaux.","title":"learnings"},{"location":"#dictionnaires","text":"","title":"Dictionnaires"},{"location":"#norms-norms","text":"Valeurs Module PyTorch Description 'ReLU' nn.ReLU() Fonction d'activation ReLU classique (Rectified Linear Unit). 'LeakyReLU' nn.LeakyReLU() ReLU avec un petit coefficient pour les valeurs n\u00e9gatives (param\u00e8tre negative_slope ). 'ELU' nn.ELU() Fonction d'activation ELU (Exponential Linear Unit), qui a une meilleure gestion des valeurs n\u00e9gatives. 'SELU' nn.SELU() SELU (Scaled Exponential Linear Unit), une version am\u00e9lior\u00e9e de l'ELU pour des r\u00e9seaux auto-normalisants. 'GELU' nn.GELU() GELU (Gaussian Error Linear Unit), une activation probabiliste bas\u00e9e sur une fonction gaussienne. 'Mish' nn.Mish() ReLU diff\u00e9rentiable en tout points avec passage n\u00e9gatif. 'Softplus' nn.Softplus() Fonction d'activation qui approxime ReLU mais de mani\u00e8re liss\u00e9e. 'Sigmoid' nn.Sigmoid() Fonction d'activation Sigmoid, qui produit une sortie entre 0 et 1. 'Tanh' nn.Tanh() Fonction d'activation Tanh, avec une sortie dans l'intervalle [-1, 1]. 'Hardtanh' nn.Hardtanh() Variante de Tanh, avec des sorties limit\u00e9es entre une plage sp\u00e9cifi\u00e9e. 'Softsign' nn.Softsign() Fonction d'activation similaire \u00e0 Tanh mais plus souple, avec des valeurs dans [-1, 1].","title":"norms() {#norms}"},{"location":"#crits-crits","text":"Valeurs Module PyTorch Description 'MSE' nn.MSELoss Perte utilis\u00e9e pour les r\u00e9gressions. 'L1' nn.L1Loss() Perte utilis\u00e9e pour la r\u00e9gularisation. 'SmoothL1' nn.SmoothL1Loss() Perte moins sensible aux outliers. 'Huber' nn.HuberLoss() Perte moins affect\u00e9e par les grands \u00e9carts. 'CrossEntropy' nn.CrossEntropyLoss() Perte utilis\u00e9e pour les probl\u00e8mes de classification multi-classes. 'KLDiv' nn.KLDivLoss() Perte utilis\u00e9e pour des mod\u00e8les probabilistes. 'PoissonNLL' nn.PoissonNLLLoss() Perte utilis\u00e9e pour la mod\u00e9lisation de comptages. 'MultiLabelSoftMargin' nn.MultiLabelSoftMarginLoss() Perte utilis\u00e9e pour les probl\u00e8mes de classification multi-\u00e9tiquettes.","title":"crits() {#crits}"},{"location":"#optims-optims","text":"Valeurs Module PyTorch Description 'Adadelta' optim.Adadelta() Optimiseur bas\u00e9 sur les gradients adaptatifs, sans n\u00e9cessit\u00e9 de r\u00e9glage du taux d'apprentissage. 'Adafactor' optim.Adafactor() Optimiseur variant d'Adam avec une mise \u00e0 jour plus efficace de la m\u00e9moire pour de grands mod\u00e8les. 'Adam' optim.Adam() Optimiseur utilisant un gradient stochastique adaptatif avec des moyennes mobiles des gradients. 'AdamW' optim.AdamW() Optimiseur avec une r\u00e9gularisation L2 (weight decay) distincte. 'Adamax' optim.Adamax() Optimiseur utilisant une norme infinie pour les gradients, plus stable pour certaines configurations. 'ASGD' optim.ASGD() Optimiseur utilis\u00e9 pour de grandes donn\u00e9es avec une moyenne des gradients. 'NAdam' optim.NAdam() Optimiseur avec une adaptation des moments de second ordre. 'RAdam' optim.RAdam() Optimiseur qui ajuste dynamiquement les moments pour stabiliser l'entra\u00eenement. 'RMSprop' optim.RMSprop() Optimiseur utilisant une moyenne mobile des carr\u00e9s des gradients pour r\u00e9duire les oscillations. 'Rprop' optim.Rprop() Optimiseur bas\u00e9 sur les mises \u00e0 jour des poids ind\u00e9pendantes des gradients. 'SGD' optim.SGD() Optimiseur souvent utilis\u00e9e avec un taux d'apprentissage constant ou ajust\u00e9.","title":"optims() {#optims}"},{"location":"#device","text":"variable principale d'allocation des performances","title":"device"},{"location":"#apple-silicon-macos","text":"Si le syst\u00e8me d'exploitation est macOS (nomm\u00e9 darwin dans platform.system() ), la fonction v\u00e9rifie si l'acc\u00e9l\u00e9rateur Metal Performance Shaders (MPS) est disponible sur l'appareil. Si MPS est disponible ( torch.backends.mps.is_available() ), l'appareil cible sera d\u00e9fini sur 'MPS' (c'est un \u00e9quivalent de CUDA pour les appareils Apple Silicon).","title":"Apple Silicon (macOS)"},{"location":"#windows","text":"Si le syst\u00e8me d'exploitation est Windows, la fonction v\u00e9rifie d'abord si CUDA (NVIDIA) est disponible avec torch.cuda.is_available() . Si c'est le cas, le p\u00e9riph\u00e9rique sera d\u00e9fini sur CUDA .","title":"Windows"},{"location":"#linux","text":"Si le syst\u00e8me d'exploitation est Linux, plusieurs v\u00e9rifications sont effectu\u00e9es : CUDA (NVIDIA) : Si torch.cuda.is_available() renvoie True , le p\u00e9riph\u00e9rique sera d\u00e9fini sur 'CUDA' . ROCm (AMD) : Si le syst\u00e8me supporte ROCm via torch.backends.hip.is_available() , l'appareil sera d\u00e9fini sur 'CUDA' (ROCm est utilis\u00e9 pour les cartes AMD dans le cadre de l'API CUDA). Intel oneAPI / XPU : Si le syst\u00e8me prend en charge Intel oneAPI ou XPU via torch.xpu.is_available() , le p\u00e9riph\u00e9rique sera d\u00e9fini sur XPU .","title":"Linux"},{"location":"#systeme-non-reconnu","text":"Si aucune des conditions ci-dessus n'est remplie, la fonction retourne 'CPU' comme p\u00e9riph\u00e9rique par d\u00e9faut.","title":"Syst\u00e8me non reconnu"}]}