<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="author" content="Alexandre Brun" /><link rel="canonical" href="https://xXxFetraxXx.github.io/NeuralNetworks/MLP/" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>MLP - NeuralNetworks</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../css/custom.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "MLP";
        var mkdocs_page_input_path = "MLP/index.md";
        var mkdocs_page_url = "/NeuralNetworks/MLP/";
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> NeuralNetworks
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="..">Accueil</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Objets</span></p>
              <ul class="current">
                  <li class="toctree-l1 current"><a class="reference internal current" >MLP</a>
    <ul class="current">
                <li class="toctree-l2 current"><a class="reference internal current" href="#">MLP</a>
    <ul class="current">
    <li class="toctree-l3"><a class="reference internal" href="#parameters">Parameters</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#attributes">Attributes</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#methods">Methods</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#objets-et-dictionnaires">Objets et dictionnaires</a>
    </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="params/">MLP.params</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="neurons/">MLP.neurons</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="nb_params/">MLP.nb_params</a>
                </li>
                <li class="toctree-l2"><a class="" href="plot.md">MLP.plot</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="train/">MLP.train</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="losses/">losses</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Latent</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../placeholder/">Latent</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../placeholder/">Latent</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../placeholder/">Latent</a>
                </li>
    </ul>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Tools</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" >image</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../tools/image/image_from_url/">image_from_url</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../tools/image/plot/">plot</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../tools/image/compare/">compare</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >MNIST</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../placeholder/">Latent</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../placeholder/">Latent</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >AirfRANS</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../placeholder/">Latent</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../placeholder/">Latent</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Dependances</span></p>
              <ul>
                  <li class="toctree-l1"><a class="" href="https://pypi.org/project/torch/">torch</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="https://pypi.org/project/visualtorch/">visualtorch</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="https://pypi.org/project/random-fourier-features-pytorch/">rff</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="https://pypi.org/project/numpy/">numpy</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="https://pypi.org/project/matplotlib/">matplotlib</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="https://airfrans.readthedocs.io/en/latest/">airfrans</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Liens</span></p>
              <ul>
                  <li class="toctree-l1"><a class="" href="https://pypi.org/project/NeuralNetworks/">PyPI</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="https://github.com/xXxFetraxXx/NeuralNetworks">Source</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">NeuralNetworks</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">Objets</li>
          <li class="breadcrumb-item">MLP</li>
      <li class="breadcrumb-item active">MLP</li>
    <li class="wy-breadcrumbs-aside">
          <a href="https://github.com/xXxFetraxXx/NeuralNetworks/edit/master/docs/MLP/index.md" class="icon icon-github"> Edit on GitHub</a>
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="multi-layer-perceptron-mlp">Multi-Layer Perceptron (MLP)<a class="headerlink" href="#multi-layer-perceptron-mlp" title="Permanent link">&para;</a></h1>
<p>Multi-Layer Perceptron (MLP) avec encodage optionnel Fourier (RFF),<br />
suivi automatique des pertes, visualisation et compilation PyTorch.</p>
<p>Cette classe fournit :</p>
<ul>
<li>Un MLP entièrement configurable (dimensions, normalisation, activation)  </li>
<li>Option d'encodage Fourier (Random Fourier Features) sur les entrées  </li>
<li>Méthodes pour entraîner le réseau avec mini-batchs et AMP (Automatic Mixed Precision)  </li>
<li>Visualisation de l'architecture via visualtorch  </li>
<li>Suivi et affichage de la perte d'entraînement  </li>
<li>Accès aux poids, biais et nombre de paramètres  </li>
<li>Compilation du modèle via <code>torch.compile</code> pour accélérer l'inférence  </li>
</ul>
<hr />
<h2 id="parameters">Parameters<a class="headerlink" href="#parameters" title="Permanent link">&para;</a></h2>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Type</th>
<th>Optional</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>layers</code></td>
<td>list[int]</td>
<td>Yes</td>
<td>Dimensions successives du réseau (entrée → couches cachées → sortie). Exemple : <code>[in_features, hidden1, hidden2, ..., out_features]</code>. Default: <code>[1, 1, 1]</code></td>
</tr>
<tr>
<td><code>init_lr</code></td>
<td>float</td>
<td>Yes</td>
<td>Taux d’apprentissage initial pour l’optimiseur. Default: <code>1e-3</code></td>
</tr>
<tr>
<td><code>Fourier</code></td>
<td>list[float]</td>
<td>Oui</td>
<td>Liste de sigma pour encodages RFF. Si None : passthrough. Default: <code>None</code></td>
</tr>
<tr>
<td><code>optim</code></td>
<td>str</td>
<td>Yes</td>
<td>Nom de l’optimiseur à utiliser (doit exister dans <code>optims()</code>). Default: <code>"Adam"</code></td>
</tr>
<tr>
<td><code>crit</code></td>
<td>str</td>
<td>Yes</td>
<td>Fonction de perte à utiliser (doit exister dans <code>crits()</code>). Default: <code>"MSE"</code></td>
</tr>
<tr>
<td><code>norm</code></td>
<td>str</td>
<td>Yes</td>
<td>Type de normalisation / activation pour les couches cachées (ex: <code>"Relu"</code>). Default: <code>"Relu"</code></td>
</tr>
<tr>
<td><code>name</code></td>
<td>str</td>
<td>Yes</td>
<td>Nom du réseau pour identification ou affichage. Default: <code>"Net"</code></td>
</tr>
<tr>
<td><code>Iscompiled</code></td>
<td>bool</td>
<td>Yes</td>
<td>Si True, compile le modèle via <code>torch.compile</code> pour accélérer l’inférence. Default: <code>True</code></td>
</tr>
</tbody>
</table>
<hr />
<h2 id="attributes">Attributes<a class="headerlink" href="#attributes" title="Permanent link">&para;</a></h2>
<ul>
<li><code>losses : list[torch.Tensor]</code> — Historique des pertes cumulées lors de l'entraînement  </li>
<li><code>layers : list[int]</code> — Dimensions du réseau, ajustées si encodage Fourier actif  </li>
<li><code>encodings</code> : liste de modules d'encodage (<code>RFF</code> ou <code>Identity</code>)  </li>
<li><code>norm : nn.Module</code> — Normalisation ou activation utilisée dans les couches cachées  </li>
<li><code>crit : nn.Module</code> — Fonction de perte PyTorch sur le device spécifié  </li>
<li><code>model : nn.Sequential</code> — MLP complet construit dynamiquement  </li>
<li><code>optim : torch.optim.Optimizer</code> — Optimiseur associé au MLP  </li>
<li><code>name : str</code> — Nom du réseau</li>
<li><code>f</code> : couche linéaire finale combinant sorties encodages  </li>
</ul>
<hr />
<h2 id="methods">Methods<a class="headerlink" href="#methods" title="Permanent link">&para;</a></h2>
<ul>
<li><code>plot(inputs, img_array)</code> — Affiche l’image originale, l’image prédite et la courbe des pertes  </li>
<li><code>train(inputs, outputs, num_epochs=1500, batch_size=1024)</code> — Entraîne le MLP avec mini-batchs et AMP, stocke les pertes  </li>
<li><code>params()</code> — Retourne tous les poids du MLP sous forme de liste d’<code>ndarray</code>  </li>
<li><code>neurons()</code> — Retourne tous les biais du MLP sous forme de liste d’<code>ndarray</code>  </li>
<li><code>nb_params()</code> — Calcule le nombre total de paramètres (poids uniquement) du réseau  </li>
</ul>
<hr />
<h2 id="objets-et-dictionnaires">Objets et dictionnaires<a class="headerlink" href="#objets-et-dictionnaires" title="Permanent link">&para;</a></h2>
<h1 id="norms"><strong>norms()</strong><a class="headerlink" href="#norms" title="Permanent link">&para;</a></h1>
<table>
<thead>
<tr>
<th><strong>Valeurs</strong></th>
<th><strong>Module PyTorch</strong></th>
<th><strong>Description</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>"Relu"</strong></td>
<td><code>nn.ReLU()</code></td>
<td>Fonction d'activation ReLU classique (Rectified Linear Unit).</td>
</tr>
<tr>
<td><strong>"LeakyRelu"</strong></td>
<td><code>nn.LeakyReLU()</code></td>
<td>ReLU avec un petit coefficient pour les valeurs négatives (paramètre <code>negative_slope</code>).</td>
</tr>
<tr>
<td><strong>"ELU"</strong></td>
<td><code>nn.ELU()</code></td>
<td>Fonction d'activation ELU (Exponential Linear Unit), qui a une meilleure gestion des valeurs négatives.</td>
</tr>
<tr>
<td><strong>"SELU"</strong></td>
<td><code>nn.SELU()</code></td>
<td>SELU (Scaled Exponential Linear Unit), une version améliorée de l'ELU pour des réseaux auto-normalisants.</td>
</tr>
<tr>
<td><strong>"GELU"</strong></td>
<td><code>nn.GELU()</code></td>
<td>GELU (Gaussian Error Linear Unit), une activation probabiliste basée sur une fonction gaussienne.</td>
</tr>
<tr>
<td><strong>"Sigmoid"</strong></td>
<td><code>nn.Sigmoid()</code></td>
<td>Fonction d'activation Sigmoid, qui produit une sortie entre 0 et 1.</td>
</tr>
<tr>
<td><strong>"Tanh"</strong></td>
<td><code>nn.Tanh()</code></td>
<td>Fonction d'activation Tanh, avec une sortie dans l'intervalle [-1, 1].</td>
</tr>
<tr>
<td><strong>"Hardtanh"</strong></td>
<td><code>nn.Hardtanh()</code></td>
<td>Variante de Tanh, avec des sorties limitées entre une plage spécifiée.</td>
</tr>
<tr>
<td><strong>"Softplus"</strong></td>
<td><code>nn.Softplus()</code></td>
<td>Fonction d'activation qui approxime ReLU mais de manière lissée.</td>
</tr>
<tr>
<td><strong>"Softsign"</strong></td>
<td><code>nn.Softsign()</code></td>
<td>Fonction d'activation similaire à Tanh mais plus souple, avec des valeurs dans [-1, 1].</td>
</tr>
</tbody>
</table>
<hr />
<h1 id="crits"><strong>crits()</strong><a class="headerlink" href="#crits" title="Permanent link">&para;</a></h1>
<table>
<thead>
<tr>
<th><strong>Valeurs</strong></th>
<th><strong>Module PyTorch</strong></th>
<th><strong>Description</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>"MSE"</strong></td>
<td><code>nn.MSELoss()</code></td>
<td>Mean Squared Error Loss, utilisée pour les régressions.</td>
</tr>
<tr>
<td><strong>"L1"</strong></td>
<td><code>nn.L1Loss()</code></td>
<td>L1 Loss (erreur absolue), souvent utilisée pour la régularisation.</td>
</tr>
<tr>
<td><strong>"SmoothL1"</strong></td>
<td><code>nn.SmoothL1Loss()</code></td>
<td>Smooth L1 Loss, une combinaison de L1 et de MSE, moins sensible aux outliers.</td>
</tr>
<tr>
<td><strong>"Huber"</strong></td>
<td><code>nn.HuberLoss()</code></td>
<td>Fonction de perte Huber, une version lissée de L1 et MSE, moins affectée par les grands écarts.</td>
</tr>
<tr>
<td><strong>"CrossEntropy"</strong></td>
<td><code>nn.CrossEntropyLoss()</code></td>
<td>Perte de Cross-Entropy, utilisée pour les problèmes de classification multi-classes.</td>
</tr>
<tr>
<td><strong>"KLDiv"</strong></td>
<td><code>nn.KLDivLoss()</code></td>
<td>Perte de divergence de Kullback-Leibler, souvent utilisée pour des modèles probabilistes.</td>
</tr>
<tr>
<td><strong>"PoissonNLL"</strong></td>
<td><code>nn.PoissonNLLLoss()</code></td>
<td>Perte de log-vraisemblance pour une distribution de Poisson, utilisée pour la modélisation de comptages.</td>
</tr>
<tr>
<td><strong>"MultiLabelSoftMargin"</strong></td>
<td><code>nn.MultiLabelSoftMarginLoss()</code></td>
<td>Perte utilisée pour les problèmes de classification multi-étiquettes.</td>
</tr>
</tbody>
</table>
<hr />
<h1 id="optims"><strong>optims()</strong><a class="headerlink" href="#optims" title="Permanent link">&para;</a></h1>
<table>
<thead>
<tr>
<th><strong>Valeurs</strong></th>
<th><strong>Module PyTorch</strong></th>
<th><strong>Description</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>"Adadelta"</strong></td>
<td><code>optim.Adadelta()</code></td>
<td>Optimiseur Adadelta, basé sur les gradients adaptatifs, sans nécessité de réglage du taux d'apprentissage.</td>
</tr>
<tr>
<td><strong>"Adafactor"</strong></td>
<td><code>optim.Adafactor()</code></td>
<td>Optimiseur Adafactor, variant d'Adam avec une mise à jour plus efficace de la mémoire pour de grands modèles.</td>
</tr>
<tr>
<td><strong>"Adam"</strong></td>
<td><code>optim.Adam()</code></td>
<td>Optimiseur Adam, utilisant un gradient stochastique adaptatif avec des moyennes mobiles des gradients et des carrés des gradients.</td>
</tr>
<tr>
<td><strong>"AdamW"</strong></td>
<td><code>optim.AdamW()</code></td>
<td>Optimiseur Adam avec une régularisation L2 (weight decay) distincte, plus efficace que <code>Adam</code> avec <code>weight_decay</code>.</td>
</tr>
<tr>
<td><strong>"Adamax"</strong></td>
<td><code>optim.Adamax()</code></td>
<td>Version d'Adam utilisant une norme infinie pour les gradients, plus stable pour certaines configurations.</td>
</tr>
<tr>
<td><strong>"ASGD"</strong></td>
<td><code>optim.ASGD()</code></td>
<td>Optimiseur ASGD (Averaged Stochastic Gradient Descent), utilisé pour de grandes données avec une moyenne des gradients.</td>
</tr>
<tr>
<td><strong>"NAdam"</strong></td>
<td><code>optim.NAdam()</code></td>
<td>Optimiseur NAdam, une version améliorée d'Adam avec une adaptation des moments de second ordre.</td>
</tr>
<tr>
<td><strong>"RAdam"</strong></td>
<td><code>optim.RAdam()</code></td>
<td>Optimiseur RAdam, une version robuste de l'Adam qui ajuste dynamiquement les moments pour stabiliser l'entraînement.</td>
</tr>
<tr>
<td><strong>"RMSprop"</strong></td>
<td><code>optim.RMSprop()</code></td>
<td>Optimiseur RMSprop, utilisant une moyenne mobile des carrés des gradients pour réduire les oscillations.</td>
</tr>
<tr>
<td><strong>"Rprop"</strong></td>
<td><code>optim.Rprop()</code></td>
<td>Optimiseur Rprop, basé sur les mises à jour des poids indépendantes des gradients.</td>
</tr>
<tr>
<td><strong>"SGD"</strong></td>
<td><code>optim.SGD()</code></td>
<td>Descente de gradient stochastique classique, souvent utilisée avec un taux d'apprentissage constant ou ajusté.</td>
</tr>
</tbody>
</table>
<hr />
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href=".." class="btn btn-neutral float-left" title="Accueil"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="params/" class="btn btn-neutral float-right" title="MLP.params">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
        <span>
          <a href="https://github.com/xXxFetraxXx/NeuralNetworks" class="fa fa-github" style="color: #fcfcfc"> GitHub</a>
        </span>
    
    
      <span><a href=".." style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="params/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "..";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
      <script src="../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
