<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="description" content="Multi-Layer Perceptrons with multi-Fourier encoding and variable learning rate" /><meta name="author" content="Alexandre Brun" /><link rel="canonical" href="https://xXxFetraxXx.github.io/NeuralNetworks/" />
      <link rel="shortcut icon" href="img/favicon.ico" />
    <title>NeuralNetworks</title>
    <link rel="stylesheet" href="css/theme.css" />
    <link rel="stylesheet" href="css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Accueil";
        var mkdocs_page_input_path = "index.md";
        var mkdocs_page_url = "/NeuralNetworks/";
      </script>
    
    <!--[if lt IE 9]>
      <script src="js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="." class="icon icon-home"> NeuralNetworks
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="./search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
                <li class="toctree-l1 current"><a class="reference internal current" href="#">Accueil</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#classes">Classes</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#mlp">MLP</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#parametres">Paramètres</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#attributs">Attributs</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#trainer">Trainer</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#parametres_1">Paramètres</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#trainertrain">Trainer.train</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#methodes">Méthodes</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#losses">losses</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#learnings">learnings</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#dictionnaires">Dictionnaires</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#norms">norms</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#optims">optims</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#crits">crits</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#device">device</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#apple-silicon-macos">Apple Silicon (macOS)</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#windows">Windows</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#linux">Linux</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#systeme-non-reconnu">Système non reconnu</a>
    </li>
        </ul>
    </li>
    </ul>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Dependances</span></p>
              <ul>
                  <li class="toctree-l1"><a class="" href="https://pypi.org/project/torch/">torch</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="https://pypi.org/project/numpy/">numpy</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="https://pypi.org/project/matplotlib/">matplotlib</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Liens</span></p>
              <ul>
                  <li class="toctree-l1"><a class="" href="https://pypi.org/project/NeuralNetworks/">PyPI</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="https://github.com/xXxFetraxXx/NeuralNetworks">Source</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href=".">NeuralNetworks</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="." class="icon icon-home" aria-label="Docs"></a></li>
      <li class="breadcrumb-item active">Accueil</li>
    <li class="wy-breadcrumbs-aside">
          <a href="https://github.com/xXxFetraxXx/NeuralNetworks/edit/master/docs/index.md" class="icon icon-github"> Edit on GitHub</a>
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="neuralnetworks-module">NeuralNetworks Module<a class="headerlink" href="#neuralnetworks-module" title="Permanent link">&para;</a></h1>
<p>Module complet pour la création et l'entraînement de <a href="https://en.wikipedia.org/wiki/Multilayer_perceptron">MultiLayer Perceptrons</a> (MLP)<br />
avec encodage optionnel <a href="https://en.wikipedia.org/wiki/Random_feature#Random_Fourier_feature">Fourier Features</a> et gestion automatique des pertes.</p>
<hr />
<h2 id="classes"><strong>Classes</strong><a class="headerlink" href="#classes" title="Permanent link">&para;</a></h2>
<h3 id="mlp"><strong>MLP</strong><a class="headerlink" href="#mlp" title="Permanent link">&para;</a></h3>
<p>Cette classe fournit :</p>
<ul>
<li>Un <a href="https://en.wikipedia.org/wiki/Multilayer_perceptron">MLP</a> entièrement configurable (dimensions, activation).</li>
<li>Option d'encodage <a href="https://en.wikipedia.org/wiki/Random_feature#Random_Fourier_feature">Fourier Features</a> sur les entrées.</li>
</ul>
<hr />
<h4 id="parametres"><strong>Paramètres</strong><a class="headerlink" href="#parametres" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th><strong>Paramètres</strong></th>
<th><strong>Type</strong></th>
<th><strong>Optionnel</strong></th>
<th><strong>Description</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><code>input_size</code></td>
<td><a href="https://docs.python.org/3/library/stdtypes.html#numeric-types-int-float-complex"><code>int</code></a></td>
<td>Oui</td>
<td>Taille des données en entrée au réseau. Default: <code>1</code></td>
</tr>
<tr>
<td><code>output_size</code></td>
<td><a href="https://docs.python.org/3/library/stdtypes.html#numeric-types-int-float-complex"><code>int</code></a></td>
<td>Oui</td>
<td>Taille des données en sortie au réseau. Default: <code>1</code></td>
</tr>
<tr>
<td><code>hidden_layers</code></td>
<td><a href="https://docs.python.org/3/library/stdtypes.html#sequence-types-list-tuple-range"><code>list[int]</code></a></td>
<td>Oui</td>
<td>Dimensions successives des couches intermédiaires du réseau. Default: <code>[1]</code></td>
</tr>
<tr>
<td><code>sigmas</code></td>
<td><a href="https://docs.python.org/3/library/stdtypes.html#sequence-types-list-tuple-range"><code>list[float]</code></a></td>
<td>Oui</td>
<td>Liste de sigma pour encodages RFF. Si None : passthrough. Default: <code>None</code></td>
</tr>
<tr>
<td><code>fourier_input_size</code></td>
<td><a href="https://docs.python.org/3/library/stdtypes.html#numeric-types-int-float-complex"><code>int</code></a></td>
<td>Oui</td>
<td>WIP. Default: <code>2</code></td>
</tr>
<tr>
<td><code>nb_fourier</code></td>
<td><a href="https://docs.python.org/3/library/stdtypes.html#numeric-types-int-float-complex"><code>int</code></a></td>
<td>Oui</td>
<td>Nombre de fréquences utilisées pour les Fourier Features. Default: <code>8</code></td>
</tr>
<tr>
<td><code>norm</code></td>
<td><a href="#norms-norms"><code>norm</code></a></td>
<td>Oui</td>
<td>Type de normalisation / activation pour les couches cachées. Default: <code>'Relu'</code></td>
</tr>
<tr>
<td><code>name</code></td>
<td><a href="https://docs.python.org/3/library/stdtypes.html#text-sequence-type-str"><code>str</code></a></td>
<td>Oui</td>
<td>Nom du réseau pour identification ou affichage. Default: <code>'Net'</code></td>
</tr>
</tbody>
</table>
<h4 id="attributs"><strong>Attributs</strong><a class="headerlink" href="#attributs" title="Permanent link">&para;</a></h4>
<ul>
<li><code>losses : list[float]</code>    — Historique des pertes cumulées lors de l'entraînement  </li>
<li><code>learnings : list[float]</code> — Historique des taux d'apprentissage utilisées lors de l'entraînement  </li>
<li><code>model : nn.Sequential</code>   — MLP complet construit dynamiquement </li>
<li><code>name : str</code>              — Nom du réseau</li>
</ul>
<table>
<thead>
<tr>
<th><strong>Attributs</strong></th>
<th><strong>Type</strong></th>
<th><strong>Description</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><code>losses</code></td>
<td><a href="https://docs.python.org/3/library/stdtypes.html#sequence-types-list-tuple-range"><code>list[float]</code></a></td>
<td>Historique des pertes cumulées lors de l'entraînement</td>
</tr>
<tr>
<td><code>learnings</code></td>
<td><a href="https://docs.python.org/3/library/stdtypes.html#sequence-types-list-tuple-range"><code>list[float]</code></a></td>
<td>Historique des taux d'apprentissage utilisées lors de l'entraînement</td>
</tr>
<tr>
<td><code>model</code></td>
<td><a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Sequential.html"><code>nn.Sequential</code></a></td>
<td>MLP complet construit dynamiquement</td>
</tr>
<tr>
<td><code>name</code></td>
<td><a href="https://docs.python.org/3/library/stdtypes.html#text-sequence-type-str"><code>str</code></a></td>
<td>Nom du réseau</td>
</tr>
</tbody>
</table>
<hr />
<h3 id="trainer"><strong>Trainer</strong><a class="headerlink" href="#trainer" title="Permanent link">&para;</a></h3>
<p>Cette classe fournit :</p>
<ul>
<li>Méthode pour entraîner des réseaux avec mini-batchs et <a href="https://docs.pytorch.org/tutorials/recipes/recipes/amp_recipe.html">Automatic Mixed Precision</a></li>
</ul>
<h4 id="parametres_1"><strong>Paramètres</strong><a class="headerlink" href="#parametres_1" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th><strong>Paramètres</strong></th>
<th><strong>Type</strong></th>
<th><strong>Optionnel</strong></th>
<th><strong>Description</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><code>*nets</code></td>
<td><a href="#mlp-mlp"><code>MLP</code></a></td>
<td>Non</td>
<td>Réseaux pour lesquels le trainer va entrainer.</td>
</tr>
<tr>
<td><code>inputs</code></td>
<td><a href="https://numpy.org/doc/stable/reference/generated/numpy.array.html"><code>numpy.array(list[float])</code></a></td>
<td>Non</td>
<td>Données en entrée au réseau.</td>
</tr>
<tr>
<td><code>outputs</code></td>
<td><a href="https://numpy.org/doc/stable/reference/generated/numpy.array.html"><code>numpy.array(list[float])</code></a></td>
<td>Non</td>
<td>Données en sortie au réseau.</td>
</tr>
<tr>
<td><code>test_size</code></td>
<td><a href="https://docs.python.org/3/library/stdtypes.html#numeric-types-int-float-complex"><code>float</code></a></td>
<td>Oui</td>
<td>Proportion des données à utiliser pendant l'entrainement. Si None : utilise toutes les données. Default: <code>None</code></td>
</tr>
<tr>
<td><code>optim</code></td>
<td><a href="#optims-optims"><code>optim</code></a></td>
<td>Oui</td>
<td>Nom de l’optimiseur à utiliser (doit exister dans <code>optims()</code>). Default: <code>'Adam'</code></td>
</tr>
<tr>
<td><code>init_lr</code></td>
<td><a href="https://docs.python.org/3/library/stdtypes.html#numeric-types-int-float-complex"><code>float</code></a></td>
<td>Oui</td>
<td>Taux d’apprentissage initial pour l’optimiseur. Default: <code>1e-3</code></td>
</tr>
<tr>
<td><code>crit</code></td>
<td><a href="#crits-crits"><code>crit</code></a></td>
<td>Oui</td>
<td>Fonction de perte à utiliser (doit exister dans <code>crits()</code>). Default: <code>MSE'</code></td>
</tr>
<tr>
<td><code>batch_size</code></td>
<td><a href="https://docs.python.org/3/library/stdtypes.html#numeric-types-int-float-complex"><code>int</code></a></td>
<td>Oui</td>
<td>Taille des minibatchs. Default: <code>1024</code></td>
</tr>
</tbody>
</table>
<h4 id="trainertrain"><strong>Trainer.train</strong><a class="headerlink" href="#trainertrain" title="Permanent link">&para;</a></h4>
<p>Lancement d'un entrainement avec le trainer définit</p>
<table>
<thead>
<tr>
<th><strong>Paramètres</strong></th>
<th><strong>Type</strong></th>
<th><strong>Optionnel</strong></th>
<th><strong>Description</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><code>num_epochs</code></td>
<td><a href="https://docs.python.org/3/library/stdtypes.html#numeric-types-int-float-complex"><code>int</code></a></td>
<td>Oui</td>
<td>Nombres d'itérations à effectuer.</td>
</tr>
<tr>
<td><code>activate_tqdm</code></td>
<td><a href="https://docs.python.org/3/library/stdtypes.html#boolean-type-bool"><code>boolean</code></a></td>
<td>Oui</td>
<td>Utilisation d'une barre de progression.</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="methodes"><strong>Méthodes</strong><a class="headerlink" href="#methodes" title="Permanent link">&para;</a></h2>
<h3 id="losses"><strong>losses</strong><a class="headerlink" href="#losses" title="Permanent link">&para;</a></h3>
<p>Affiche les résidus en fonction des époques d'entrainement des réseaux.</p>
<h3 id="learnings"><strong>learnings</strong><a class="headerlink" href="#learnings" title="Permanent link">&para;</a></h3>
<p>Affiche les taux d'apprentissage en fonction des époques d'entrainement des réseaux.</p>
<hr />
<h2 id="dictionnaires"><strong>Dictionnaires</strong><a class="headerlink" href="#dictionnaires" title="Permanent link">&para;</a></h2>
<h3 id="norms"><strong>norms</strong><a class="headerlink" href="#norms" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th><strong>Valeurs</strong></th>
<th><strong>Module PyTorch</strong></th>
<th><strong>Description</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><code>'ReLU'</code></td>
<td><a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.ReLU.html"><code>nn.ReLU()</code></a></td>
<td>Fonction d'activation ReLU classique (Rectified Linear Unit).</td>
</tr>
<tr>
<td><code>'LeakyReLU'</code></td>
<td><a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.LeakyReLU.html"><code>nn.LeakyReLU()</code></a></td>
<td>ReLU avec un petit coefficient pour les valeurs négatives (paramètre <code>negative_slope</code>).</td>
</tr>
<tr>
<td><code>'ELU'</code></td>
<td><a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.ELU.html"><code>nn.ELU()</code></a></td>
<td>Fonction d'activation ELU (Exponential Linear Unit), qui a une meilleure gestion des valeurs négatives.</td>
</tr>
<tr>
<td><code>'SELU'</code></td>
<td><a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.SELU.html"><code>nn.SELU()</code></a></td>
<td>SELU (Scaled Exponential Linear Unit), une version améliorée de l'ELU pour des réseaux auto-normalisants.</td>
</tr>
<tr>
<td><code>'GELU'</code></td>
<td><a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.GELU.html"><code>nn.GELU()</code></a></td>
<td>GELU (Gaussian Error Linear Unit), une activation probabiliste basée sur une fonction gaussienne.</td>
</tr>
<tr>
<td><code>'Mish'</code></td>
<td><a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Mish.html"><code>nn.Mish()</code></a></td>
<td>ReLU différentiable en tout points avec passage négatif.</td>
</tr>
<tr>
<td><code>'Softplus'</code></td>
<td><a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Softplus.html"><code>nn.Softplus()</code></a></td>
<td>Fonction d'activation qui approxime ReLU mais de manière lissée.</td>
</tr>
<tr>
<td><code>'Sigmoid'</code></td>
<td><a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html"><code>nn.Sigmoid()</code></a></td>
<td>Fonction d'activation Sigmoid, qui produit une sortie entre 0 et 1.</td>
</tr>
<tr>
<td><code>'Tanh'</code></td>
<td><a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Tanh.html"><code>nn.Tanh()</code></a></td>
<td>Fonction d'activation Tanh, avec une sortie dans l'intervalle [-1, 1].</td>
</tr>
<tr>
<td><code>'Hardtanh'</code></td>
<td><a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Hardtanh.html"><code>nn.Hardtanh()</code></a></td>
<td>Variante de Tanh, avec des sorties limitées entre une plage spécifiée.</td>
</tr>
<tr>
<td><code>'Softsign'</code></td>
<td><a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Softsign.html"><code>nn.Softsign()</code></a></td>
<td>Fonction d'activation similaire à Tanh mais plus souple, avec des valeurs dans [-1, 1].</td>
</tr>
</tbody>
</table>
<hr />
<h3 id="optims"><strong>optims</strong><a class="headerlink" href="#optims" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th><strong>Valeurs</strong></th>
<th><strong>Module PyTorch</strong></th>
<th><strong>Description</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><code>'Adadelta'</code></td>
<td><a href="https://docs.pytorch.org/docs/stable/generated/torch.optim.Adadelta.html"><code>optim.Adadelta()</code></a></td>
<td>Optimiseur basé sur les gradients adaptatifs, sans nécessité de réglage du taux d'apprentissage.</td>
</tr>
<tr>
<td><code>'Adafactor'</code></td>
<td><a href="https://docs.pytorch.org/docs/stable/generated/torch.optim.Adafactor.html"><code>optim.Adafactor()</code></a></td>
<td>Optimiseur variant d'Adam avec une mise à jour plus efficace de la mémoire pour de grands modèles.</td>
</tr>
<tr>
<td><code>'Adam'</code></td>
<td><a href="https://docs.pytorch.org/docs/stable/generated/torch.optim.Adam.html"><code>optim.Adam()</code></a></td>
<td>Optimiseur utilisant un gradient stochastique adaptatif avec des moyennes mobiles des gradients.</td>
</tr>
<tr>
<td><code>'AdamW'</code></td>
<td><a href="https://docs.pytorch.org/docs/stable/generated/torch.optim.AdamW.html"><code>optim.AdamW()</code></a></td>
<td>Optimiseur avec une régularisation L2 (weight decay) distincte.</td>
</tr>
<tr>
<td><code>'Adamax'</code></td>
<td><a href="https://docs.pytorch.org/docs/stable/generated/torch.optim.Adamax.html"><code>optim.Adamax()</code></a></td>
<td>Optimiseur utilisant une norme infinie pour les gradients, plus stable pour certaines configurations.</td>
</tr>
<tr>
<td><code>'ASGD'</code></td>
<td><a href="https://docs.pytorch.org/docs/stable/generated/torch.optim.ASGD.html"><code>optim.ASGD()</code></a></td>
<td>Optimiseur utilisé pour de grandes données avec une moyenne des gradients.</td>
</tr>
<tr>
<td><code>'NAdam'</code></td>
<td><a href="https://docs.pytorch.org/docs/stable/generated/torch.optim.NAdam.html"><code>optim.NAdam()</code></a></td>
<td>Optimiseur avec une adaptation des moments de second ordre.</td>
</tr>
<tr>
<td><code>'RAdam'</code></td>
<td><a href="https://docs.pytorch.org/docs/stable/generated/torch.optim.RAdam.html"><code>optim.RAdam()</code></a></td>
<td>Optimiseur qui ajuste dynamiquement les moments pour stabiliser l'entraînement.</td>
</tr>
<tr>
<td><code>'RMSprop'</code></td>
<td><a href="https://docs.pytorch.org/docs/stable/generated/torch.optim.RMSprop.html"><code>optim.RMSprop()</code></a></td>
<td>Optimiseur utilisant une moyenne mobile des carrés des gradients pour réduire les oscillations.</td>
</tr>
<tr>
<td><code>'Rprop'</code></td>
<td><a href="https://docs.pytorch.org/docs/stable/generated/torch.optim.Rprop.html"><code>optim.Rprop()</code></a></td>
<td>Optimiseur basé sur les mises à jour des poids indépendantes des gradients.</td>
</tr>
<tr>
<td><code>'SGD'</code></td>
<td><a href="https://docs.pytorch.org/docs/stable/generated/torch.optim.SGD.html"><code>optim.SGD()</code></a></td>
<td>Optimiseur souvent utilisée avec un taux d'apprentissage constant ou ajusté.</td>
</tr>
</tbody>
</table>
<hr />
<h3 id="crits"><strong>crits</strong><a class="headerlink" href="#crits" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th><strong>Valeurs</strong></th>
<th><strong>Module PyTorch</strong></th>
<th><strong>Description</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><code>'MSE'</code></td>
<td><a href="https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html"><code>nn.MSELoss</code></a></td>
<td>Perte utilisée pour les régressions.</td>
</tr>
<tr>
<td><code>'L1'</code></td>
<td><a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.L1Loss.html"><code>nn.L1Loss()</code></a></td>
<td>Perte utilisée pour la régularisation.</td>
</tr>
<tr>
<td><code>'SmoothL1'</code></td>
<td><a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.SmoothL1Loss.html"><code>nn.SmoothL1Loss()</code></a></td>
<td>Perte moins sensible aux outliers.</td>
</tr>
<tr>
<td><code>'Huber'</code></td>
<td><a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.HuberLoss.html"><code>nn.HuberLoss()</code></a></td>
<td>Perte moins affectée par les grands écarts.</td>
</tr>
<tr>
<td><code>'CrossEntropy'</code></td>
<td><a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html"><code>nn.CrossEntropyLoss()</code></a></td>
<td>Perte utilisée pour les problèmes de classification multi-classes.</td>
</tr>
<tr>
<td><code>'KLDiv'</code></td>
<td><a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html"><code>nn.KLDivLoss()</code></a></td>
<td>Perte utilisée pour des modèles probabilistes.</td>
</tr>
<tr>
<td><code>'PoissonNLL'</code></td>
<td><a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.PoissonNLLLoss.html"><code>nn.PoissonNLLLoss()</code></a></td>
<td>Perte utilisée pour la modélisation de comptages.</td>
</tr>
<tr>
<td><code>'MultiLabelSoftMargin'</code></td>
<td><a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.MultiLabelSoftMarginLoss.html"><code>nn.MultiLabelSoftMarginLoss()</code></a></td>
<td>Perte utilisée pour les problèmes de classification multi-étiquettes.</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="device"><strong>device</strong><a class="headerlink" href="#device" title="Permanent link">&para;</a></h2>
<p>variable principale d'allocation des performances</p>
<h3 id="apple-silicon-macos"><strong>Apple Silicon (macOS)</strong><a class="headerlink" href="#apple-silicon-macos" title="Permanent link">&para;</a></h3>
<ul>
<li>Si le système d'exploitation est macOS (nommé <code>darwin</code> dans <code>platform.system()</code>), la fonction vérifie si l'accélérateur <strong>Metal Performance Shaders</strong> (MPS) est disponible sur l'appareil.</li>
<li>Si MPS est disponible (<code>torch.backends.mps.is_available()</code>), l'appareil cible sera défini sur <code>'mps'</code> (c'est un équivalent de CUDA pour les appareils Apple Silicon).</li>
</ul>
<h3 id="windows"><strong>Windows</strong><a class="headerlink" href="#windows" title="Permanent link">&para;</a></h3>
<ul>
<li>Si le système d'exploitation est Windows, la fonction vérifie d'abord si <strong>CUDA</strong> (NVIDIA) est disponible avec <code>torch.cuda.is_available()</code>. Si c'est le cas, le périphérique sera défini sur <strong>CUDA</strong>.</li>
</ul>
<h3 id="linux"><strong>Linux</strong><a class="headerlink" href="#linux" title="Permanent link">&para;</a></h3>
<ul>
<li>Si le système d'exploitation est Linux, plusieurs vérifications sont effectuées :</li>
<li><strong>CUDA</strong> (NVIDIA) : Si <code>torch.cuda.is_available()</code> renvoie <code>True</code>, le périphérique sera défini sur <code>'cuda'</code>.</li>
<li><strong>ROCm</strong> (AMD) : Si le système supporte <strong>ROCm</strong> via <code>torch.backends.hip.is_available()</code>, l'appareil sera défini sur <code>'cuda'</code> (ROCm est utilisé pour les cartes AMD dans le cadre de l'API CUDA).</li>
<li><strong>Intel oneAPI / XPU</strong> : Si le système prend en charge <strong>Intel oneAPI</strong> ou <strong>XPU</strong> via <code>torch.xpu.is_available()</code>, le périphérique sera défini sur <strong>XPU</strong>.</li>
</ul>
<h3 id="systeme-non-reconnu"><strong>Système non reconnu</strong><a class="headerlink" href="#systeme-non-reconnu" title="Permanent link">&para;</a></h3>
<ul>
<li>Si aucune des conditions ci-dessus n'est remplie, la fonction retourne <code>'cpu'</code> comme périphérique par défaut.</li>
</ul>
<hr />
              
            </div>
          </div><footer>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
        <span>
          <a href="https://github.com/xXxFetraxXx/NeuralNetworks" class="fa fa-github" style="color: #fcfcfc"> GitHub</a>
        </span>
    
    
    
  </span>
</div>
    <script src="js/jquery-3.6.0.min.js"></script>
    <script>var base_url = ".";</script>
    <script src="js/theme_extra.js"></script>
    <script src="js/theme.js"></script>
      <script src="search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>

<!--
MkDocs version : 1.6.1
Build Date UTC : 2026-01-17 21:03:10.444009+00:00
-->
