# **NeuralNetworks.optims**

| **Valeurs**   | **Module PyTorch**                                                                               | **Description**                                                                                       |
|---------------|--------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------|
| `'Adadelta'`  | [`optim.Adadelta()`](https://docs.pytorch.org/docs/stable/generated/torch.optim.Adadelta.html)   | Optimiseur basé sur les gradients adaptatifs, sans nécessité de réglage du taux d'apprentissage.      |
| `'Adafactor'` | [`optim.Adafactor()`](https://docs.pytorch.org/docs/stable/generated/torch.optim.Adafactor.html) | Optimiseur variant d'Adam avec une mise à jour plus efficace de la mémoire pour de grands modèles.    |
| `'Adam'`      | [`optim.Adam()`](https://docs.pytorch.org/docs/stable/generated/torch.optim.Adam.html)           | Optimiseur utilisant un gradient stochastique adaptatif avec des moyennes mobiles des gradients.      |
| `'AdamW'`     | [`optim.AdamW()`](https://docs.pytorch.org/docs/stable/generated/torch.optim.AdamW.html)         | Optimiseur avec une régularisation L2 (weight decay) distincte.                                       |
| `'Adamax'`    | [`optim.Adamax()`](https://docs.pytorch.org/docs/stable/generated/torch.optim.Adamax.html)       | Optimiseur utilisant une norme infinie pour les gradients, plus stable pour certaines configurations. |
| `'ASGD'`      | [`optim.ASGD()`](https://docs.pytorch.org/docs/stable/generated/torch.optim.ASGD.html)           | Optimiseur utilisé pour de grandes données avec une moyenne des gradients.                            |
| `'NAdam'`     | [`optim.NAdam()`](https://docs.pytorch.org/docs/stable/generated/torch.optim.NAdam.html)         | Optimiseur avec une adaptation des moments de second ordre.                                           |
| `'RAdam'`     | [`optim.RAdam()`](https://docs.pytorch.org/docs/stable/generated/torch.optim.RAdam.html)         | Optimiseur qui ajuste dynamiquement les moments pour stabiliser l'entraînement.                       |
| `'RMSprop'`   | [`optim.RMSprop()`](https://docs.pytorch.org/docs/stable/generated/torch.optim.RMSprop.html)     | Optimiseur utilisant une moyenne mobile des carrés des gradients pour réduire les oscillations.       |
| `'Rprop'`     | [`optim.Rprop()`](https://docs.pytorch.org/docs/stable/generated/torch.optim.Rprop.html)         | Optimiseur basé sur les mises à jour des poids indépendantes des gradients.                           |
| `'SGD'`       | [`optim.SGD()`](https://docs.pytorch.org/docs/stable/generated/torch.optim.SGD.html)             | Optimiseur souvent utilisée avec un taux d'apprentissage constant ou ajusté.                          |